---
title: (Speech Recognition) Conformer ë¦¬ë·° ë° ì„¤ëª…
author: simon sanghyeon
date: 2022-07-28
categories: [Speech Recognition]
tags: [Speech AI, ASR, End to End, Paper Review]
render_with_liquid: true
use_math: true
---
ì´ í¬ìŠ¤íŠ¸ëŠ” ê°œì¸ì ìœ¼ë¡œ ê³µë¶€í•œ ë‚´ìš©ì„ ì •ë¦¬í•˜ê³  í•„ìš”í•œ ë¶„ë“¤ì—ê²Œ ì§€ì‹ì„ ê³µìœ í•˜ê¸° ìœ„í•´ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤. ì§€ì í•˜ì‹¤ ë‚´ìš©ì´ ìˆë‹¤ë©´, ì–¸ì œë“  ëŒ“ê¸€ ë˜ëŠ” ë©”ì¼ë¡œ ì•Œë ¤ì£¼ì‹œê¸°ë¥¼ ë°”ëë‹ˆë‹¤.

---


ğŸ’¡ 2020ë…„, êµ¬ê¸€ì—ì„œ ë°œí‘œí•œ â€œConformer : Convolution-augmented Transformer for Speech Recognitionâ€ ë…¼ë¬¸ì„ ì„¤ëª…í•œ ê¸€ì´ë‹¤.


# 01. ë“¤ì–´ê°€ë©°

## ê¸°ì¡´ end-to-end ìë™ ìŒì„± ì¸ì‹ ì ‘ê·¼ ë°©ì‹

acoustic model, pronunciation model, language model ë“± ìˆ˜ë§ì€ ì»´í¬ë„ŒíŠ¸ë“¤ë¡œ ì´ë£¨ì–´ì ¸ìˆì—ˆë˜ ê³¼ê±°ì˜ ìŒì„± ì¸ì‹ ì•„í‚¤í…ì²˜ëŠ” ë”¥ëŸ¬ë‹ ê¸°ìˆ ì´ ë„ì…ë˜ë©´ì„œ end-to-end ë°©ì‹ìœ¼ë¡œ ë°”ë€Œì–´ì™”ë‹¤. conformerê°€ ë°œí‘œë˜ì—ˆë˜ 2020ë…„ê¹Œì§€ë„ ë”¥ëŸ¬ë‹ì„ í™œìš©í•œ end-to-end ASRì— ëŒ€í•œ ë‹¤ì–‘í•œ ì ‘ê·¼ ë°©ì‹ë“¤ì´ ìˆì—ˆë‹¤.

ëŒ€í‘œì ìœ¼ë¡œ `Recurrent Neural Network(RNN)`ë¥¼ í™œìš©í•œ ì ‘ê·¼ ë°©ì‹ì´ ìˆì—ˆë‹¤. RNNì€ ìŒí–¥ ì‹ í˜¸ sequenceì— ëŒ€í•œ time step ë³„ ì˜ì¡´ì„±(dependency)ì„ í¬ì°©í•˜ëŠ” ë° íƒì›”í•˜ê¸° ë•Œë¬¸ì— ë§ì€ ì—°êµ¬ë“¤ì´ RNNì„ ì ìš©í•˜ì˜€ë‹¤.

- RNN ê¸°ë°˜ì˜ ASR ì„ í–‰ ì—°êµ¬
    - [State-of-the-Art Speech Recognition with Sequence-to-Sequence Models](https://ieeexplore.ieee.org/abstract/document/8462105)
    - [Exploring Architectures, Data and Units For Streaming End-to-End Speech Recognition with RNN-Transducer](https://arxiv.org/abs/1801.00841)
    - [Streaming End-to-end Speech Recognition For Mobile Devices](https://arxiv.org/abs/1811.06621)

í•œí¸, ìµœê·¼ `transformer`ëŠ” self-attention ê¸°ë²•ìœ¼ë¡œ sequenceì˜ ê±°ë¦¬ê°€ ë©€ì–´ë„ ë§¥ë½ ì •ë³´ë“¤ì„ ì˜ íŒŒì•…í•  ìˆ˜ ìˆë‹¤. ì´ëŸ¬í•œ ì¥ì ìœ¼ë¡œ RNN ê¸°ë°˜ì˜ NLP íŒë„ë¥¼ ë°”ê¾¼ transformerëŠ” ì»´í“¨í„°ë¹„ì „ ë¿ë§Œ ì•„ë‹ˆë¼ end-to-end ìŒì„± ì¸ì‹ì—ë„ í° ê¸°ì—¬ë¥¼ í–ˆë‹¤.

- transformer ê¸°ë°˜ì˜ ASR ì„ í–‰ ì—°êµ¬
    - [Transformer Transducer: A Streamable Speech Recognition Model with Transformer Encoders and RNN-T Loss](https://arxiv.org/abs/2002.02562)

ê·¸ë¦¬ê³ , `Convolution Neural Network(CNN)` ì—­ì‹œ ë ˆì´ì–´ë§ˆë‹¤ receptive fieldë¥¼ í†µí•´ ì§€ì—­ì ì¸ ë§¥ë½ ì •ë³´ë¥¼ í¬ì°©í•˜ëŠ” ë° íš¨ê³¼ì ì´ë¯€ë¡œ end-to-end ìŒì„± ì¸ì‹ì—ì„œë„ ì„±ê³µì ìœ¼ë¡œ ì‚¬ìš©ë˜ì–´ì™”ë‹¤.

- CNN ê¸°ë°˜ì˜ ASR ì„ í–‰ ì—°êµ¬
    - [Jasper: An End-to-End Convolutional Neural Acoustic Model](https://arxiv.org/abs/1904.03288)
    - [QuartzNet: Deep Automatic Speech Recognition with 1D Time-Channel Separable Convolutions](https://arxiv.org/abs/1910.10261)
    - [ContextNet: Improving Convolutional Neural Networks for Automatic Speech Recognition with Global Context](https://arxiv.org/abs/2005.03191)

## ê¸°ì¡´ end-to-end ë°©ì‹ì˜ í•œê³„

ì•ì„œ, ìµœê·¼ì˜ end-to-end ìë™ ìŒì„± ì¸ì‹ ëª¨ë¸ë¡œì¨ transformerì™€ CNNì´ ë§ì´ í™œìš©ëœë‹¤ê³  í•˜ì˜€ìœ¼ë‚˜, ì´ ë‘ ëª¨ë¸ ëª¨ë‘ ê·¼ë³¸ì ìœ¼ë¡œ í•œê³„ì ì´ ì¡´ì¬í•œë‹¤. transformerì˜ ê²½ìš° ê¸´ ê¸¸ì´ì˜ contextë¥¼ `global`í•˜ê²Œ íŒŒì•…í•  ìˆ˜ ìˆëŠ” ë°˜ë©´, ì§€ì—­ì ì¸ íŒ¨í„´ ì •ë³´ë¥¼ ì„¬ì„¸í•˜ê²Œ íŒŒì•…í•˜ëŠ” ë°ëŠ” ê°•ì ì´ ìˆì§€ ì•Šë‹¤. ë°˜ë©´, CNNì€ ì§€ì—­ì ì¸(`local`) íŒ¨í„´ ì •ë³´ë¥¼ íŒŒì•…í•˜ëŠ” ë°ëŠ” íƒì›”í•˜ì§€ë§Œ, ì§€ì—­ì ì¸ ì •ë³´ë¥¼ í™œìš©í•´ì„œ global íŒ¨í„´ ì •ë³´ë¥¼ íŒŒì•…í•˜ê¸° ìœ„í•´ì„œëŠ” ì•„ì£¼ ë§ì€ ë ˆì´ì–´ì™€ ê·¸ì— ë”°ë¥¸ íŒŒë¼ë¯¸í„°ê°€ í•„ìš”í•˜ë‹¤.

ë”°ë¼ì„œ conformerì˜ ì €ìë“¤ì€ transformerì˜ ì¥ì ê³¼ CNNì˜ ì¥ì ì„ í•œ ë° ëª¨ì•„ ê·¸ íš¨ê³¼ë¥¼ ê·¹ëŒ€í™” í•  ìˆ˜ ìˆë„ë¡ ìë™ ìŒì„± ì¸ì‹ ëª¨ë¸ì— transformerì˜ self-attentionê³¼ convolution ì—°ì‚°ì„ í•©ì³ì„œ ì‚¬ìš©í•  ê²ƒì„ ì œì•ˆí•œë‹¤. ì¦‰, conformerë€ **CON**volution + trans**FORMER**ë¼ëŠ” ì´ë¦„ì—ì„œë„ ì•Œ ìˆ˜ ìˆë“¯ì´ transformerì™€ CNNì˜ ì¥ì ì„ í•©ì¹œ ëª¨ë¸ì´ë‹¤.

---

# 02. Conformer ëª¨ë¸

conformerëŠ” `conformer encoder`ì™€ `LSTM decoder`ë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤. LSTM decoderëŠ” ë‹¨ìˆœíˆ ê¸€ìë“¤ì˜ sequenceë¥¼ ì¶œë ¥í•˜ê¸° ìœ„í•œ ê²ƒì´ë¯€ë¡œ, ì—¬ê¸°ì„œëŠ” conformer encoder ìœ„ì£¼ë¡œ ì‚´í´ë³´ê² ë‹¤. conformer encoderì˜ ì „ì²´ì ì¸ ì•„í‚¤í…ì²˜ëŠ” [ê·¸ë¦¼01]ê³¼ ê°™ë‹¤.

![fig01](/assets/img/2022-07-28-Conformer/fig01.png){: width="500" height="500"}
*[ê·¸ë¦¼01] conformerì˜ encoder*


conformerì˜ encoderëŠ” ASR ë°ì´í„° ì¦ê°• ê¸°ë²•ì¸ [`SpecAug`](https://www.isca-speech.org/archive_v0/Interspeech_2019/pdfs/2680.pdf)ë¥¼ ê±°ì¹¨ìœ¼ë¡œì¨ ì‹œì‘ëœë‹¤. ìŒì„± ì‹ í˜¸ëŠ” SpecAug ë ˆì´ì–´ë¥¼ ê±°ì¹œ í›„, `Convolution Subsampling` ë ˆì´ì–´ë¥¼ í†µí•´ íŠ¹ì§•ì„ ì¶”ì¶œí•˜ê³ , `Linear` ë ˆì´ì–´ ë° `Dropout`ì„ ê±°ì³ `Conformer Block`ì„ í†µê³¼í•¨ìœ¼ë¡œì¨ encodingëœë‹¤. ì—¬ê¸°ì„œì˜ í•µì‹¬ì€ ë‹¨ì—° conformer blockì´ë‹¤.

## Conformer Block

![fig02](/assets/img/2022-07-28-Conformer/fig02.png)
*[ê·¸ë¦¼02] transformer blockì„ ë³€í˜•í•œ (ì¢Œ)macaron-netê³¼ (ìš°)conformer block*

conformer blockì€ `macaron-like`í•œ ë°©ì‹ìœ¼ë¡œ transformer ë¸”ë¡ì„ ë³€í˜•í•œë‹¤. macaron-like í•˜ë‹¤ë¼ëŠ” ë§ì€ transformerë¥¼ numerical ODE solverì˜ ê´€ì ì—ì„œ í•´ì„í•˜ê³ ì í–ˆë˜ ì—°êµ¬ì¸ `[macaron-net]`(https://arxiv.org/abs/1906.02762) ë°©ì‹ì„ ì°¨ìš©í–ˆìŒì„ ì˜ë¯¸í•œë‹¤. ì´ëŠ” ê¸°ì¡´ì˜ `Multi-Head Self Attention(MHSA)` â†’ `Position-wise Feed Forward(FFN)`ë¡œ ì´ì–´ì§€ëŠ” transformer block êµ¬ì¡°ë¥¼ ë§ˆì¹˜ ë§ˆì¹´ë¡± ëª¨ì–‘ì²˜ëŸ¼ `FFN` â†’ `MHSA` â†’ `FFN` ë°©ì‹ìœ¼ë¡œ ë³€ê²½í•œ ê²ƒì´ë‹¤. ì´ë•Œ FFNì—ì„œëŠ” `half-step residual weights`ë¼ëŠ” ë°©ì‹ìœ¼ë¡œ ë…íŠ¹í•˜ê²Œ residual connectionì„ êµ¬ì„±í•œë‹¤. $\tilde{x}_i = x_i + \cfrac{1}{2} \text{FFN}(x_i)$ì™€ ê°™ì´ residual connectionì— 1/2ë§Œí¼ì˜ weightë¥¼ ì£¼ëŠ” ë°©ì‹ì´ë‹¤.

macaron-likeí•œ conformerëŠ” `FFN` â†’ `MHSA` â†’ `Convolution Module` â†’ `FFN`ë¡œ ì´ë£¨ì–´ì ¸ìˆìœ¼ë©°, ì•„ë˜ì™€ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.

$\tilde{x}_i = x_i + \cfrac{1}{2} \text{FFN}(x_i)$

$x'_i = \tilde{x}_i + \text{MHSA}(\tilde{x}_i)$

$x'\'_i=x'_i + \text{Conv}(x'_i)$

$y_i = \text{Layernorm}(x'\'_i + \cfrac{1}{2} \text{FFN}(x'\'_i))$

$x_i$ëŠ” $i$ë²ˆì§¸ conformer blockì˜ ì…ë ¥ê°’ì„ ì˜ë¯¸í•˜ê³  $y_i$ëŠ” encodingëœ ì¶œë ¥ê°’ì„ ì˜ë¯¸í•œë‹¤. ê·¸ëŸ¼, macaron-like FFN ì‚¬ì´ì— ìˆëŠ” ê°ê°ì˜ ëª¨ë“ˆì— ëŒ€í•´ì„œ ì‚´í´ë³´ë„ë¡ í•˜ê² ë‹¤.

## Multi-Head Self-Attention(MHSA) Module

![fig03](/assets/img/2022-07-28-Conformer/fig03.png)
*[ê·¸ë¦¼03] conformerì˜ multi-head self-attention ëª¨ë“ˆ*

ë¨¼ì €, MHSA ëª¨ë“ˆì„ ì‚´í´ë³´ê² ë‹¤. ê¸°ë³¸ì ì¸ transformerì˜ MHSAì™€ëŠ” ë‹¬ë¦¬, Transformer-XLì˜ relative positional embedding ê¸°ë²•ì„ ì ìš©í•œ MHSAë¥¼ ì‚¬ìš©í•˜ì˜€ë‹¤.

ë‹¤ìŒì€ Transformer-XLì˜ relative positional embeddingì— ëŒ€í•œ ì„¤ëª…ì´ë‹¤.

![fig04](/assets/img/2022-07-28-Conformer/fig04.png){: width="500" height="500"}
![fig05](/assets/img/2022-07-28-Conformer/fig05.png){: width="500" height="500"}
![fig06](/assets/img/2022-07-28-Conformer/fig06.png){: width="500" height="500"}
![fig07](/assets/img/2022-07-28-Conformer/fig07.png){: width="500" height="500"}

ì €ìë“¤ì— ë”°ë¥´ë©´, relative positional embeddingì„ ì ìš©í•˜ë©´ self-attention ëª¨ë“ˆì´ ê¸¸ì´ê°€ ë‹¤ë¥¸ ì…ë ¥ê°’ì— ëŒ€í•´ì„œë„ ì˜ generalize í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ë°œí™”ì˜ ê¸¸ì´ê°€ ìƒì´í•˜ë”ë¼ë„ ê°•ê±´í•œ encoderê°€ ë  ìˆ˜ ìˆë‹¤ê³  í•œë‹¤. ì´ì²˜ëŸ¼ relative positional embeddingì„ ì ìš©í•˜ì—¬ MHSAì„ ìˆ˜í–‰ë˜ê³ ë‚˜ë©´ dropoutì„ ê±°ì³ residual connection ëœ í›„ ë‹¤ìŒ ë‹¨ê³„ì¸ Convolution Moduleë¡œ ë„˜ì–´ê°€ê²Œ ëœë‹¤.

## Convolution Module

![fig08](/assets/img/2022-07-28-Conformer/fig08.png)
*[ê·¸ë¦¼04] conformerì˜ convolution ëª¨ë“ˆ*

MHSA ëª¨ë“ˆì„ ê±°ì¹œ representationì€ Convolution Moduleë¡œ ë„˜ì–´ì˜¨ë‹¤. Convolution Moduleì—ì„œëŠ” Conv ì—°ì‚°ì„ í†µí•´ì„œ ìŒì„± ì‹ í˜¸ representationì˜ ì§€ì—­ì ì¸ ì •ë³´ë¥¼ í•™ìŠµí•œë‹¤. Convolution Moduleë¡œ ë„˜ì–´ì˜¨ ìŒì„± ì‹ í˜¸ representationì€  Layernormì„ ê±°ì³ [Pointwise Conv](https://paperswithcode.com/method/pointwise-convolution) â†’ [Glu Activation](https://paperswithcode.com/method/glu) â†’ [1D DepthwiseConv](https://paperswithcode.com/method/depthwise-convolution) â†’ BatchNorm â†’ [Swish Activation](https://paperswithcode.com/method/swish) â†’ Pointwise Conv â†’ Dropoutì„ ê±°ì¹œë‹¤. ê·¸ë¦¬ê³  ë‹¤ì‹œê¸ˆ FFN moduleë¡œ í˜ëŸ¬ê°„ë‹¤.

## Feed Forward Module

![fig09](/assets/img/2022-07-28-Conformer/fig09.png)
*[ê·¸ë¦¼05] conformerì˜ feed forward ëª¨ë“ˆ*

FFN ëª¨ë“ˆì— ëŒ€í•œ ì„¸ë¶€ ì‚¬í•­ë“¤ì€ [ê·¸ë¦¼05]ì™€ ê°™ë‹¤. ë‹¤ë¥¸ ëª¨ë“ˆì—ì„œì™€ ë™ì¼í•˜ê²Œ Layernormì€ ê¸°ë³¸ì ìœ¼ë¡œ ì ìš©í•œ í›„, 2íšŒì˜ Linear Layer ë“±ì„ í†µê³¼í•˜ëŠ” êµ¬ì¡°ë‹¤.

---

# 03. Experiments

970 ì‹œê°„ì˜ labeled speech datasetì¸ LibriSpeechë¥¼ ì´ìš©í•´ì„œ evaluate í•˜ì˜€ë‹¤. ì¶”ê°€ì ìœ¼ë¡œ í™œìš©í•œ ì–¸ì–´ëª¨ë¸ì„ í•™ìŠµí•˜ê¸° ìœ„í•´ 800M word token text-only corpusë¥¼ ì‚¬ìš©í–ˆë‹¤ê³  í•œë‹¤.

decoderëŠ” ë‹¨ìˆœíˆ í•œ ì¸µì„ ì‚¬ìš©í•œ LSTM decoderë¥¼ ì‚¬ìš©í•˜ì˜€ìœ¼ë©°, í•™ìŠµí•œ ì–¸ì–´ëª¨ë¸ì€ `shallow fusion` ë°©ì‹ìœ¼ë¡œ í™œìš©í–ˆë‹¤. ì°¸ê³ ë¡œ shallow fusionì˜ ê°œë…ì€ [ê·¸ë¦¼06]ê³¼ ê°™ë‹¤.

![fig10](/assets/img/2022-07-28-Conformer/fig10.png){: width="400" height="400"}
*[ê·¸ë¦¼06] shallow fusion ë„ì‹*

$\mathbf{y}^* = \arg \max_{\mathbf{y}} (\log P_{e2e}(\mathbf{y} \| \mathbf{x}) + \lambda \log P_{LM}(\mathbf{y}))$

ì¦‰, ìŒì„± ì‹ í˜¸ sequence $\mathbf{x}$ê°€ ì£¼ì–´ì¡Œì„ ë•Œ `end-to-end(e2e)` ë°©ì‹ìœ¼ë¡œ ê¸€ìì˜ sequence $\mathbf{y}$ë¥¼ decoding í•˜ëŠ” ëª¨ë¸ $P(\mathbf{y} \| \mathbf{x})$(ì—¬ê¸°ì„œëŠ” conformer encoder + LSTM decoder)ê³¼ ë³„ë„ë¡œ í•™ìŠµí•œ ì–¸ì–´ëª¨ë¸(ì—¬ê¸°ì„œëŠ” 3ì¸µì§œë¦¬ LSTM)ì„ í•¨ê»˜ ê³ ë ¤í•˜ì—¬ ìµœì¢… ì¶œë ¥ë  ê¸€ìì˜ í™•ë¥ ì„ ê²°ì •í•˜ëŠ” ê¸°ë²•ì¸ ê²ƒì´ë‹¤.

![fig11](/assets/img/2022-07-28-Conformer/fig11.png){: width="500" height="500"}
*[í‘œ01] conformer ëª¨ë¸ í¬ê¸°ë³„ íŒŒë¼ë¯¸í„°*

conformerë„ ëª¨ë¸ì˜ í¬ê¸°ì— ë”°ë¼ì„œ Conformer(S), Conformer(M), Conformer(L)ë¡œ êµ¬ë¶„í•  ìˆ˜ ìˆìœ¼ë©°, ê°ê°ì˜ íŒŒë¼ë¯¸í„°ëŠ” [í‘œ01]ê³¼ ê°™ë‹¤.

![fig12](/assets/img/2022-07-28-Conformer/fig12.png){: width="500" height="500"}
*[í‘œ02] conformer ëª¨ë¸ì˜ ì„±ëŠ¥ ë¹„êµí‘œ*

[í‘œ02]ëŠ” confermerê°€ ê³µê°œë  ë‹¹ì‹œì˜ SOTA modelì´ì—ˆë˜ ContextNet, Transformer Transducer ë“±ê³¼ LibriSpeech ë°ì´í„°ì…‹ì„ ê¸°ì¤€ìœ¼ë¡œ `Word Error Rate(WER)` ì„±ëŠ¥ì„ ë¹„êµí•œ ê²°ê³¼ì´ë‹¤. ì–¸ì–´ëª¨ë¸ì„ ì‚¬ìš©í•˜ì§€ ì•Šì€ Conformer(M) ëª¨ë¸ë¡œë„ ê¸°ì¡´ì˜ Transformerë‚˜ LSTM, ë˜ëŠ” CNNì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ëª¨ë¸ë³´ë‹¤ ìš°ì›”í•œ ì„±ëŠ¥ì„ ë³´ì„ì„ ì•Œ ìˆ˜ ìˆë‹¤. ì–¸ì–´ëª¨ë¸ì„ ë§ë¶™ì—¬ ì‚¬ìš©í•œ conformer ëª¨ë¸ë“¤ì€ ê°€ì¥ ë‚®ì€ WERì„ ë‹¬ì„±í•˜ë©´ì„œ ê¸°ì¡´ì˜ SOTA ëª¨ë¸ì„ ë›°ì–´ë„˜ì—ˆë‹¤.

---

# 04. ì •ë¦¬í•˜ë©°

![fig13](/assets/img/2022-07-28-Conformer/fig13.png){: width="500" height="500"}
*[ê·¸ë¦¼07] 2022ë…„ 7ì›” ë§ ê¸°ì¤€ LibriSpeech SOTA ë­í‚¹*

[ê·¸ë¦¼07]ì€ í˜„ì¬(2022ë…„ 7ì›” ë§)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ [paperswithcode.com](https://paperswithcode.com/sota/speech-recognition-on-librispeech-test-clean)ì—ì„œ ë°œì·Œí•œ LibriSpeech test-clean ë°ì´í„° ê¸°ì¤€ ìŒì„± ì¸ì‹ SOTA ëª¨ë¸ ë­í‚¹ì´ë‹¤. 2020ë…„ì— ì œì•ˆëœ conformerëŠ” ê·¸ ìì²´ë¡œ ì•„ì§ë„ ìƒìœ„ rankingì— ìˆìœ¼ë©°, í˜„ì¬ ê¸°ì¤€ ìµœê³  ì„±ëŠ¥ ì—­ì‹œ conformerë¥¼ í™œìš©í•œ ëª¨ë¸ì´ë‹¤. global contextë¥¼ íŒŒì•…í•˜ëŠ” transformerì˜ ì¥ì ê³¼ local contextë¥¼ íŒŒì•…í•˜ëŠ” CNNì˜ ì¥ì ì„ í•©ì³¤ë‹¤ëŠ” ì ì´ í™•ì‹¤í•˜ê²Œ ì‘ìš©í•œ ê²ƒ ê°™ë‹¤.

---

# 05. ì°¸ê³  ë¬¸í—Œ

[1] `ì› ë…¼ë¬¸` : [Gulati, Anmol, et al. "Conformer: Convolution-augmented transformer for speech recognition." arXiv preprint arXiv:2005.08100 (2020).](https://arxiv.org/abs/2005.08100)<br>
[2] `Macaron-Net` : [Lu, Yiping, et al. "Understanding and improving transformer from a multi-particle dynamic system point of view." arXiv preprint arXiv:1906.02762 (2019).](https://arxiv.org/abs/1906.02762)<br>
[3] `Shallow Fusion` : [Cabrera, Rodrigo, et al. "Language model fusion for streaming end to end speech recognition." arXiv preprint arXiv:2104.04487 (2021).](https://www.arxiv-vanity.com/papers/2104.04487/)
