---
title: (NLP) BERTopic 개념 정리
author: simon sanghyeon
date: 2022-09-21
categories: [Natural Language Processing]
tags: [Language Model, NLP, NLU, Paper Review]
render_with_liquid: true
use_math: true
---
이 포스트는 개인적으로 공부한 내용을 정리하고 필요한 분들에게 지식을 공유하기 위해 작성되었습니다. 지적하실 내용이 있다면, 언제든 댓글 또는 메일로 알려주시기를 바랍니다.

---

# 01. Introduction

전통적인 토픽 모델링의 방법으로는 `Latent Dirichlet Allocation(LDA)`와 `Non-Negative Matrix Factorization(NMF)`가 주로 사용된다. 이들은 document를 단어의 출현 빈도 기반으로 다루는 표현하는 방식인 `Bag-of-Words(BoW)`로써 설명한다는 특징이 있다. 따라서 모델이 단어들 간의 의미론적 관계를 포착하지 못하는 한계가 있다. 한편, BERT와 같은 트랜스포머 인코더 기반의 모델은 단어들 간의 양방향 문맥이 반영된 representation을 생성할 수 있다는 장점이 있다.

`BERTopic`은 이러한 양방향의 의미를 파악할 수 있는 BERT의 장점을 토픽 모델링 태스크에 활용하고자 했다. 이를 위해, BERTopic은 사전 학습된 트랜스포머 기반 언어 모델(i.e., BERT)로부터 `(1)document의 정보를 파악한 임베딩을 생성`하고, 해당 임베딩으로 `(2)차원 축소 및 클러스터링`을 수행한 후, `(3)class-based TF-IDF`를 통해 토픽의 representation을 생성한다.

# 02. Document Embeddings

BERTopic은 벡터 공간 상에서 가까운 document는 의미론적으로 연관성이 있는 주제를 다룬다는 가정 하에, document를 벡터 공간 상에 임베딩 하였다. 임베딩 방법론으로는 BERT의 문장 임베딩 성능을 우수하게 개선시킨 `Sentence-BERT(SBERT)`를 활용했다. 논문에서는 SBERT를 사용하였으나, 이외에도 distil-BERT 등 어떠한 임베딩 방법을 사용하든 무관하다고 한다.

아래의 코드는 distil-BERT로 임베딩을 구하는 예다. PyTorch 설치 이후, `pip install sentence-transformers`로 패키지를 설치하여 실행할 수 있다.

```python
from sentence_transformers import SentenceTransformer
model = SentenceTransformer('distilbert-base-nli-mean-tokens')
embeddings = model.encode(data, show_progress_bar=True)
```

*출처 : 공식 포스트*

# 03. Document Clustering

유사한 토픽을 가지는 document는 임베딩 공간 상에 가까이 위치하도록 클러스터링 하고자 한다.

클러스터링에 앞서, 데이터의 차원이 크다면, 데이터 포인트 간의 최근접 거리가 멀어지는 `차원의 저주`가 발생하게 된다. (Allaoui et al., 2020)에 따르면, UMAP을 활용한 차원 축소 방식은 저차원 공간에서도 local 구조를 잘 보존할 수 있기 때문에 k-Means나 HDBSCAN 클러스터링과 함께 사용하면 성능 향상에 효과적으로 기여할 수 있다고 한다.

이에 BERTopic의 저자들은 고차원 데이터의 local 및 global 특징 모두를 잘 보존할 수 있는 `UMAP`을 활용하여 document 임베딩의 차원을 축소하였다. 이후, 차원이 축소된 document 임베딩에 대해서 DBSCAN 알고리즘의 계층적(hierarchical) 버전인 `HDBSCAN 알고리즘`을 적용하여 클러스터링 하였다.

아래의 코드는 UMAP을 활용하여 차원 축소된 임베딩을 구하는 예다. `pip install umap-learn` 명령으로 패키지를 설치할 수 있다.

```python
import umap
umap_embeddings = umap.UMAP(n_neighbors=15,
                            n_components=5,
                            metric='cosine').fit_transform(embeddings)
```

*출처 : 공식 포스트*

또한 아래의 코드는 HDBSCAN을 통해 클러스터링을 수행하는 예다. `pip install hdbscan` 명령으로 관련 패키지를 설치할 수 있다.

```python
import hdbscan
cluster = hdbscan.HDBSCAN(min_cluster_size=15,
                          metric='euclidean',
                          cluster_selection_method='eom').fit(umap_embeddings)
```

*출처 : 공식 포스트*

# 04. Topic Representation through Class-based TF-IDF

클러스터링을 완료하면, 각 클러스터마다 하나의 토픽이 배정될 것이다. 한편, 우리가 알고 싶은 정보는 결국 ‘각 클러스터들이 다른 클러스터와 어떤 점에서 다른 것이고, 각 document 클러스터에서 어떻게 토픽을 도출해내는가’라고 할 수 있다. 이에 BERTopic의 저자는 TF-IDF의 구조를 변형한 `class-based TF-IDF`를 제안한다. class-based TF-IDF는 개별 document가 아닌, document 클러스터(토픽) 관점에서 각 단어들의 중요도를 모델링한다. 즉, 각 클러스터마다 `topic-word distribution`을 구할 수 있는 것이다.

## TF-IDF

class-based TF-IDF를 보기 위해서 우선 TF-IDF부터 살펴보자. TF-IDF는 document 내에서 단어의 빈도수를 고려하여 중요한 정도를 가중치로 주는 방법으로,

$W_{t,d}=tf_{t,d} \cdot \log(\cfrac{N}{df_t})$ 와 같이 표현할 수 있다.

우선, $tf_{t, d}$는 특정 document $d$에서 특정 단어 $t$의 등장 횟수를 의미한다. 즉, **특정 단어 $t$가 ‘특정’ document에서만 많이 나오면 TF-IDF 값이 커진다.**

또한 log term 안의 분모에 위치한 $df_t$는 특정 단어 $t$가 등장한 document의 수를 의미하고, 분자에 위치한 $N$은 총 document의 수를 의미한다. 따라서 분모에 위치한 $df_t$를 고려하면, **특정 단어 $t$가 ‘여러’ document 전반에 걸쳐서 많이 나오면 TF-IDF 값이 작아진다.** 예를 들어, “a”나 “the”와 같은 단어의 경우 모든 document에서 많이 나오므로 그 값이 작다.

## Class-based TF-IDF

class-based TF-IDF는 document들로 클러스터링 된 군집들에 대하여 TF-IDF의 개념을 적용하기 위해 변형된 형태라고 볼 수 있으며, $W_{t, c}=tf_{t, c} \cdot \log(1 + \cfrac{A}{tf_t})$로 정의한다.

우선 class-based TF-IDF는 TF-IDF와 달리 $tf_{t, c}$를 사용한다. class-based TF-IDF에서는 클러스터 내 모든 document들을 concatenate 함으로써 단 하나의 `single document`로 간주하는데, $tf_{t, c}$는 특정한 ‘하나’의 클러스터(i.e., class), 다시 말해 concatenate 되어있는 모든 document들에서 특정 단어 $t$가 등장한 횟수를 의미한다. 따라서 **특정 단어 $t$가 ‘특정’ 클러스터에서만 많이 나오면 class-based TF-IDF 값이 커진다.**

또한 log term 안의 분모에 위치한 $tf_t$는 ‘모든’ class에서 단어 $t$의 등장 횟수를 의미하고, 분자에 위치한 $A$는 클래스 당 평균 단어 수를 의미한다. 따라서 분모에 위치한 $tf_t$를 고려하면, **특정 단어 $t$가 ‘여러’ 클러스터 전반에 걸쳐서 많이 나오면 class-based TF-IDF 값이 작아진다.** 참고로 여기서 1을 더한 이유는 이 값이 반드시 양수만 가질 수 있도록 하기 위함이다.

이러한 과정을 마친 후, 자잘한 class-based TF-IDF representation을 사용자가 지정한 토픽의 수만큼 가장 가까운 representation과 반복적으로 merge 함으로써 토픽의 수를 조정할 수 있다.

# 참고 문헌
[1] `원 논문` : [BERTopic: Neural topic modeling with a class-based TF-IDF procedure](https://arxiv.org/pdf/2203.05794.pdf)<br>
[2] `저자의 블로그` : [Topic Modeling with BERT](https://towardsdatascience.com/topic-modeling-with-bert-779f7db187e6)
