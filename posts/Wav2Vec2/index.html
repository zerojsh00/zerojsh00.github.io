<!DOCTYPE html><html lang="ko" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.3.2" /><meta property="og:title" content="(Speech Recognition) Wav2Vec2.0 리뷰 및 설명" /><meta name="author" content="simon sanghyeon" /><meta property="og:locale" content="ko" /><meta name="description" content="이 포스트는 개인적으로 공부한 내용을 정리하고 필요한 분들에게 지식을 공유하기 위해 작성되었습니다. 지적하실 내용이 있다면, 언제든 댓글 또는 메일로 알려주시기를 바랍니다." /><meta property="og:description" content="이 포스트는 개인적으로 공부한 내용을 정리하고 필요한 분들에게 지식을 공유하기 위해 작성되었습니다. 지적하실 내용이 있다면, 언제든 댓글 또는 메일로 알려주시기를 바랍니다." /><link rel="canonical" href="https://zerojsh00.github.io/posts/Wav2Vec2/" /><meta property="og:url" content="https://zerojsh00.github.io/posts/Wav2Vec2/" /><meta property="og:site_name" content="Simon’s Research Center" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-07-31T00:00:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="(Speech Recognition) Wav2Vec2.0 리뷰 및 설명" /><meta name="twitter:site" content="@" /><meta name="twitter:creator" content="@simon sanghyeon" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"simon sanghyeon"},"dateModified":"2022-07-31T21:54:34+09:00","datePublished":"2022-07-31T00:00:00+09:00","description":"이 포스트는 개인적으로 공부한 내용을 정리하고 필요한 분들에게 지식을 공유하기 위해 작성되었습니다. 지적하실 내용이 있다면, 언제든 댓글 또는 메일로 알려주시기를 바랍니다.","headline":"(Speech Recognition) Wav2Vec2.0 리뷰 및 설명","mainEntityOfPage":{"@type":"WebPage","@id":"https://zerojsh00.github.io/posts/Wav2Vec2/"},"url":"https://zerojsh00.github.io/posts/Wav2Vec2/"}</script><title>(Speech Recognition) Wav2Vec2.0 리뷰 및 설명 | Simon's Research Center</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Simon's Research Center"><meta name="application-name" content="Simon's Research Center"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get MODE_ATTR() { return "data-mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.notify(); } /* flipMode() */ </script> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "AMS" } }, tex2jax: { inlineMath: [ ['$', '$'] ], displayMath: [ ['$$', '$$'] ], processEscapes: true, } }); MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) { alert("Math Processing Error: "+message[1]); }); MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) { alert("Math Processing Error: "+message[1]); }); </script> <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script><meta name="google-site-verification" content="J_a4XJ2oefe52p6EYvjTFdh9LW5Yc5RyGg1HpQOjWaA" /><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" class="mx-auto"> <img src="/assets/img/avatar.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Simon's Research Center</a></div><div class="site-subtitle font-italic">성장하는 연구원의 공책</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <button class="mode-toggle btn" aria-label="Switch Mode"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/zerojsh00" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['zerojsh00','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>(Speech Recognition) Wav2Vec2.0 리뷰 및 설명</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>(Speech Recognition) Wav2Vec2.0 리뷰 및 설명</h1><div class="post-meta text-muted"><div> By <em> Sanghyeon </em></div><div class="d-flex"><div> <span> Posted <em class="timeago" data-ts="1659193200" data-toggle="tooltip" data-placement="bottom" data-tooltip-df="llll" > 2022-07-31 </em> </span> <span> Updated <em class="timeago" data-ts="1659272074" data-toggle="tooltip" data-placement="bottom" data-tooltip-df="llll" > 2022-07-31 </em> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="3341 words"> <em>18 min</em> read</span></div></div></div><div class="post-content"><p>이 포스트는 개인적으로 공부한 내용을 정리하고 필요한 분들에게 지식을 공유하기 위해 작성되었습니다. 지적하실 내용이 있다면, 언제든 댓글 또는 메일로 알려주시기를 바랍니다.</p><hr /><h1 id="00-들어가며">00. 들어가며</h1><p>2020년, Facebook에서 Wav2Vec 2.0을 발표했다. 앞서 살펴보았던 <a href="https://zerojsh00.github.io/posts/Wav2Vec/">Wav2Vec</a> 및 <a href="https://zerojsh00.github.io/posts/VQWav2Vec/">VQ-Wav2Vec</a>과 마찬가지로, Wav2Vec 2.0 역시 음성 신호가 전사되어있는 labeled data가 부족하다는 이슈에 대해 <code class="language-plaintext highlighter-rouge">self-supervised learning</code> 기법으로 pre-training 하는 방법론을 제시한다. pre-training이 완료된 Wav2Vec 2.0은 이후 <a href="https://zerojsh00.github.io/posts/Connectionist-Temporal-Classification/"><code class="language-plaintext highlighter-rouge">Connectionist Temporal Classification(CTC) loss</code></a>를 활용해서 적은 양의 labeld data로 fine-tuning 하여 활용할 수 있다.</p><p>놀라운 것은 Wav2Vec 2.0으로 pre-training된 모델이 단지 10분 분량의 labeled data만으로 fine-tuning 되었을 때 Librispeech 데이터셋 기준으로 <code class="language-plaintext highlighter-rouge">Word Error Rate(WER)</code>이 깨끗한 음성에 대해서는 4.8을, 이외의 음성에 대해서는 8.2를 기록했다는 점이다. 즉, 매우 적은 양의 데이터만 있으면 어느 정도 동작하는 음성 인식기를 쉽게 만들 수 있게 되었다는 점에서 큰 의의가 있는 것이다.</p><p><img data-src="/assets/img/2022-07-31-Wav2Vec2/fig01.png" alt="fig01" data-proofer-ignore> <em>[그림01] 학습 시간에 따른 Wav2Vec2.0의 성능</em></p><p>한편, TIMIT phoneme recognition 문제 등에서 SOTA를 달성했으며, labeled data를 더욱 줄여서 오로지 1시간의 labeled data만으로 fine-tuning 했음에도 100배 많은 labeled data로 학습한 기존의 SOTA self-training 방식 모델보다 더욱 나은 성능을 보였다. 게다가 960 시간의 모든 Libreespeech의 labeled data를 활용했을 때는 깨끗한 음성의 경우 WER이 1.8, 그 이외에 대해서는 3.3을 달성하였다.</p><p>세상에는 다양한 방언 뿐만 아니라 7,000 여 개나 되는 언어가 존재한다. Wav2Vec 2.0은 이렇게 다양한 언어에 대해서도 매우 적은 양의 데이터만 있으면 높은 정확도를 보이는 음성 인식 모델을 구축할 수 있는 세상을 열었다. 그렇다면, 과연 어떻게 작동하는지 Wav2Vec 2.0을 살펴보겠다.</p><hr /><h1 id="01-모델">01. 모델</h1><p><img data-src="/assets/img/2022-07-31-Wav2Vec2/fig02.png" alt="fig02" data-proofer-ignore> <em>[그림02] pre-training 과정에서의 Wav2Vec 2.0 모델 아키텍처</em></p><p>[그림02]은 pre-training 과정에서의 Wav2Vec 2.0 모델 아키텍처를 보여준다.</p><h2 id="feature-encoder"><span class="mr-2">Feature Encoder</span><a href="#feature-encoder" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>기존 Wav2Vec 모델들과 마찬가지로 <code class="language-plaintext highlighter-rouge">feature encoder</code> 네트워크 $f:\mathcal{X} \mapsto \mathcal{Z}$가 존재한다. multi-layer CNN으로 구성된 이 네트워크는 원시 음성 신호 sequence 입력값인 $\mathcal{X}$를 입력 받아서 매 $T$ 시점마다 <code class="language-plaintext highlighter-rouge">latent speech representation</code>인 $\mathbf{z}_1, … , \mathbf{z}_T$를 출력한다. 이후 latent speech representation $\mathbf{z}_1$, … , $\mathbf{z}_T$는 두 모듈에 각각 나뉘어서 입력값으로 흘러간다.</p><h2 id="contexualized-representations-with-transformers"><span class="mr-2">Contexualized Representations with Transformers</span><a href="#contexualized-representations-with-transformers" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>latent speech representation이 입력되는 한 모듈은 <code class="language-plaintext highlighter-rouge">contextulized representation을 위한 transformer 모듈</code> $g:\mathcal{Z} \mapsto \mathcal{C}$이다. 즉, $\mathbf{z}_1, … , \mathbf{z}_T$ sequence가 입력되면, transformer 블록(transformer encoder 블록)에 의해 sequence 내 모든 맥락 정보가 파악된 $\mathbf{c}_1, … , \mathbf{c}_T$ sequence가 출력된다.</p><p>특징으로는, transformer 블록에서는 기본적인 absolute positional embedding을 사용하지 않고, convolution 연산을 통해서 <code class="language-plaintext highlighter-rouge">relative positional embedding</code>과 유사한 효과를 주었다고 한다.</p><h2 id="quantization-module"><span class="mr-2">Quantization Module</span><a href="#quantization-module" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p><img data-src="/assets/img/2022-07-31-Wav2Vec2/fig03.png" alt="fig03" data-proofer-ignore> <em>[그림03] G개의 codebook, V개의 code words를 활용한 Wav2Vec 2.0의 quantization 모듈</em></p><p>latent speech representation $\mathbf{z}$가 입력되는 또 다른 모듈은 <code class="language-plaintext highlighter-rouge">quantization 모듈</code> $\mathcal{Z} \mapsto \mathcal{Q}$ 이다. vector quantization의 개념이 헷갈리면 <a href="https://zerojsh00.github.io/posts/Vector-Quantization/">[여기]</a>를 참고하면 좋을 것이다. [그림03]의 중앙을 보면 <code class="language-plaintext highlighter-rouge">codebook 행렬</code> $e \in \mathbb{R}^{V \times d / G}$이 $G$개 존재하여 $G \times V$ 크기의 multiple codebooks를 이루고 있다. 즉, 하나의 codebook은 [그림03] 중앙에서 보여지는 $V$개의 <code class="language-plaintext highlighter-rouge">code word 벡터</code>(작은 네모 한 개) sequence 한 행에 대응된다. 이들은 모두 학습 가능한 파라미터로 구현된다. 즉, codebook을 embedding matrix로, code word를 embedding 벡터로 생각하자. (참고로 저자는 $G=2$, $V=320$을 사용했다.)</p><p>이러한 세팅 하에서 quantization 모듈에서 일어나는 일들을 설명해보겠다. encoding 된 $z_t$가 주어졌을 때, $z_t$는 레이어를 통과하여 logit으로 변환되고, gumbel softmax(<a href="https://zerojsh00.github.io/posts/VQWav2Vec/">참고</a>) 및 argmax를 통해 one-hot encoding 된 후(즉, <code class="language-plaintext highlighter-rouge">이산화 과정</code>), 마치 NLP에서 embedding matrix에서 특정 단어에 해당하는 embedding 벡터를 뽑아내듯, 각 codebook 내에서 하나의 code word 벡터를 골라낸다. code word 벡터는 $G$개의 codebook 행렬에서 각각 하나씩 추출됨에 따라 총 $G$개의 $e_{1}, …, e_{G} \in \mathbb{R}^{d / G}$ 벡터들로 추출될 것이다. 이후, $G$개의 벡터 모두를 concatenate 하여 $e_{t} \in \mathbb{R}^{d}$를 만든다([그림03]에서의 세번째 과정). 여기서 linear transformation $\mathbb{R}^{d} \mapsto \mathbb{R}^{f}$을 통해서 quantized representation $q_{t} \in \mathbb{R}^{f}$를 최종적으로 만들어낸다.([그림03]에서의 마지막 과정)</p><h3 id="codebook과-code-word의-의미"><span class="mr-2">codebook과 code word의 의미</span><a href="#codebook과-code-word의-의미" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>복잡하다. 그런데 도대체 codebook과 code word가 의미는 것은 무엇일까? code word는 일종의 <code class="language-plaintext highlighter-rouge">음소(phoneme)에 대한 representation</code>이라고 생각하면 될 것 같다. 아무리 언어가 다르더라도 사람이 발음할 수 있는 음소는 사실상 유한하다고 봐도 좋기 때문에, 이를 마치 embedding 벡터로 표현한 것이 code word인 셈이다. 또한 이들이 모여 만들어진 행렬이 codebook인 셈이다. 즉, $V$개의 음소 중 현 시점에 음성 encoding 벡터 $z_t$와 가장 대응될 음소 벡터를 이산화 과정을 통해 골라낸 것이 $q_t$라고 할 수 있다.</p><h3 id="수식으로-표현한-gumbel-softmax"><span class="mr-2">수식으로 표현한 gumbel softmax</span><a href="#수식으로-표현한-gumbel-softmax" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>원 논문에 나와 있는 수식으로 다시 살펴보겠다. encoder를 거쳐 나온 $\mathbf{z}$는 레이어를 통과하여 로짓으로 $\mathbf{l} \in \mathbb{R}^{G \times V}$ 변환된다. 이때 $G$개의 multiple codebook을 사용하므로, $g$번째 codebook에서 $v$번째 code word 벡터가 선택될 확률을 gumbel softmax로 표현하면 다음과 같다.</p><p>$p_{g, v} = \cfrac{\exp(l_{g, v} + n_v) / \tau }{\sum_{k=1}^{V} \exp (l_{g, k} + n_k) / \tau}$</p><p>이때 $\tau$는 gumbel softmax의 non-negative temperature, $n=- \log ( \log (u) )$이며, $u$는 uniform distribution $\mathcal{U}(0, 1)$에서 랜덤하게 sampling된 값이다. gumbel softmax의 특징답게, forward pass에서는 확률 값에 대한 argmax를 통해 나온 index에 해당하는 codeword가 선택되지만, backward pass에서는 gumbel softmax에 대한 기울기가 계산되어 학습된다.</p><hr /><h1 id="02-학습-방식">02. 학습 방식</h1><p>Wav2Vec 2.0은 pre-training 과정에서 BERT와 유사하게 마스킹 기법을 도입한 것이 특징이다. 학습의 목적식은 마스킹된 부분에 해당되는 quantized representation인지, 다른 부분에 해당되는 quantized representation인지를 구분하는 문제로 구성되어 있다. pre-training을 마치면, labeled data로 fine-tuning이 진행된다.</p><h2 id="마스킹"><span class="mr-2">마스킹</span><a href="#마스킹" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>마스킹 기법을 살펴보겠다. 마스킹은 feature encoder의 출력값인 $\mathbf{z}$를 transformer block인 context network에 입력하기 전에 수행된다. transformer block의 self-attention 효과를 보기 위해 마스킹을 적용하는 것이므로, 당연히 quantization 모듈의 입력값에는 마스킹을 수행하지 않는다.</p><p>그 과정은 다음과 같다.</p><p><img data-src="/assets/img/2022-07-31-Wav2Vec2/fig04.png" alt="fig04" data-proofer-ignore> <em>[그림04] masking 인덱스의 시작점을 선택함</em></p><p><img data-src="/assets/img/2022-07-31-Wav2Vec2/fig05.png" alt="fig05" data-proofer-ignore> <em>[그림05] masking을 진행함</em></p><p>우선, [그림04]와 같이 $p=0.065$의 확률로 $\mathbf{z}$ sequence를 마스킹의 시작점으로 선정한다. 그리고는 [그림05]와 같이 $M=10$ 만큼 연달아 마스킹을 수행한다. 이때, $p$와 $M$은 하이퍼파라미터이다. 시작 지점에 따라서 당연히 마스킹이 겹치는 경우가 생길 수 있다. 마스킹은 <code class="language-plaintext highlighter-rouge">trained feature vector</code>로 마스킹되는 부위를 대체하는 방식이며, 모든 마스킹 부위는 동일하게 해당 feature vector를 사용한다.</p><h2 id="목적식loss-function"><span class="mr-2">목적식(Loss Function)</span><a href="#목적식loss-function" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>$\mathcal{L} = \mathcal{L}_{m} + \alpha \mathcal{L}_d$</p><p>loss function은 위와 같이 정의된다. $\mathcal{L}_m$은 다른 Wav2Vec 버전들과 유사하게 pre-training 과정에서 계산되는 <code class="language-plaintext highlighter-rouge">contrastive loss</code> term이다. 추가로 $\mathcal{L}_d$ term이 더해져 있는데, 이는 <code class="language-plaintext highlighter-rouge">diversity loss</code>라고 부른다. 이때 $\alpha$는 하이퍼파라미터이다.</p><h3 id="contrastive-loss"><span class="mr-2">Contrastive Loss</span><a href="#contrastive-loss" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>$\mathcal{L}_m = - \log \cfrac{ \exp{ (sim(\mathbf{c}_t , \mathbf{q}_t)/ \mathcal{κ}) } }{ \sum _ {\tilde{\mathbf{q}} \sim \mathbf{Q}_t} \exp{(sim(\mathbf{c}_t , \tilde{\mathbf{q}})/\mathcal{κ})}}$</p><p>contrastive loss term $\mathcal{L}_m$은 위와 같다. 수식을 뜯어보자. [그림02]와 함께 보면 좋을 것이다. $t$는 마스킹이 수행된 time step이다. $\mathbf{c}_t$는 마스킹이 수행된 time step에서 추출된 context representation으로, 해당 시점 기준으로 전체 sequence에 대한 맥락 정보가 반영되어 있다. $\tilde{\mathbf{q}} \in \mathbf{Q}_t$는 $K$개의 <code class="language-plaintext highlighter-rouge">distractor(방해 요소)</code>와 정답 역할을 하는 1개의 $\mathbf{q}_t$로 구성되어 총 $K+1$개의 candidate quantized representation이다. 이때, $K$개의 distractor는 동일 발화의 다른 마스크 time step으로부터 랜덤하게 추출한 값들이다. $κ$는 학습 과정에서 temperature 역할을 하는 상수값이며, $sim()$ 함수는 <code class="language-plaintext highlighter-rouge">cosine similarity</code> 연산이다.</p><p>따라서, 이를 종합해 볼 때, contrastive loss는 마스킹 $t$시점 context representation $\mathbf{c} _ t$이 주어졌을 때 정답에 해당하는 $\mathbf{q} _ t$를 오답 역할을 하는 다른 candidate quantized representation 가운데서 구분해내는 역할을 한다.</p><h3 id="diversity-loss"><span class="mr-2">Diversity Loss</span><a href="#diversity-loss" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>$\mathcal{L}_d = \cfrac{1}{GV} * (-H(\bar{p} _ g)) = \cfrac{1}{GV} \sum _ {g=1}^G \sum _ {v=1}^V \bar{p} _ {g, v} \log(\bar{p} _ {g,v})$</p><p>diversity loss $\mathcal{L}_d$은 위와 같으며, 일종의 regularization 기법이다. 저자가 사용한 codebook의 수 $G=2$개였으며, 각각의 codebook 내에서 $V=320$개의 code word를 사용했다. 즉, 첫번째 codebook에서 320가지의 quantized representation이 나올 수 있고, 두번째에서도 마찬가지이므로, 총 $320 \times 320 = 102400$가지의 quantized representation 조합이 나올 수 있다.</p><p>그런데 우리는 Wav2Vec 2.0이 실제로 이 모든 조합의 확률을 다 고려해서 골고루 quantized representation을 만드는지 알 수 없다. 수많은 code word 선택에 대한 경우의 수가 있는데, 제대로 활용하지 못한다면 codebook을 활용할 이유가 없는 것과 다름 없다.</p><p>저자는 이러한 이슈를 방지하기 위해서 <a href="https://ko.wikipedia.org/wiki/%EC%A0%95%EB%B3%B4_%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC"><code class="language-plaintext highlighter-rouge">정보 이론의 엔트로피</code></a> 개념인 $H(X) = - \sum_{x} P(x) \log{(P(x))}$ 를 도입했다. 정보 엔트로피는 균등한 분포 하에서 가장 큰 값을 가진다. 예를 들어서, 앞뒷면의 확률이 모두 동일하게 1/2인 동전을 던질 때의 엔트로피가 앞뒷면의 확률이 불균등한 동전을 던질 때보다 엔트로피가 높다. 요컨대, 저자는 엔트로피를 극대화하기 위한 term을 diversity loss에 포함하여 모든 Wav2vec 2.0 모델이 모든 code word를 균등하게 고려할 수 있게 설계한 것이다.</p><h2 id="fine-tuning"><span class="mr-2">Fine-tuning</span><a href="#fine-tuning" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>모델이 pre-training을 마치고 나서, fine-tuning 단계에서는 quantization이 활용되지 않는다. 그대신, 무작위로 초기화된 <code class="language-plaintext highlighter-rouge">linear projection layer</code>를 모델의 최상단에 두고, context representation $\mathbf{c}$를 통과시키는 방식이다. 이 레이어는 풀고자 하는 task의 어휘(vocabulary)의 수를 의미하는 $C$개 class만큼의 차원으로 projection 한다. 그 후, CTC loss 및 SpecAugment 기법 등을 이용해서 labeled data에 대해 fine-tuning 된다. 또한 저자는 fine-tuning 단계에서도 마스킹 기법을 그대로 유지했다고 한다.</p><hr /><h1 id="03-결론과-느낀-점">03. 결론과 느낀 점</h1><p>정리하자면, Wav2Vec 2.0은 기존 Wav2Vec 버전들과 유사하게 self-supervised learning, 그중에서도 contrastive learning을 이용해서 학습했다. 무엇보다 end-to-end로 transformer 블록을 활용했다는 점은 VQ-Wav2Vec과의 차이점이기도 하다. 그리고 multiple codebook을 효과적으로 이용하기 위해 diversity loss를 사용한 것도 독특하다.</p><p>NLP에서의 언어모델도 대규모 학습 데이터로 pre-training을 하지만, 특정 언어에 종속적인 경우가 많다. Wav2Vec 2.0은 언어모델과는 다르게 특정 언어에 종속되지 않고 다양한 언어 및 방언에 확장될 수 있다는 점이 굉장히 인상 깊다. Wav2Vec 2.0을 시작으로 하여 음성 인식 분야의 진입 장벽이 굉장히 낮아져서 추후 모두가 쉽사리 음성 AI 기술을 구현할 날이 곧 올 것만 같다.</p><hr /><h1 id="04-참고-문헌">04. 참고 문헌</h1><p>[1] <code class="language-plaintext highlighter-rouge">원 논문</code> : <a href="https://arxiv.org/pdf/2006.11477.pdf">Baevski, Alexei, et al. “wav2vec 2.0: A framework for self-supervised learning of speech representations.” Advances in Neural Information Processing Systems 33 (2020): 12449-12460.</a><br /> [2] <code class="language-plaintext highlighter-rouge">Łukasz Sus의 블로그</code> : <a href="https://towardsdatascience.com/wav2vec-2-0-a-framework-for-self-supervised-learning-of-speech-representations-7d3728688cae">https://towardsdatascience.com/wav2vec-2-0-a-framework-for-self-supervised-learning-of-speech-representations-7d3728688cae</a><br /> [3] <code class="language-plaintext highlighter-rouge">정보 엔트로피</code> : <a href="https://ko.wikipedia.org/wiki/%EC%A0%95%EB%B3%B4_%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC">위키피디아</a></p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/speech-recognition/'>Speech Recognition</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/speech-ai/" class="post-tag no-text-decoration" >Speech AI</a> <a href="/tags/wav2vec/" class="post-tag no-text-decoration" >Wav2Vec</a> <a href="/tags/feature-extraction/" class="post-tag no-text-decoration" >Feature Extraction</a> <a href="/tags/paper-review/" class="post-tag no-text-decoration" >Paper Review</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=%28Speech+Recognition%29+Wav2Vec2.0+%EB%A6%AC%EB%B7%B0+%EB%B0%8F+%EC%84%A4%EB%AA%85+-+Simon%27s+Research+Center&url=https%3A%2F%2Fzerojsh00.github.io%2Fposts%2FWav2Vec2%2F" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=%28Speech+Recognition%29+Wav2Vec2.0+%EB%A6%AC%EB%B7%B0+%EB%B0%8F+%EC%84%A4%EB%AA%85+-+Simon%27s+Research+Center&u=https%3A%2F%2Fzerojsh00.github.io%2Fposts%2FWav2Vec2%2F" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fzerojsh00.github.io%2Fposts%2FWav2Vec2%2F&text=%28Speech+Recognition%29+Wav2Vec2.0+%EB%A6%AC%EB%B7%B0+%EB%B0%8F+%EC%84%A4%EB%AA%85+-+Simon%27s+Research+Center" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div></div><script src="https://utteranc.es/client.js" repo="zerojsh00/zerojsh00.github.io" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/Network-Policy/">(K8S) 네트워크 정책(Network Policy) 기초 개념</a><li><a href="/posts/CNI-Weave/">(K8S) CNI Weave의 기초 개념</a><li><a href="/posts/Container-Networking-Interface/">(K8S) 네트워크 기초 정리 - CNI</a><li><a href="/posts/Storage-in-Docker/">(K8S) 도커의 스토리지(Storage in Docker)</a><li><a href="/posts/Security-Context/">(K8S) Security Context</a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/k8s/">K8S</a> <a class="post-tag" href="/tags/kubernetes/">Kubernetes</a> <a class="post-tag" href="/tags/paper-review/">Paper Review</a> <a class="post-tag" href="/tags/security/">Security</a> <a class="post-tag" href="/tags/network/">Network</a> <a class="post-tag" href="/tags/speech-ai/">Speech AI</a> <a class="post-tag" href="/tags/scheduling/">Scheduling</a> <a class="post-tag" href="/tags/nlp/">NLP</a> <a class="post-tag" href="/tags/storage/">Storage</a> <a class="post-tag" href="/tags/asr/">ASR</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="tail-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/Wav2Vec/"><div class="card-body"> <em class="timeago small" data-ts="1658329200" > 2022-07-21 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>(Speech Recognition) Wav2Vec(1.0) 리뷰 및 설명</h3><div class="text-muted small"><p> 이 포스트는 개인적으로 공부한 내용을 정리하고 필요한 분들에게 지식을 공유하기 위해 작성되었습니다. 지적하실 내용이 있다면, 언제든 댓글 또는 메일로 알려주시기를 바랍니다. 00. 들어가며 음성 전문가의 도메인 지식과 푸리에 변환 등을 거쳐 추출해내는 음성 신호인Mel-Frequency Cepstral Coefficients(MFCC)와는 달리...</p></div></div></a></div><div class="card"> <a href="/posts/VQWav2Vec/"><div class="card-body"> <em class="timeago small" data-ts="1658415600" > 2022-07-22 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>(Speech Recognition) VQ-Wav2Vec 리뷰 및 설명</h3><div class="text-muted small"><p> 이 포스트는 개인적으로 공부한 내용을 정리하고 필요한 분들에게 지식을 공유하기 위해 작성되었습니다. 지적하실 내용이 있다면, 언제든 댓글 또는 메일로 알려주시기를 바랍니다. 01. 개요 VQ-Wav2Vec의 핵심은 Wav2Vec에 Vector Quantization을 적용하였다는 점이다. VQ-Wav2Vec은 Wav2Vec 방식과 유사한 sel...</p></div></div></a></div><div class="card"> <a href="/posts/MFCC/"><div class="card-body"> <em class="timeago small" data-ts="1658242800" > 2022-07-20 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>(Speech Recognition) 음성 신호 특징 추출과 MFCC</h3><div class="text-muted small"><p> 이 포스트는 개인적으로 공부한 내용을 정리하고 필요한 분들에게 지식을 공유하기 위해 작성되었습니다. 지적하실 내용이 있다면, 언제든 댓글 또는 메일로 알려주시기를 바랍니다. 00. 들어가며 Wav2Vec과 같이 뉴럴 네트워크 기반의 음성 신호 특징 추출 기법이 개발되기 전에는 음성 도메인 지식과 공식들에 기반하여 음성 신호의 특징을 추출하였다. 대표...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/Vector-Quantization/" class="btn btn-outline-primary" prompt="Older"><p>Vector Quantization과 Codebook 개념 정리</p></a> <a href="/posts/K8S_Control-Plane-Components/" class="btn btn-outline-primary" prompt="Newer"><p>(K8S) 컨트롤 플레인 컴포넌트</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center text-muted"><div class="footer-left"><p class="mb-0"> © 2023 <a href="https://github.com/zerojsh00">Sanghyeon</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/k8s/">K8S</a> <a class="post-tag" href="/tags/kubernetes/">Kubernetes</a> <a class="post-tag" href="/tags/paper-review/">Paper Review</a> <a class="post-tag" href="/tags/security/">Security</a> <a class="post-tag" href="/tags/network/">Network</a> <a class="post-tag" href="/tags/speech-ai/">Speech AI</a> <a class="post-tag" href="/tags/scheduling/">Scheduling</a> <a class="post-tag" href="/tags/nlp/">NLP</a> <a class="post-tag" href="/tags/storage/">Storage</a> <a class="post-tag" href="/tags/asr/">ASR</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/ko.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script>
