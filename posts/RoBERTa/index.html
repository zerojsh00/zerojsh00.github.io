<!DOCTYPE html><html lang="ko" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.3.2" /><meta property="og:title" content="(NLP) RoBERTa 리뷰 및 설명" /><meta name="author" content="simon sanghyeon" /><meta property="og:locale" content="ko" /><meta name="description" content="이 포스트는 개인적으로 공부한 내용을 정리하고 필요한 분들에게 지식을 공유하기 위해 작성되었습니다. 지적하실 내용이 있다면, 언제든 댓글 또는 메일로 알려주시기를 바랍니다." /><meta property="og:description" content="이 포스트는 개인적으로 공부한 내용을 정리하고 필요한 분들에게 지식을 공유하기 위해 작성되었습니다. 지적하실 내용이 있다면, 언제든 댓글 또는 메일로 알려주시기를 바랍니다." /><link rel="canonical" href="https://zerojsh00.github.io/posts/RoBERTa/" /><meta property="og:url" content="https://zerojsh00.github.io/posts/RoBERTa/" /><meta property="og:site_name" content="Simon’s Research Center" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-09-07T00:00:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="(NLP) RoBERTa 리뷰 및 설명" /><meta name="twitter:site" content="@" /><meta name="twitter:creator" content="@simon sanghyeon" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"simon sanghyeon"},"dateModified":"2022-09-07T15:57:29+09:00","datePublished":"2022-09-07T00:00:00+09:00","description":"이 포스트는 개인적으로 공부한 내용을 정리하고 필요한 분들에게 지식을 공유하기 위해 작성되었습니다. 지적하실 내용이 있다면, 언제든 댓글 또는 메일로 알려주시기를 바랍니다.","headline":"(NLP) RoBERTa 리뷰 및 설명","mainEntityOfPage":{"@type":"WebPage","@id":"https://zerojsh00.github.io/posts/RoBERTa/"},"url":"https://zerojsh00.github.io/posts/RoBERTa/"}</script><title>(NLP) RoBERTa 리뷰 및 설명 | Simon's Research Center</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Simon's Research Center"><meta name="application-name" content="Simon's Research Center"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get MODE_ATTR() { return "data-mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.notify(); } /* flipMode() */ </script> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "AMS" } }, tex2jax: { inlineMath: [ ['$', '$'] ], displayMath: [ ['$$', '$$'] ], processEscapes: true, } }); MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) { alert("Math Processing Error: "+message[1]); }); MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) { alert("Math Processing Error: "+message[1]); }); </script> <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script><meta name="google-site-verification" content="J_a4XJ2oefe52p6EYvjTFdh9LW5Yc5RyGg1HpQOjWaA" /><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" class="mx-auto"> <img src="/assets/img/avatar.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Simon's Research Center</a></div><div class="site-subtitle font-italic">성장하는 연구원의 공책</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <button class="mode-toggle btn" aria-label="Switch Mode"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/zerojsh00" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['zerojsh00','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>(NLP) RoBERTa 리뷰 및 설명</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>(NLP) RoBERTa 리뷰 및 설명</h1><div class="post-meta text-muted"><div> By <em> Sanghyeon </em></div><div class="d-flex"><div> <span> Posted <em class="timeago" data-ts="1662476400" data-toggle="tooltip" data-placement="bottom" data-tooltip-df="llll" > 2022-09-07 </em> </span> <span> Updated <em class="timeago" data-ts="1662533849" data-toggle="tooltip" data-placement="bottom" data-tooltip-df="llll" > 2022-09-07 </em> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="2473 words"> <em>13 min</em> read</span></div></div></div><div class="post-content"><p>이 포스트는 개인적으로 공부한 내용을 정리하고 필요한 분들에게 지식을 공유하기 위해 작성되었습니다. 지적하실 내용이 있다면, 언제든 댓글 또는 메일로 알려주시기를 바랍니다.</p><hr /><h1 id="01-요약">01. 요약</h1><p>RoBERTa는 <strong>R</strong>obustly <strong>O</strong>ptimized <strong>BERT</strong> <strong>a</strong>pproach의 약자이다. ELMo, GPT, BERT, XLM, XLNet 등 <code class="language-plaintext highlighter-rouge">self-training</code> 방식인 기존 언어 모델들은 기존 모델들에 비해 비약적인 성능 향상을 보였다. 하지만, 이처럼 다양한 방식의 모델들이 존재함에도 불구하고, 모델 관점에서 무엇이 성능을 극대화하는 요인인지 알기 어려웠다.</p><p>Facebook AI의 RoBERTa 저자들은 BERT의 사전학습 방식이 상당히 undertrained 되어 있다고 주장하며, BERT 이후 나온 post-BERT 방식을 능가하는 새로운 사전학습 방식으로 RoBERTa를 제안했다. BERT의 사전학습에는 상당한 하드웨어 자원과 학습 시간이 필요하여, 다방면에서 실험을 진행해보는 데 한계가 있을 수 있다. 그럼에도 불구하고 RoBERTa의 저자들은 모델 관점에서 BERT의 디자인을 다양하게 변형 및 실험해보았다는 기여를 했다. 이러한 시도의 결과로, 저자들은 RoBERTa를 통해 undertrained 되어 있는 BERT를 더욱 강건하게 최적화 할 수 있는 방법을 제시했다.</p><hr /><h1 id="02-roberta의-특징">02. RoBERTa의 특징</h1><p>RoBERTa는 BERT와 유사하지만, 다음과 같은 특징이 있다.</p><ul><li><strong>특징1</strong> : 더욱 <code class="language-plaintext highlighter-rouge">많은 데이터</code>를 사용하여, 더욱 <code class="language-plaintext highlighter-rouge">큰 batch</code>로, 더욱 <code class="language-plaintext highlighter-rouge">오래</code> 모델을 학습하였다.<li><strong>특징2</strong> : BERT의 사전학습 방식에서 <code class="language-plaintext highlighter-rouge">Next Sentence Prediction(NSP)</code> 태스크를 제거하였다.<li><strong>특징3</strong> : 더욱 <code class="language-plaintext highlighter-rouge">긴 sequence</code>로 학습하였다.<li><strong>특징4</strong> : 사전학습 데이터에 더욱 <code class="language-plaintext highlighter-rouge">동적 마스킹 방식</code>을 적용하였다.</ul><h2 id="특징1---더욱-많은-데이터-더욱-큰-batch-더욱-오래-학습"><span class="mr-2">특징1 - 더욱 많은 데이터, 더욱 큰 batch, 더욱 오래 학습</span><a href="#특징1---더욱-많은-데이터-더욱-큰-batch-더욱-오래-학습" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><h3 id="더욱-많은-데이터"><span class="mr-2">더욱 많은 데이터</span><a href="#더욱-많은-데이터" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>BERT처럼 대규모 코퍼스로 사전학습하는 방식의 언어 모델은 학습 데이터의 크기가 매우 중요하다. (Baevski et al. 2019)에 따르면, 데이터의 규모를 키우는 것이 end-task의 성능 향상에 도움이 된다고 한다. RoBERTa 이전에도 original BERT보다 더욱 많은 데이터를 사용했던 시도들은 있었으나, 해당 데이터들이 공개되어 있지는 않았다. 따라서 RoBERTa 저자들은 최대한 많은 학습 데이터를 활용해서 언어 모델을 학습해보고자 했다. 사용한 데이터는 아래와 같다.</p><ul><li>BOOK CORPUS와 영어 WIKIPEDIA<ul><li>original BERT를 학습하기 위해서 사용되었던 16GB의 데이터다.</ul><li>CC-NEWS <code class="language-plaintext highlighter-rouge">RoBERTa에서 추가됨</code><ul><li>저자들이 수집한 2016년 9월 ~ 2019년 2월 동안의 뉴스 데이터로, 6,300 만 개의 크롤링된 영어 뉴스 기사로 구성되었으며, 76GB에 달한다.</ul><li>OPEN WEB TEXT <code class="language-plaintext highlighter-rouge">RoBERTa에서 추가됨</code><ul><li>최소 3개의 좋아요를 받은 Reddit으로부터 추출된 텍스트 데이터로, 38GB에 달한다.</ul><li>STORIES <code class="language-plaintext highlighter-rouge">RoBERTa에서 추가됨</code><ul><li>CommonCrawl 데이터의 일부로, 31GB에 달한다.</ul></ul><h3 id="더욱-큰-batch"><span class="mr-2">더욱 큰 batch</span><a href="#더욱-큰-batch" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>(Ott et al., 2018)에 따르면, 기계번역 모델은 큰 미니배치 사이즈를 이용할 때 더욱 빠르게 수렴되고 좋은 성능을 낼 수 있다. (You et al., 2019)는 이러한 방식이 BERT에도 적용된다고 주장한다. 이에 RoBERTa 저자들은 더욱 큰 배치 사이즈를 적용해보았다.</p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 500 500'%3E%3C/svg%3E" data-src="/assets/img/2022-09-07-RoBERTa/fig01.png" alt="fig01" width="500" height="500" data-proofer-ignore></p><p>위와 같이 배치 사이즈(bsz)와 스텝(steps)의 곱이 유사하도록 환경을 구성함으로써 계산 복잡도를 유사하게 맞추어 놓은 후 비교했을 때, 배치 사이즈가 커질 때 perplexity(PPL)과 end-task의 정확도 모두 증가할 수 있음을 확인했다.</p><h3 id="더욱-오래-학습"><span class="mr-2">더욱 오래 학습</span><a href="#더욱-오래-학습" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>RoBERTa의 저자들은 더욱 오랜 스텝 동안 사전학습 하는 것이 궁극적으로 더 나은 성능을 보이는지를 실험하였다.</p><h2 id="특징2---next-sentence-prediction-태스크-제거"><span class="mr-2">특징2 - Next Sentence Prediction 태스크 제거</span><a href="#특징2---next-sentence-prediction-태스크-제거" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>original BERT는 사전학습 시 두 개의 문장 segment를 입력하는데, 50%는 같은 문서에서 인접한 문장들을, 50%는 다른 문서에서 각각 추출한 문장들을 사용하여 두 문장이 인접한 문장인지를 학습한다. 이를 Next Sentence Prediction(NSP) 태스크라고 하며, NSP loss로 구현된다.</p><p>original BERT가 이러한 방식으로 학습하는 이유는 BERT가 NSP loss를 이용해서 학습했을 때 Question-answering NLI(QNLI), Multi-genre NLI(MNLI), SQuAD 데이터 태스크를 더욱 잘 수행하였기 때문이다. 한편, (Lample and Conneau, 2019) 등 여러 연구들은 NSP loss의 필요성에 대한 의문을 제시했다. 이러한 간극을 이해하기 위해서 RoBERTa는 다양한 실험을 통해 NSP loss의 효과를 입증하고자 했다.</p><h3 id="실험1--segment-pair-with-nsp"><span class="mr-2">실험1 : SEGMENT-PAIR with NSP</span><a href="#실험1--segment-pair-with-nsp" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>이 방식은 original BERT와 동일한 입력값 설정으로, 두 <code class="language-plaintext highlighter-rouge">segment</code>로 이루어진 입력값이 입력되지만, 통합 512 토큰 이하가 사용된다. 두 segment는 인접해 있을 수도 있고, 아닐 수도 있다.</p><h3 id="실험2--sentence-pair-with-nsp"><span class="mr-2">실험2 : SENTENCE-PAIR with NSP</span><a href="#실험2--sentence-pair-with-nsp" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>두 <code class="language-plaintext highlighter-rouge">sentence(완전한 자연어 문장)</code>로 이루어진 입력값이 입력된다. 이 경우, 512 토큰에 맞추어 가공한 segment에 비해 문장이 짧을 수 있으므로, 배치 사이즈를 늘림으로써 전체적인 토큰의 수가 실험1과 유사하도록 설정되었다. 두 sentence는 인접해 있을 수도 있고, 아닐 수도 있다.</p><h3 id="실험3--full-sentences-without-nsp"><span class="mr-2">실험3 : FULL-SENTENCES without NSP</span><a href="#실험3--full-sentences-without-nsp" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>통합 512 토큰 이하로 구성된, 인접한 <code class="language-plaintext highlighter-rouge">sentence(완전한 자연어 문장)</code>들로 이루어진 입력값이 입력된다. 이때 sentence는 두 문장 이상이 가능하다. 인접한 문장들이지만, 하나의 문서(document)를 넘어서는 경우, 다음 문서에서 문장을 샘플링하되, 다른 문서로 넘어갔다는 표시를 위해 별도로 특별한 separator 토큰을 추가 해준다. 그리고 NSP loss를 사용하지 않았다.</p><h3 id="실험4--doc-sentences-without-nsp"><span class="mr-2">실험4 : DOC-SENTENCES without NSP</span><a href="#실험4--doc-sentences-without-nsp" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>실험3과 유사하게 입력값을 구성한다. 그러나 하나의 문서를 넘어서는 문장 구성은 허용하지 않는다. 문서 끝에서 추출되는 문장들은 512 토큰보다 짧을 수 있으니, 이러한 경우 동적으로 배치 사이즈를 늘려줌으로써 조정했다. 그리고 NSP loss를 사용하지 않았다.</p><h3 id="실험-결과"><span class="mr-2">실험 결과</span><a href="#실험-결과" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 600 600'%3E%3C/svg%3E" data-src="/assets/img/2022-09-07-RoBERTa/fig02.png" alt="fig02" width="600" height="600" data-proofer-ignore></p><p>우선, 예상대로 실험2(SENTENCE-PAIR with NSP)는 original BERT의 방식인 실험1(SEGMENT-PAIR with NSP)에 비해 downstream task 성능이 떨어졌다. 이를 통해, sentence를 사용하는 경우, 문장이 짧을 때 long-range dependencies가 있을 수 있으며, 이에 따라 모델이 long-range 맥락을 학습할 수 없다는 가설을 입증했다.</p><p>또한, NSP loss를 제거했음에도 불구하고 downstream task의 성능 향상을 볼 수 있었다. 특히 실험4(DOC-SENTENCES without NSP)의 결과가 두드러졌지만, 매번 배치 사이즈를 동적으로 변경시켜주어야 한다는 불편함이 있어 이후 실험에서는 실험3(FULL-SENTENCES without NSP)의 방식을 활용했다고 한다.</p><h2 id="특징3---더욱-긴-sequence로-학습"><span class="mr-2">특징3 - 더욱 긴 sequence로 학습</span><a href="#특징3---더욱-긴-sequence로-학습" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>original BERT처럼 최대 512 길이의 토큰 sequence를 사용하되, 다른 점으로는 short sequence를 랜덤하게 사용하지 않았고, 처음 90%의 업데이트 동안 reduced sequence length를 사용하지 않았다.</p><h2 id="특징4---동적-마스킹-방식-적용"><span class="mr-2">특징4 - 동적 마스킹 방식 적용</span><a href="#특징4---동적-마스킹-방식-적용" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>original BERT는 <code class="language-plaintext highlighter-rouge">Masked Language Model(MLM)</code> 학습 방식을 위해 [MASK] 토큰을 전처리 단에서 만들어사용한다. 이는 학습 단계에서 고정된 데이터이므로 <code class="language-plaintext highlighter-rouge">static masking</code>이라 할 수 있다. 이 경우, 모델은 매 epoch마다 동일하게 마스킹된 데이터를 중복하여 보게된다. RoBERTa에서는 [MASK] 토큰을 전처리 단계에서 만드는 것이 아니라, 모델에 텍스트 데이터가 주입되는 시점에 생성해서 사용한다. 이렇게 동적으로 데이터를 마스킹하는 방식을 <code class="language-plaintext highlighter-rouge">dynamic masking</code> 이라고 한다. dynamic masking 기법은 더욱 많은 텍스트 데이터를 활용하여 더욱 오래 사전학습하는 RoBERTa에 필수적이며, 실제로 downstream task 성능 향상에도 기여했다고 한다.</p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 500 500'%3E%3C/svg%3E" data-src="/assets/img/2022-09-07-RoBERTa/fig03.png" alt="fig03" width="500" height="500" data-proofer-ignore></p><hr /><h1 id="03-roberta-실험-세팅">03. RoBERTa 실험 세팅</h1><p>논문에서는 BERT-LARGE 크기의 아키텍처(레이어 수 = 24, 히든 사이즈 = 1024, 어텐션 헤드 수 = 16, 355M 파라미터)를 사용하였다. 1024개의 V100 GPU로 대략 하루 정도의 시간동안 학습했다.</p><hr /><h1 id="04-roberta-실험-결과">04. RoBERTa 실험 결과</h1><p><img data-src="/assets/img/2022-09-07-RoBERTa/fig04.png" alt="fig04" data-proofer-ignore></p><p>더욱 많은 데이터(data 160GB)로, 더 크게(bsz 8K), 더 오래(500K) 사전학습 할수록 성능이 좋아지는 것을 볼 수 있다.</p><p><img data-src="/assets/img/2022-09-07-RoBERTa/fig05.png" alt="fig05" data-proofer-ignore> <img data-src="/assets/img/2022-09-07-RoBERTa/fig06.png" alt="fig06" data-proofer-ignore></p><p>GLUE, SQuAD, RACE datasets로 실험한 결과들을 비교해보면, 많은 Natural Language Understanding(NLU) 태스크에서 RoBERTa가 SOTA를 달성하였다.</p><hr /><h1 id="05-참고-문헌">05. 참고 문헌</h1><p>[1] <code class="language-plaintext highlighter-rouge">원 논문</code> : <a href="https://arxiv.org/pdf/1907.11692.pdf">https://arxiv.org/pdf/1907.11692.pdf</a></p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/natural-language-processing/'>Natural Language Processing</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/language-model/" class="post-tag no-text-decoration" >Language Model</a> <a href="/tags/nlp/" class="post-tag no-text-decoration" >NLP</a> <a href="/tags/nlu/" class="post-tag no-text-decoration" >NLU</a> <a href="/tags/paper-review/" class="post-tag no-text-decoration" >Paper Review</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=%28NLP%29+RoBERTa+%EB%A6%AC%EB%B7%B0+%EB%B0%8F+%EC%84%A4%EB%AA%85+-+Simon%27s+Research+Center&url=https%3A%2F%2Fzerojsh00.github.io%2Fposts%2FRoBERTa%2F" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=%28NLP%29+RoBERTa+%EB%A6%AC%EB%B7%B0+%EB%B0%8F+%EC%84%A4%EB%AA%85+-+Simon%27s+Research+Center&u=https%3A%2F%2Fzerojsh00.github.io%2Fposts%2FRoBERTa%2F" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fzerojsh00.github.io%2Fposts%2FRoBERTa%2F&text=%28NLP%29+RoBERTa+%EB%A6%AC%EB%B7%B0+%EB%B0%8F+%EC%84%A4%EB%AA%85+-+Simon%27s+Research+Center" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div></div><script src="https://utteranc.es/client.js" repo="zerojsh00/zerojsh00.github.io" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/Network-Policy/">(K8S) 네트워크 정책(Network Policy) 기초 개념</a><li><a href="/posts/CNI-Weave/">(K8S) CNI Weave의 기초 개념</a><li><a href="/posts/Container-Networking-Interface/">(K8S) 네트워크 기초 정리 - CNI</a><li><a href="/posts/Storage-in-Docker/">(K8S) 도커의 스토리지(Storage in Docker)</a><li><a href="/posts/Security-Context/">(K8S) Security Context</a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/k8s/">K8S</a> <a class="post-tag" href="/tags/kubernetes/">Kubernetes</a> <a class="post-tag" href="/tags/paper-review/">Paper Review</a> <a class="post-tag" href="/tags/security/">Security</a> <a class="post-tag" href="/tags/network/">Network</a> <a class="post-tag" href="/tags/speech-ai/">Speech AI</a> <a class="post-tag" href="/tags/scheduling/">Scheduling</a> <a class="post-tag" href="/tags/nlp/">NLP</a> <a class="post-tag" href="/tags/storage/">Storage</a> <a class="post-tag" href="/tags/asr/">ASR</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="tail-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/ELECTRA/"><div class="card-body"> <em class="timeago small" data-ts="1662994800" > 2022-09-13 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>(NLP) ELECTRA 리뷰 및 설명</h3><div class="text-muted small"><p> 이 포스트는 개인적으로 공부한 내용을 정리하고 필요한 분들에게 지식을 공유하기 위해 작성되었습니다. 지적하실 내용이 있다면, 언제든 댓글 또는 메일로 알려주시기를 바랍니다. 01. 개요 ICLR 2020에서 Google Brain 팀은 새로운 사전 학습 방법론으로 ELECTRA를 제안했다. ELECTRA는 “Efficiently Learning...</p></div></div></a></div><div class="card"> <a href="/posts/BERTopic/"><div class="card-body"> <em class="timeago small" data-ts="1663686000" > 2022-09-21 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>(NLP) BERTopic 개념 정리</h3><div class="text-muted small"><p> 이 포스트는 개인적으로 공부한 내용을 정리하고 필요한 분들에게 지식을 공유하기 위해 작성되었습니다. 지적하실 내용이 있다면, 언제든 댓글 또는 메일로 알려주시기를 바랍니다. 01. Introduction 전통적인 토픽 모델링의 방법으로는 Latent Dirichlet Allocation(LDA)와 Non-Negative Matrix Factor...</p></div></div></a></div><div class="card"> <a href="/posts/DistilBERT/"><div class="card-body"> <em class="timeago small" data-ts="1676818800" > 2023-02-20 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>(NLP) DistilBERT 리뷰 및 설명</h3><div class="text-muted small"><p> 이 포스트는 개인적으로 공부한 내용을 정리하고 필요한 분들에게 지식을 공유하기 위해 작성되었습니다. 지적하실 내용이 있다면, 언제든 댓글 또는 메일로 알려주시기를 바랍니다. 01. Introduction 2018년, 자연어처리 영역의 위대한 한 획을 그은 BERT를 대표로하여, 자연어처리 영역은 대규모의 사전학습 언어모델(large-scal...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/Persistent-Volume-Claims/" class="btn btn-outline-primary" prompt="Older"><p>(K8S) 퍼시스턴트 볼륨 클레임(Persistent Volume Claim)</p></a> <a href="/posts/Storage-Class/" class="btn btn-outline-primary" prompt="Newer"><p>(K8S) 스토리지 클래스와 다이나믹 프로비저닝</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center text-muted"><div class="footer-left"><p class="mb-0"> © 2023 <a href="https://github.com/zerojsh00">Sanghyeon</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/k8s/">K8S</a> <a class="post-tag" href="/tags/kubernetes/">Kubernetes</a> <a class="post-tag" href="/tags/paper-review/">Paper Review</a> <a class="post-tag" href="/tags/security/">Security</a> <a class="post-tag" href="/tags/network/">Network</a> <a class="post-tag" href="/tags/speech-ai/">Speech AI</a> <a class="post-tag" href="/tags/scheduling/">Scheduling</a> <a class="post-tag" href="/tags/nlp/">NLP</a> <a class="post-tag" href="/tags/storage/">Storage</a> <a class="post-tag" href="/tags/asr/">ASR</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/ko.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script>
