<!DOCTYPE html><html lang="ko" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.3.2" /><meta property="og:title" content="(NLP) StarSpace" /><meta name="author" content="simon sanghyeon" /><meta property="og:locale" content="ko" /><meta name="description" content="이 포스트는 개인적으로 공부한 내용을 정리하고 필요한 분들에게 지식을 공유하기 위해 작성되었습니다. 지적하실 내용이 있다면, 언제든 댓글 또는 메일로 알려주시기를 바랍니다." /><meta property="og:description" content="이 포스트는 개인적으로 공부한 내용을 정리하고 필요한 분들에게 지식을 공유하기 위해 작성되었습니다. 지적하실 내용이 있다면, 언제든 댓글 또는 메일로 알려주시기를 바랍니다." /><link rel="canonical" href="https://zerojsh00.github.io/posts/StarSpace/" /><meta property="og:url" content="https://zerojsh00.github.io/posts/StarSpace/" /><meta property="og:site_name" content="Simon’s Research Center" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2023-02-13T00:00:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="(NLP) StarSpace" /><meta name="twitter:site" content="@" /><meta name="twitter:creator" content="@simon sanghyeon" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"simon sanghyeon"},"dateModified":"2023-02-13T00:00:00+09:00","datePublished":"2023-02-13T00:00:00+09:00","description":"이 포스트는 개인적으로 공부한 내용을 정리하고 필요한 분들에게 지식을 공유하기 위해 작성되었습니다. 지적하실 내용이 있다면, 언제든 댓글 또는 메일로 알려주시기를 바랍니다.","headline":"(NLP) StarSpace","mainEntityOfPage":{"@type":"WebPage","@id":"https://zerojsh00.github.io/posts/StarSpace/"},"url":"https://zerojsh00.github.io/posts/StarSpace/"}</script><title>(NLP) StarSpace | Simon's Research Center</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Simon's Research Center"><meta name="application-name" content="Simon's Research Center"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get MODE_ATTR() { return "data-mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.notify(); } /* flipMode() */ </script> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "AMS" } }, tex2jax: { inlineMath: [ ['$', '$'] ], displayMath: [ ['$$', '$$'] ], processEscapes: true, } }); MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) { alert("Math Processing Error: "+message[1]); }); MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) { alert("Math Processing Error: "+message[1]); }); </script> <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script><meta name="google-site-verification" content="J_a4XJ2oefe52p6EYvjTFdh9LW5Yc5RyGg1HpQOjWaA" /><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" class="mx-auto"> <img src="/assets/img/avatar.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Simon's Research Center</a></div><div class="site-subtitle font-italic">성장하는 연구원의 공책</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <button class="mode-toggle btn" aria-label="Switch Mode"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/zerojsh00" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['zerojsh00','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>(NLP) StarSpace</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>(NLP) StarSpace</h1><div class="post-meta text-muted"><div> By <em> Sanghyeon </em></div><div class="d-flex"><div> <span> Posted <em class="timeago" data-ts="1676214000" data-toggle="tooltip" data-placement="bottom" data-tooltip-df="llll" > 2023-02-13 </em> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="1721 words"> <em>9 min</em> read</span></div></div></div><div class="post-content"><p>이 포스트는 개인적으로 공부한 내용을 정리하고 필요한 분들에게 지식을 공유하기 위해 작성되었습니다. 지적하실 내용이 있다면, 언제든 댓글 또는 메일로 알려주시기를 바랍니다.</p><hr /><h1 id="00-들어가기에-앞서">00. 들어가기에 앞서</h1><hr /><p>2017년, Facebook AI에서 공개한 StarSpace는 그 구조가 간단하면서도 다양한 도메인의 내용들을 임베딩할 수 있다는 강력한 장점을 가지고 있다. 챗봇의 의도 분류기를 연구했던 시절에 처음 접했던 개념이었지만, 개인적인 필요에 의해 다시 한 번 살펴볼 일이 있어 이참에 본 포스트를 통해 정리하고자 한다.</p><p>원 논문 <a href="https://arxiv.org/abs/1709.03856">StarSpace: Embed All The Things!</a>를 참고하기를 바란다.</p><h1 id="01-introduction">01. Introduction</h1><hr /><blockquote><h2 id="a-general-purpose-neural-embedding-model"><span class="mr-2">“a general-purpose neural embedding model”</span><a href="#a-general-purpose-neural-embedding-model" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2></blockquote><p>StarSpace는 아래와 같이 다양한 문제들을 해결할 수 있는 신경망 임베딩 모델이다.</p><ul><li>텍스트 분류 태스크 (자연어처리 분야)<ul><li>e.g., 감정 분석, 의도 분류 등</ul><li>엔티티에 대해 랭크를 부여하는 태스크 (Information Retrieval 분야)<ul><li>e.g., 웹에서 쿼리를 날렸을 때, 웹 상의 수많은 도큐먼트들에 대해서 랭킹을 부여하는 등</ul><li>협업 필터링 기반의 추천 (추천시스템 분야)<ul><li>문서, 음악, 비디오 추천 등</ul><li>이산적인 feature로 이루어진 콘텐츠에 대한 추천 (추천시스템 분야)<ul><li>e.g., 문서의 단어들 등</ul><li>그래프 임베딩 (그래프 임베딩 분야)<ul><li>e.g., Freebase</ul><li>기타 단어, 문장, 문서 등에 대한 임베딩 등등</ul><p>StarSpace 모델은 이처럼 다양한 분야에서 임베딩 태스크들에 응용할 수 있다는 특징으로 그 이름이 지어졌다. <code class="language-plaintext highlighter-rouge">Star</code>는 <code class="language-plaintext highlighter-rouge">*</code>, 즉, <code class="language-plaintext highlighter-rouge">모든 엔티티 타입</code>을 의미하며, <code class="language-plaintext highlighter-rouge">Space</code>는 <code class="language-plaintext highlighter-rouge">임베딩 공간</code>을 의미하는데, 풀어보자면, 엔티티 타입이 어떠하든 모든지 한 공간에 임베딩 할 수 있다고 해석할 수 있다.</p><h1 id="02-model">02. Model</h1><hr /><h2 id="02-1-이산적인-특징-벡터를-입력-받는-starspace"><span class="mr-2">02-1. 이산적인 특징 벡터를 입력 받는 StarSpace</span><a href="#02-1-이산적인-특징-벡터를-입력-받는-starspace" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>StarSpace 모델을 통해 임베딩 하고자 하는 entity는 이산적인(discrete) 특징을 가진다. 대표적인 예로는 단어의 수를 셈하여 만드는 BOW(Bag-Of-Words) 벡터가 있겠다. 풀고자하는 태스크에 따라 꼭 단어가 아니더라도, 특정 유저가 좋아요를 누른 영화나 물건 등과 같이 이산적으로 표현할 수 있는 특징이면 StarSpace 모델을 활용하기에 적합하다.</p><p><em>(허나, 필자의 개인적인 생각과 경험으로는, 반드시 이산적인 특징 벡터만 가능한가 싶다. 예컨대 실수 값으로 이루어진 TF-IDF 벡터도 입력값으로 쓸 수 있어 보인다.)</em></p><h2 id="02-2-서로-다른-종류의-엔티티를-같은-공간-상에-임베딩하는-starspace"><span class="mr-2">02-2. 서로 다른 종류의 엔티티를 같은 공간 상에 임베딩하는 StarSpace</span><a href="#02-2-서로-다른-종류의-엔티티를-같은-공간-상에-임베딩하는-starspace" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>StarSpace의 가장 강력한 특징은 <code class="language-plaintext highlighter-rouge">서로 다른 종류의 엔티티를 같은 공간 상에 임베딩</code> 할 수 있다는 점이다. ‘서로 다른 종류의 엔티티’란 뭘까? 예를 들면, 사람들이 올려놓은 게시글과, 이에 대해서 부여된 해시태그들이라고 생각해볼 수 있겠다.</p><p><img data-src="/assets/img/2023-02-13-StarSpace/fig01.png" alt="fig01" data-proofer-ignore> <em>[출처 : Rasa Algorithm Whiteboard (Youtube)]</em></p><p>위 그림을 살펴보자.</p><p><code class="language-plaintext highlighter-rouge">(d2) : I love veggie pizza</code>라는 document는 <code class="language-plaintext highlighter-rouge">#pizza</code>라는 해시태그가 달릴수도 있으며, <code class="language-plaintext highlighter-rouge">#positive</code> 및 <code class="language-plaintext highlighter-rouge">#vegetarian</code>이라는 해시태그 또한 달릴 수 있다. 또한 <code class="language-plaintext highlighter-rouge">(d5) : Gimme Pepperoni Pizza</code>라는 document 역시 피자와 관련된 글이므로 앞선 문장처럼 <code class="language-plaintext highlighter-rouge">#pizza</code>의 해시태그가 달릴 수 있다. 이러한 관계들을 위의 그림과 같이 표(table) 형태로 표현할 수 있다.</p><p>그렇다면 어떻게 해야 이러한 관계들을 이용하여 같은 공간 상에 임베딩 할 수 있을까?</p><p><img data-src="/assets/img/2023-02-13-StarSpace/fig02.png" alt="fig02" data-proofer-ignore> <em>[출처 : Rasa Algorithm Whiteboard (Youtube)]</em></p><p>StarSpace 모델의 핵심은 <strong>동일한 레이블(e.g., 해시태그)이 부여된 엔티티(e.g., document)는 가깝게 임베딩되고, 다른 레이블이 부여된 엔티티는 멀리 임베딩 되도록 하는 메커니즘</strong>이다. 그렇게 학습을 마치고 나면, <strong>엔티티 임베딩들과 레이블 임베딩들이 유사성을 기반으로 하여 일종의 클러스터를 이룰 것</strong>이다.</p><p>이는 StarSpace가 가지는 중요한 특징이라고 할 수 있다. BERT와 같이 대규모 일반 도메인 말뭉치를 활용하여 학습하는 언어모델과 달리, <strong>우리가 가지고 있는 엔티티에 해당하는 도메인으로 학습</strong>하기 때문에, 특정 closed domain task를 풀 때 훨씬 유용하게 사용될 수 있다. 이러한 특징 때문에 <a href="https://rasa.com/">Rasa</a>에서 StarSpace를 시나리오 기반 챗봇의 의도 분류기로써 활용하는 것으로 생각된다.</p><h2 id="02-3-모델의-구조"><span class="mr-2">02-3. 모델의 구조</span><a href="#02-3-모델의-구조" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p><img data-src="/assets/img/2023-02-13-StarSpace/fig03.png" alt="fig03" data-proofer-ignore> <em>[출처 : Rasa Algorithm Whiteboard (Youtube)]</em></p><p>모델의 구조는 제법 심플하다. 엔티티에 대한 임베딩 및 레이블에 대한 임베딩을 각각 linear layer를 통과시켜 $k$ 차원의 공감에 projection 한다. 그후 코사인 유사도 또는 벡터의 내적을 통해 유사도 점수를 구한 후, 해당 점수와 레이블 간의 <code class="language-plaintext highlighter-rouge">ranking loss</code>를 구하는 방식이다.</p><p><img data-src="/assets/img/2023-02-13-StarSpace/fig04.png" alt="fig04" data-proofer-ignore> <em>[출처 : https://cw.fel.cvut.cz/old/_media/courses/xp36vpd/vpdstarspace.pdf]</em></p><h1 id="03-구현">03. 구현</h1><hr /><p>아래는 StarSpace를 참고하여 의도 분류를 구현한 코드의 예시다.</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">import</span> <span class="n">torch.optim</span> <span class="k">as</span> <span class="n">optim</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="s">'cuda'</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s">'cpu'</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">StarSpace</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">X_dim</span><span class="p">,</span> <span class="n">n_labels</span><span class="p">,</span> <span class="n">emb_dim</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">drop_rate</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>

        <span class="c1"># feature들에 대한 임베딩
</span>        <span class="n">self</span><span class="p">.</span><span class="n">feature_emb_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">X_dim</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">drop_rate</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">emb_dim</span><span class="p">)</span>
        <span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># 레이블에 대한 임베딩
</span>        <span class="n">self</span><span class="p">.</span><span class="n">label_emb</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">n_labels</span><span class="p">,</span> <span class="n">emb_dim</span><span class="p">,</span> <span class="n">max_norm</span><span class="o">=</span><span class="mf">10.0</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">label_emb_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">emb_dim</span><span class="p">,</span> <span class="n">emb_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">emb_dim</span><span class="p">,</span> <span class="n">emb_dim</span><span class="p">)</span>
        <span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># loss
</span>        <span class="n">self</span><span class="p">.</span><span class="n">ce_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">CrossEntropyLoss</span><span class="p">()</span>  <span class="c1"># Softmax + CrossEntropyLoss
</span>        <span class="n">self</span><span class="p">.</span><span class="n">softmax</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="c1"># 임베딩 레이어를 통과하여 임베딩을 획득
</span>        <span class="n">feature_emb</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">feature_emb_layer</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="c1"># 어차피 positive target 임베딩과 negative target 임베딩이 함께 있음
</span>        <span class="n">y_embs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">label_emb_layer</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">label_emb</span><span class="p">.</span><span class="n">weight</span><span class="p">)</span>

        <span class="c1"># 정답에 대한 레이블의 스코어와 negative 레이블의 스코어가 함께 계산됨
</span>        <span class="n">sim_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">feature_emb</span><span class="p">,</span> <span class="n">y_embs</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">ce_loss</span><span class="p">(</span><span class="n">sim_scores</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">confidence</span><span class="p">,</span> <span class="n">prediction</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">sim_scores</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="p">{</span><span class="s">"loss"</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="s">"prediction"</span><span class="p">:</span> <span class="n">prediction</span><span class="p">,</span> <span class="s">"confidence"</span><span class="p">:</span> <span class="n">confidence</span><span class="p">}</span>
</pre></table></code></div></div><h1 id="04-참고-문헌">04. 참고 문헌</h1><hr /><p>[1] <code class="language-plaintext highlighter-rouge">원 논문</code> : <a href="https://arxiv.org/abs/1709.03856">StarSpace: Embed All The Things!</a><br /> [2] <code class="language-plaintext highlighter-rouge">Rasa Algorithm Whiteboard</code> : <a href="https://www.youtube.com/watch?v=ZT3_9Kjx7oI&amp;t=573s">StarSpace</a></p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/natural-language-processing/'>Natural Language Processing</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/nlp/" class="post-tag no-text-decoration" >NLP</a> <a href="/tags/embedding/" class="post-tag no-text-decoration" >Embedding</a> <a href="/tags/paper-review/" class="post-tag no-text-decoration" >Paper Review</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=%28NLP%29+StarSpace+-+Simon%27s+Research+Center&url=https%3A%2F%2Fzerojsh00.github.io%2Fposts%2FStarSpace%2F" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=%28NLP%29+StarSpace+-+Simon%27s+Research+Center&u=https%3A%2F%2Fzerojsh00.github.io%2Fposts%2FStarSpace%2F" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fzerojsh00.github.io%2Fposts%2FStarSpace%2F&text=%28NLP%29+StarSpace+-+Simon%27s+Research+Center" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div></div><script src="https://utteranc.es/client.js" repo="zerojsh00/zerojsh00.github.io" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/Network-Policy/">(K8S) 네트워크 정책(Network Policy) 기초 개념</a><li><a href="/posts/CNI-Weave/">(K8S) CNI Weave의 기초 개념</a><li><a href="/posts/Container-Networking-Interface/">(K8S) 네트워크 기초 정리 - CNI</a><li><a href="/posts/Storage-in-Docker/">(K8S) 도커의 스토리지(Storage in Docker)</a><li><a href="/posts/Security-Context/">(K8S) Security Context</a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/k8s/">K8S</a> <a class="post-tag" href="/tags/kubernetes/">Kubernetes</a> <a class="post-tag" href="/tags/paper-review/">Paper Review</a> <a class="post-tag" href="/tags/security/">Security</a> <a class="post-tag" href="/tags/network/">Network</a> <a class="post-tag" href="/tags/speech-ai/">Speech AI</a> <a class="post-tag" href="/tags/scheduling/">Scheduling</a> <a class="post-tag" href="/tags/nlp/">NLP</a> <a class="post-tag" href="/tags/storage/">Storage</a> <a class="post-tag" href="/tags/asr/">ASR</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="tail-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/ELECTRA/"><div class="card-body"> <em class="timeago small" data-ts="1662994800" > 2022-09-13 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>(NLP) ELECTRA 리뷰 및 설명</h3><div class="text-muted small"><p> 이 포스트는 개인적으로 공부한 내용을 정리하고 필요한 분들에게 지식을 공유하기 위해 작성되었습니다. 지적하실 내용이 있다면, 언제든 댓글 또는 메일로 알려주시기를 바랍니다. 01. 개요 ICLR 2020에서 Google Brain 팀은 새로운 사전 학습 방법론으로 ELECTRA를 제안했다. ELECTRA는 “Efficiently Learning...</p></div></div></a></div><div class="card"> <a href="/posts/BERTopic/"><div class="card-body"> <em class="timeago small" data-ts="1663686000" > 2022-09-21 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>(NLP) BERTopic 개념 정리</h3><div class="text-muted small"><p> 이 포스트는 개인적으로 공부한 내용을 정리하고 필요한 분들에게 지식을 공유하기 위해 작성되었습니다. 지적하실 내용이 있다면, 언제든 댓글 또는 메일로 알려주시기를 바랍니다. 01. Introduction 전통적인 토픽 모델링의 방법으로는 Latent Dirichlet Allocation(LDA)와 Non-Negative Matrix Factor...</p></div></div></a></div><div class="card"> <a href="/posts/RoBERTa/"><div class="card-body"> <em class="timeago small" data-ts="1662476400" > 2022-09-07 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>(NLP) RoBERTa 리뷰 및 설명</h3><div class="text-muted small"><p> 이 포스트는 개인적으로 공부한 내용을 정리하고 필요한 분들에게 지식을 공유하기 위해 작성되었습니다. 지적하실 내용이 있다면, 언제든 댓글 또는 메일로 알려주시기를 바랍니다. 01. 요약 RoBERTa는 Robustly Optimized BERT approach의 약자이다. ELMo, GPT, BERT, XLM, XLNet 등 self-traini...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/BERTopic/" class="btn btn-outline-primary" prompt="Older"><p>(NLP) BERTopic 개념 정리</p></a> <a href="/posts/DistilBERT/" class="btn btn-outline-primary" prompt="Newer"><p>(NLP) DistilBERT 리뷰 및 설명</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center text-muted"><div class="footer-left"><p class="mb-0"> © 2023 <a href="https://github.com/zerojsh00">Sanghyeon</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/k8s/">K8S</a> <a class="post-tag" href="/tags/kubernetes/">Kubernetes</a> <a class="post-tag" href="/tags/paper-review/">Paper Review</a> <a class="post-tag" href="/tags/security/">Security</a> <a class="post-tag" href="/tags/network/">Network</a> <a class="post-tag" href="/tags/speech-ai/">Speech AI</a> <a class="post-tag" href="/tags/scheduling/">Scheduling</a> <a class="post-tag" href="/tags/nlp/">NLP</a> <a class="post-tag" href="/tags/storage/">Storage</a> <a class="post-tag" href="/tags/asr/">ASR</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/ko.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script>
