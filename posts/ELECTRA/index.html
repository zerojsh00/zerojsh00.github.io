<!DOCTYPE html><html lang="ko" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.3.2" /><meta property="og:title" content="(NLP) ELECTRA 리뷰 및 설명" /><meta name="author" content="simon sanghyeon" /><meta property="og:locale" content="ko" /><meta name="description" content="이 포스트는 개인적으로 공부한 내용을 정리하고 필요한 분들에게 지식을 공유하기 위해 작성되었습니다. 지적하실 내용이 있다면, 언제든 댓글 또는 메일로 알려주시기를 바랍니다." /><meta property="og:description" content="이 포스트는 개인적으로 공부한 내용을 정리하고 필요한 분들에게 지식을 공유하기 위해 작성되었습니다. 지적하실 내용이 있다면, 언제든 댓글 또는 메일로 알려주시기를 바랍니다." /><link rel="canonical" href="https://zerojsh00.github.io/posts/ELECTRA/" /><meta property="og:url" content="https://zerojsh00.github.io/posts/ELECTRA/" /><meta property="og:site_name" content="Simon’s Research Center" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-09-13T00:00:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="(NLP) ELECTRA 리뷰 및 설명" /><meta name="twitter:site" content="@" /><meta name="twitter:creator" content="@simon sanghyeon" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"simon sanghyeon"},"dateModified":"2022-09-13T00:00:00+09:00","datePublished":"2022-09-13T00:00:00+09:00","description":"이 포스트는 개인적으로 공부한 내용을 정리하고 필요한 분들에게 지식을 공유하기 위해 작성되었습니다. 지적하실 내용이 있다면, 언제든 댓글 또는 메일로 알려주시기를 바랍니다.","headline":"(NLP) ELECTRA 리뷰 및 설명","mainEntityOfPage":{"@type":"WebPage","@id":"https://zerojsh00.github.io/posts/ELECTRA/"},"url":"https://zerojsh00.github.io/posts/ELECTRA/"}</script><title>(NLP) ELECTRA 리뷰 및 설명 | Simon's Research Center</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Simon's Research Center"><meta name="application-name" content="Simon's Research Center"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get MODE_ATTR() { return "data-mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.notify(); } /* flipMode() */ </script> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "AMS" } }, tex2jax: { inlineMath: [ ['$', '$'] ], displayMath: [ ['$$', '$$'] ], processEscapes: true, } }); MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) { alert("Math Processing Error: "+message[1]); }); MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) { alert("Math Processing Error: "+message[1]); }); </script> <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script><meta name="google-site-verification" content="J_a4XJ2oefe52p6EYvjTFdh9LW5Yc5RyGg1HpQOjWaA" /><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" class="mx-auto"> <img src="/assets/img/avatar.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Simon's Research Center</a></div><div class="site-subtitle font-italic">성장하는 연구원의 공책</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <button class="mode-toggle btn" aria-label="Switch Mode"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/zerojsh00" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['zerojsh00','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>(NLP) ELECTRA 리뷰 및 설명</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>(NLP) ELECTRA 리뷰 및 설명</h1><div class="post-meta text-muted"><div> By <em> Sanghyeon </em></div><div class="d-flex"><div> <span> Posted <em class="timeago" data-ts="1662994800" data-toggle="tooltip" data-placement="bottom" data-tooltip-df="llll" > 2022-09-13 </em> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="2861 words"> <em>15 min</em> read</span></div></div></div><div class="post-content"><p>이 포스트는 개인적으로 공부한 내용을 정리하고 필요한 분들에게 지식을 공유하기 위해 작성되었습니다. 지적하실 내용이 있다면, 언제든 댓글 또는 메일로 알려주시기를 바랍니다.</p><hr /><h1 id="01-개요">01. 개요</h1><p>ICLR 2020에서 Google Brain 팀은 새로운 사전 학습 방법론으로 ELECTRA를 제안했다. ELECTRA는 “<strong>E</strong>fficiently <strong>L</strong>earning an <strong>E</strong>ncoder that <strong>C</strong>lassifies <strong>T</strong>oken <strong>R</strong>eplacements <strong>A</strong>ccurately”의 약자로, 사전 학습에서의 효율성을 개선하여 BERT보다 빠르게 사전 학습 할 수 있으면서 downstream task에서도 높은 성능을 보였다.</p><hr /><h1 id="02-기존bert-사전-학습-방식의-문제점">02. 기존(BERT) 사전 학습 방식의 문제점</h1><p>언어 모델이 토큰들에 대한 representation을 학습하는 대표적인 방법은 <code class="language-plaintext highlighter-rouge">denoising autoencoder</code> 방식이다. 이 방식은 BERT의 <code class="language-plaintext highlighter-rouge">Masked Language Modeling(MLM)</code> 방식이 대표적인데, 일반적으로 레이블링이 되어 있지 않은 입력 시퀀스의 일부(대략 15%)를 <strong>[MASK]</strong> 토큰으로 치환하고, 치환된 <strong>[MASK]</strong> 토큰의 원래 단어가 무엇이었는지를 맞추는 방식으로 학습한다. Transformer의 self-attention은 양방향의 문맥을 파악할 수 있으므로 이러한 MLM 방식은 매우 효과적인 결과를 낼 수 있다. 하지만 BERT 같은 커다란 언어 모델의 학습이 고작 15% 정도의 <strong>[MASK]</strong>로 치환된 토큰들만을 이용하여 학습해야 하기 때문에 계산 비용이 매우 크다는 단점이 있다.</p><p>ELECTRA의 저자들은 이처럼 적은 수의 <strong>[MASK]</strong>로 치환된 토큰들만 가지고서 큰 언어 모델을 학습하는 방식이 비효율적이라고 지적하며, MLM 방식의 사전 학습이 아닌, <code class="language-plaintext highlighter-rouge">Replaced Token Detection(RTD)</code>이라는 새로운 사전 학습 방식을 제안한다.</p><hr /><h1 id="03-electra의-구조">03. ELECTRA의 구조</h1><p><img data-src="/assets/img/2022-09-13-ELECTRA/fig01.png" alt="fig01" data-proofer-ignore></p><p>ELECTRA의 사전 학습 방식은 마치 GAN과 같이 <code class="language-plaintext highlighter-rouge">Generator</code>와 <code class="language-plaintext highlighter-rouge">Discriminator</code> 네트워크의 학습으로 이루어진다. EL<strong><code class="language-plaintext highlighter-rouge">E</code></strong>CTRA의 이름에서 알 수 있듯이, Generator와 Discriminator 모두 기본적으로 BERT처럼 Transformer의 <code class="language-plaintext highlighter-rouge">Encoder</code> 구조이다. 이때, Discriminator가 바로 ELECTRA 모델에 해당하며, Generator는 사전 학습 시 Discriminator의 입력값을 만들어주는 네트워크에 불과하다. <strong>사전 학습을 마치면 Generator는 떼어내고 Discriminator만을 이용하여 downstream task에 대해 fine tuning 한다.</strong></p><p>ELECTRA의 사전 학습 방식이 Generator와 Discriminator 구조로 이루어져 있고 jointly 학습되기 때문에 GAN과 같이 학습될 것이라고 생각할 수 있으나, 엄연히 다르다. GAN에서는 Generator가 Discriminator를 속이기 위해서 adversarial하게 학습을 하지만, ELECTRA의 사전 학습에서는 Generator가 maximum likelihood로 학습한다.</p><h2 id="generator"><span class="mr-2">Generator</span><a href="#generator" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Generator는 BERT의 사전 학습에서 Next Sentence Prediction(NSP)를 제외한 학습 방법, 즉, MLM만을 적용한 사전 학습 방법과 동일하다.</p><p>우선, 입력 토큰 시퀀스 $\mathbf{x}=[x_1, x_2, … , x_n]$이 주어졌을 때, 전체 토큰의 약 15%($k=\lceil 0.15n \rceil$)개의 토큰을 <strong>[MASK]</strong>로 치환하여 마스킹할 위치의 집합 $\mathbf{m}=[m_1, m_2, …, m_k]$을 만든다. 이때 마스킹 위치는 수식으로 $m_i \sim \text{unif}{1, n} \text{ for } i=1 \text{ to }k$ 로 표현할 수 있다.</p><p>마스킹할 위치의 집합 $\mathbf{m}$이 결정되었다면, 해당 위치의 토큰은 <strong>[MASK]</strong>로 치환하는데, 이를 $\mathbf{x}^{masked}=\text{REPLACE}(\mathbf{x}, \mathbf{m}, \text{[MASK]})$로 표현할 수 있다.</p><p>Generator는 이후 BERT의 MLM과 동일한 방식으로 <strong>[MASK]</strong>로 치환된 토큰의 원래 토큰이 무엇이었는지 예측한다. $t$번째 토큰을 예측한다고 하면, 아래와 같이 softmax로 표현할 수 있다.</p><p>$p_G(x_t | \mathbf{x}) = \cfrac{\exp{(e(x_t)^T h_G(\mathbf{x})_t)}}{\sum_x^\prime\exp{(e(x^\prime)^T h_G(\mathbf{x})_t)}}$</p><p>이때, $h_G(\mathbf{x})$는 Generator 네트워크를 거쳐 나온 contextualized vector representations를 의미한다. 따라서 $h_G(\mathbf{x})_t$는 $t$번째 <strong>[MASK]</strong> 토큰의 representation이 되겠다. 또한 $e(\cdot)$는 토큰의 임베딩을 의미한다.</p><p>최종적으로 Generator는 BERT의 MLM과 같은 손실 함수를 이용해서 학습한다.</p><p>$\mathcal{L}_\text{MLM} (\mathbf{x}, \theta_G) = \mathbb{E}\bigg( \sum_{ i \in \mathbf{m} } -\log{p_G(x_i | \mathbf{x}^{\text{masked}})} \bigg)$</p><h2 id="discriminator"><span class="mr-2">Discriminator</span><a href="#discriminator" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Discriminator는 Generator의 입력 토큰 시퀀스에서 <strong>[MASK]</strong> 부분을 Generator가 예측한 값으로 대입하여 입력값으로 사용한다. 그 예시는 다음과 같다.</p><ul><li>원래 입력 토큰 시퀀스 $\mathbf{x}$<ul><li>[ <code class="language-plaintext highlighter-rouge">the</code>, chef, <code class="language-plaintext highlighter-rouge">cooked</code>, the, meal ]</ul><li>Generator의 입력 토큰 시퀀스 $\mathbf{x}^{\text{masked}}$<ul><li>[ <code class="language-plaintext highlighter-rouge">[MASK]</code>, chef, <code class="language-plaintext highlighter-rouge">[MASK]</code>, the, meal ]<ul><li>원래 입력 토큰 시퀀스의 15%를 마스킹 함</ul></ul><li>Discriminator의 입력 토큰 시퀀스 $\mathbf{x}^{\text{corrupt}}$<ul><li>[ <code class="language-plaintext highlighter-rouge">the</code>, chef, <code class="language-plaintext highlighter-rouge">ate</code>, the, meal ]<ul><li><code class="language-plaintext highlighter-rouge">the</code> 토큰은 Generator가 정확히 예측하였음<li><code class="language-plaintext highlighter-rouge">ate</code> 토큰은 cooked가 정답이지만, Generator가 확률적으로 plausible 예측하였음</ul></ul></ul><p>즉, Discriminator의 입력 토큰 시퀀스는 Generator의 예측값을 기반으로 원래 입력 토큰 시퀀스가 재구성되기 때문에, 일부 토큰이 확률적으로 그럴싸할 수는 있지만 원래 토큰과는 다른 토큰으로 구성되어 있을 수 있다(plausible but synthetically generated replacements).</p><p>이렇게 Generator의 예측으로 <strong>[MASK]</strong> 토큰을 대체하여 재구성한 Discriminator의 입력 토큰 시퀀스를 $\mathbf{x}^{\text{corrupt}}=\text{REPLACE}(\mathbf{x}, \mathbf{m}, \mathbf{\hat{x}})$라고 표현할 수 있다. 이때 Generator의 softmax를 통해 예측한 토큰을 $\hat{x}_i \sim p_G(x_i | \mathbf{x}^{\text{masked}}) \text{ for } i \in \mathbf{m}$ 로 표현할 수 있다.</p><p>최종적으로 Discriminator는 입력 시퀀스의 각 토큰이 원래 토큰인지(<code class="language-plaintext highlighter-rouge">original</code>), 아니면 Generator에 의해 대체된 토큰(<code class="language-plaintext highlighter-rouge">replaced</code>)인지를 구분하는 문제로 학습한다. 이는 <code class="language-plaintext highlighter-rouge">sigmoid output layer</code>로 구현되며 $t$번째 토큰에 대한 예측값은 $D(\mathbf{x}^{\text{corrupt}}, t) = \text{sigmoid}(w^{T}h_{D}(\mathbf{x}^{\text{corrupt}})_t)$로 표현할 수 있다. 이때, $h_D(\mathbf{x}^{\text{corrupt}})$는 Discriminator 네트워크를 거쳐 나온 contextualized vector representations를 의미한다.</p><p>최종적으로 Discriminator는 아래와 같은 크로스 엔트로피 손실 함수를 이용해서 학습한다.</p><p>$\mathcal{L}_{\text{Disc}}(\mathbf{x}, \theta_D)=\mathbb{E}\bigg( \sum_{t=1}^{n} -1(\mathbf{x}_t^{\text{corrupt}}=\mathbf{x}_t) \log{D(\mathbf{x}^{\text{corrupt}}, t)} -1(\mathbf{x}_t^{\text{corrupt}} \neq \mathbf{x}_t) \log{(1-D(\mathbf{x}^{\text{corrupt}},t))} \bigg)$</p><h2 id="combined-loss"><span class="mr-2">Combined Loss</span><a href="#combined-loss" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>ELECTRA는 궁극적으로 다음과 같이 Generator의 loss와 Discriminator의 loss를 결합하여 손실 함수를 구성하고, 이 손실 함수가 최소화되도록 사전 학습을 진행한다.</p><p>$\min_{\theta_G, \theta_D} \sum_{\mathbf{x} \in \mathcal{X}} \mathcal{L}_\text{MLM}(\mathbf{x}, \theta_G) + \lambda \mathcal{L}_\text{Disc}(\mathbf{x}, \theta_D)$</p><p>이때 $\mathcal{X}$는 대규모 raw 텍스트 코퍼스를 의미하며, $\lambda$(논문에서는 $\lambda=50$을 사용함)는 Generator와 Discriminator의 스케일을 맞추는 파라미터이다. 한편, 사전 학습 과정에서 sampling 과정을 거치기 때문에 Discriminator의 loss는 Generator로 back-propagate 되지 않는다.</p><hr /><h1 id="04-experiments">04. Experiments</h1><h2 id="weight-sharing"><span class="mr-2">Weight Sharing</span><a href="#weight-sharing" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>만약, Generator와 Discriminator 네트워크 크기가 동일하다면, 두 네트워크의 가중치를 공유해서 사용하는 것이 더욱 효율적일 수 있을 것이라 생각할 수 있다. 이러한 호기심에 대해 ELECTRA 저자들은 두 네트워크가 동일한 크기라고 가정한 후 가중치를 공유하는 실험을 진행해보았다.</p><p>(1)가중치를 공유하지 않은 경우와 (2)토큰 임베딩만을 공유한 경우, 그리고 (3)모든 가중치를 공유한 경우 각각에 대하여 GLUE score를 측정한 결과, 각각 83.6, 84.3, 84.4점을 달성했다. 즉, 가중치를 공유하면 성능이 향상된다는 것이다. 그러나, (3)모든 가중치를 공유한 경우에는 (2)토큰 임베딩만을 공유한 경우에 비해 아주 약간의 성능 향상은 있었으나, Generator가 Discriminator와 반드시 동일한 크기만큼 커야 한다는 제약이 있다는 점을 고려하여 단점이 더 크다고 할 수 있다. 따라서 저자들은 (2)토큰 임베딩만을 공유하는 경우를 최적으로 여기고 사용한다.</p><h2 id="smaller-generators"><span class="mr-2">Smaller Generators</span><a href="#smaller-generators" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p><img data-src="/assets/img/2022-09-13-ELECTRA/fig02.png" alt="fig02" data-proofer-ignore></p><p>Generator와 Discriminator의 크기가 동일하면 그만큼 학습할 때의 계산량도 상당할 것이다. ELECTRA의 저자들은 Generator의 크기를 작게하여 계산량을 줄여보았다. 실험 결과, 오히려 Generator의 크기가 Discriminator의 크기보다 작을 때(대략 Discriminator의 1/4 ~ 1/2 크기) 더 높은 GLUE score를 달성할 수 있었다. 저자들은 이 현상에 대해서 너무나 Generator의 크기가 커지면, Discriminator는 실제 데이터의 분포를 모델링 하는 것 보다 Generator를 모델링 하는 데 파라미터를 많이 사용하게되는 단점이 있을 것이라고 해석했다.</p><h2 id="training-algorithms"><span class="mr-2">Training Algorithms</span><a href="#training-algorithms" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p><img data-src="/assets/img/2022-09-13-ELECTRA/fig03.png" alt="fig03" data-proofer-ignore></p><p>ELECTRA의 저자들은 다음과 같이 Generator와 Discriminator의 크기를 동일하게 설정한 후 Two-Stage 방식으로 학습을 해보았다.</p><ul><li>우선, Generator만 n스텝 학습한다.<li>이후, Generator의 가중치로 Discriminator의 가중치를 초기화하고, Generator의 가중치는 고정한 후, Discriminator만 n스텝 학습한다.</ul><p>또한 GAN과 같이 Generator를 adversarial한 방식으로도 구성하여 실험해보았다. 결과적으로는 다양한 시도을 했지만 성능 향상엔 큰 도움을 얻을 수 없었으며, 기본적인 방식으로 Generator와 Discriminator를 결합하여(jointly) 학습할 때 성능이 가장 좋았다.</p><h2 id="small-models"><span class="mr-2">Small Models</span><a href="#small-models" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p><img data-src="/assets/img/2022-09-13-ELECTRA/fig04.png" alt="fig04" data-proofer-ignore></p><p>저자들은 GPU 한 대에서도 빠르게 ELECTRA를 사전 학습 할 수 있도록 ELECTRA-small 모델을 개발해서 실험해보았다. BERT-Base의 하이퍼파라미터를 사용하되, 시퀀스 길이를 512 토큰에서 128 토큰으로, 배치 사이즈를 256에서 128로, 은닉층의 차원을 768에서 256으로, 임베딩 차원을 768에서 128로 줄여서 실험해보았다. 그럼에도 불구하고 Table 1에서 볼 수 있듯, 놀라울 수준의 GLUE score를 달성하였다.</p><h2 id="large-models"><span class="mr-2">Large Models</span><a href="#large-models" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p><img data-src="/assets/img/2022-09-13-ELECTRA/fig05.png" alt="fig05" data-proofer-ignore></p><p>저자들은 SOTA 모델들과 비교해보기 위해서 BERT-LARGE에 준하는 ELECTRA-LARGE 모델을 개발하여 실험해 보았다. ELECTRA-400K는 RoBERTa 사전 학습의 대략 1/4 수준의 연산만 수행한 모델인데도 불구하고, Table 2에서 알 수 있듯, GLUE dev set에서 RoBERTa와 XLNet에 준하는 성적을 거두었다. RoBERTa의 연산 수준만큼 사전 학습 한 ELECTRA-1.75M 모델은 대부분의 태스크에서 최고점을 기록했다. Table 3에서 알 수 있듯, ELECTRA는 GLUE test set에서도 대부분의 태스크에서 최고점을 달성했다.</p><p><img data-src="/assets/img/2022-09-13-ELECTRA/fig06.png" alt="fig06" data-proofer-ignore></p><p>ELECTRA는 SQuAD 데이터셋에서도 매우 우수한 결과를 보였다.</p><hr /><h1 id="05-결론">05. 결론</h1><p>ELECTRA는 언어의 representation을 학습하기 위해서 replaced token detection이라는 새로운 self-supervised task기법을 제시하였다. 이는 기존의 사전 학습 방식들에 비해서 연산을 효율적으로 수행할 수 있으며, 실제로 downstream task에서도 훌륭한 성능을 보였다.</p><hr /><h1 id="06-참고-문헌">06. 참고 문헌</h1><p>[1] <code class="language-plaintext highlighter-rouge">원 논문</code> : <a href="https://arxiv.org/abs/2003.10555">ELECTRA: pre-training text encoders as discriminators rather than generators</a><br /> [2] <code class="language-plaintext highlighter-rouge">Scatter Lab의 블로그</code> : <a href="https://tech.scatterlab.co.kr/electra-review/">tech.scatterlab.co.kr/electra-review</a></p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/natural-language-processing/'>Natural Language Processing</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/language-model/" class="post-tag no-text-decoration" >Language Model</a> <a href="/tags/nlp/" class="post-tag no-text-decoration" >NLP</a> <a href="/tags/nlu/" class="post-tag no-text-decoration" >NLU</a> <a href="/tags/paper-review/" class="post-tag no-text-decoration" >Paper Review</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=%28NLP%29+ELECTRA+%EB%A6%AC%EB%B7%B0+%EB%B0%8F+%EC%84%A4%EB%AA%85+-+Simon%27s+Research+Center&url=https%3A%2F%2Fzerojsh00.github.io%2Fposts%2FELECTRA%2F" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=%28NLP%29+ELECTRA+%EB%A6%AC%EB%B7%B0+%EB%B0%8F+%EC%84%A4%EB%AA%85+-+Simon%27s+Research+Center&u=https%3A%2F%2Fzerojsh00.github.io%2Fposts%2FELECTRA%2F" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fzerojsh00.github.io%2Fposts%2FELECTRA%2F&text=%28NLP%29+ELECTRA+%EB%A6%AC%EB%B7%B0+%EB%B0%8F+%EC%84%A4%EB%AA%85+-+Simon%27s+Research+Center" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div></div><script src="https://utteranc.es/client.js" repo="zerojsh00/zerojsh00.github.io" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/Network-Policy/">(K8S) 네트워크 정책(Network Policy) 기초 개념</a><li><a href="/posts/CNI-Weave/">(K8S) CNI Weave의 기초 개념</a><li><a href="/posts/Container-Networking-Interface/">(K8S) 네트워크 기초 정리 - CNI</a><li><a href="/posts/Storage-in-Docker/">(K8S) 도커의 스토리지(Storage in Docker)</a><li><a href="/posts/Security-Context/">(K8S) Security Context</a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/k8s/">K8S</a> <a class="post-tag" href="/tags/kubernetes/">Kubernetes</a> <a class="post-tag" href="/tags/paper-review/">Paper Review</a> <a class="post-tag" href="/tags/security/">Security</a> <a class="post-tag" href="/tags/network/">Network</a> <a class="post-tag" href="/tags/speech-ai/">Speech AI</a> <a class="post-tag" href="/tags/scheduling/">Scheduling</a> <a class="post-tag" href="/tags/nlp/">NLP</a> <a class="post-tag" href="/tags/storage/">Storage</a> <a class="post-tag" href="/tags/asr/">ASR</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="tail-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/RoBERTa/"><div class="card-body"> <em class="timeago small" data-ts="1662476400" > 2022-09-07 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>(NLP) RoBERTa 리뷰 및 설명</h3><div class="text-muted small"><p> 이 포스트는 개인적으로 공부한 내용을 정리하고 필요한 분들에게 지식을 공유하기 위해 작성되었습니다. 지적하실 내용이 있다면, 언제든 댓글 또는 메일로 알려주시기를 바랍니다. 01. 요약 RoBERTa는 Robustly Optimized BERT approach의 약자이다. ELMo, GPT, BERT, XLM, XLNet 등 self-traini...</p></div></div></a></div><div class="card"> <a href="/posts/BERTopic/"><div class="card-body"> <em class="timeago small" data-ts="1663686000" > 2022-09-21 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>(NLP) BERTopic 개념 정리</h3><div class="text-muted small"><p> 이 포스트는 개인적으로 공부한 내용을 정리하고 필요한 분들에게 지식을 공유하기 위해 작성되었습니다. 지적하실 내용이 있다면, 언제든 댓글 또는 메일로 알려주시기를 바랍니다. 01. Introduction 전통적인 토픽 모델링의 방법으로는 Latent Dirichlet Allocation(LDA)와 Non-Negative Matrix Factor...</p></div></div></a></div><div class="card"> <a href="/posts/DistilBERT/"><div class="card-body"> <em class="timeago small" data-ts="1676818800" > 2023-02-20 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>(NLP) DistilBERT 리뷰 및 설명</h3><div class="text-muted small"><p> 이 포스트는 개인적으로 공부한 내용을 정리하고 필요한 분들에게 지식을 공유하기 위해 작성되었습니다. 지적하실 내용이 있다면, 언제든 댓글 또는 메일로 알려주시기를 바랍니다. 01. Introduction 2018년, 자연어처리 영역의 위대한 한 획을 그은 BERT를 대표로하여, 자연어처리 영역은 대규모의 사전학습 언어모델(large-scal...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/CNI-Weave/" class="btn btn-outline-primary" prompt="Older"><p>(K8S) CNI Weave의 기초 개념</p></a> <a href="/posts/Service-Networking/" class="btn btn-outline-primary" prompt="Newer"><p>(K8S) 서비스 네트워킹</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center text-muted"><div class="footer-left"><p class="mb-0"> © 2023 <a href="https://github.com/zerojsh00">Sanghyeon</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/k8s/">K8S</a> <a class="post-tag" href="/tags/kubernetes/">Kubernetes</a> <a class="post-tag" href="/tags/paper-review/">Paper Review</a> <a class="post-tag" href="/tags/security/">Security</a> <a class="post-tag" href="/tags/network/">Network</a> <a class="post-tag" href="/tags/speech-ai/">Speech AI</a> <a class="post-tag" href="/tags/scheduling/">Scheduling</a> <a class="post-tag" href="/tags/nlp/">NLP</a> <a class="post-tag" href="/tags/storage/">Storage</a> <a class="post-tag" href="/tags/asr/">ASR</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/ko.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script>
