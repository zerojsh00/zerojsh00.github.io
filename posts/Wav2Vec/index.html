<!DOCTYPE html><html lang="ko" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.3.2" /><meta property="og:title" content="(Speech Recognition) Wav2Vec(1.0) 리뷰 및 설명" /><meta name="author" content="simon sanghyeon" /><meta property="og:locale" content="ko" /><meta name="description" content="이 포스트는 개인적으로 공부한 내용을 정리하고 필요한 분들에게 지식을 공유하기 위해 작성되었습니다. 지적하실 내용이 있다면, 언제든 댓글 또는 메일로 알려주시기를 바랍니다." /><meta property="og:description" content="이 포스트는 개인적으로 공부한 내용을 정리하고 필요한 분들에게 지식을 공유하기 위해 작성되었습니다. 지적하실 내용이 있다면, 언제든 댓글 또는 메일로 알려주시기를 바랍니다." /><link rel="canonical" href="https://zerojsh00.github.io/posts/Wav2Vec/" /><meta property="og:url" content="https://zerojsh00.github.io/posts/Wav2Vec/" /><meta property="og:site_name" content="Simon’s Research Center" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-07-21T00:00:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="(Speech Recognition) Wav2Vec(1.0) 리뷰 및 설명" /><meta name="twitter:site" content="@" /><meta name="twitter:creator" content="@simon sanghyeon" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"simon sanghyeon"},"dateModified":"2022-07-31T21:50:46+09:00","datePublished":"2022-07-21T00:00:00+09:00","description":"이 포스트는 개인적으로 공부한 내용을 정리하고 필요한 분들에게 지식을 공유하기 위해 작성되었습니다. 지적하실 내용이 있다면, 언제든 댓글 또는 메일로 알려주시기를 바랍니다.","headline":"(Speech Recognition) Wav2Vec(1.0) 리뷰 및 설명","mainEntityOfPage":{"@type":"WebPage","@id":"https://zerojsh00.github.io/posts/Wav2Vec/"},"url":"https://zerojsh00.github.io/posts/Wav2Vec/"}</script><title>(Speech Recognition) Wav2Vec(1.0) 리뷰 및 설명 | Simon's Research Center</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Simon's Research Center"><meta name="application-name" content="Simon's Research Center"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get MODE_ATTR() { return "data-mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.notify(); } /* flipMode() */ </script> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "AMS" } }, tex2jax: { inlineMath: [ ['$', '$'] ], displayMath: [ ['$$', '$$'] ], processEscapes: true, } }); MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) { alert("Math Processing Error: "+message[1]); }); MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) { alert("Math Processing Error: "+message[1]); }); </script> <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script><meta name="google-site-verification" content="J_a4XJ2oefe52p6EYvjTFdh9LW5Yc5RyGg1HpQOjWaA" /><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" class="mx-auto"> <img src="/assets/img/avatar.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Simon's Research Center</a></div><div class="site-subtitle font-italic">성장하는 연구원의 공책</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <button class="mode-toggle btn" aria-label="Switch Mode"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/zerojsh00" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['zerojsh00','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>(Speech Recognition) Wav2Vec(1.0) 리뷰 및 설명</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>(Speech Recognition) Wav2Vec(1.0) 리뷰 및 설명</h1><div class="post-meta text-muted"><div> By <em> Sanghyeon </em></div><div class="d-flex"><div> <span> Posted <em class="timeago" data-ts="1658329200" data-toggle="tooltip" data-placement="bottom" data-tooltip-df="llll" > 2022-07-21 </em> </span> <span> Updated <em class="timeago" data-ts="1659271846" data-toggle="tooltip" data-placement="bottom" data-tooltip-df="llll" > 2022-07-31 </em> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="1420 words"> <em>7 min</em> read</span></div></div></div><div class="post-content"><p>이 포스트는 개인적으로 공부한 내용을 정리하고 필요한 분들에게 지식을 공유하기 위해 작성되었습니다. 지적하실 내용이 있다면, 언제든 댓글 또는 메일로 알려주시기를 바랍니다.</p><hr /><h1 id="00-들어가며">00. 들어가며</h1><p>음성 전문가의 도메인 지식과 푸리에 변환 등을 거쳐 추출해내는 음성 신호인<a href="https://zerojsh00.github.io/posts/MFCC/"><code class="language-plaintext highlighter-rouge">Mel-Frequency Cepstral Coefficients(MFCC)</code></a>와는 달리, 최근에는 뉴럴 네트워크(Neural Network) 기반의 음성 신호 특징 추출이 많이 활용되고 있다. MFCC를 활용하여 음성 신호의 특징(feature)을 추출하기 위해서는 매우 복잡한 도메인 지식과 공식들이 적용되는 반면, 뉴럴 네트워크 기반의 특징 추출 방식은 많은 음성 도메인 지식을 필요로 하지 않는다는 장점이 있다.</p><p>본 포스트는 대표적인 뉴럴 네트워크 기반의 음성 신호 특징 추출 방식인 <code class="language-plaintext highlighter-rouge">Wav2Vec(1.0)</code>을 톺아보고자 한다. 최근에는 <code class="language-plaintext highlighter-rouge">Wav2Vec(2.0)</code>까지 공개되었으나, 이 글에서는 우선적으로 Wav2vec(1.0) 버전을 살펴보고자 한다.</p><hr /><h1 id="01-개요">01. 개요</h1><p>Wav2Vec이 나오기 이전(2019년도 이전), 음성 인식 모델이 좋은 성능을 보이기 위해서는 음성 신호가 텍스트로 전사되어 있는 대량의 데이터가 필요했다. 한편, 컴퓨터 비전이나 자연어처리 분야에서는 대량의 unlabeled 데이터를 이용하여 모델이 pre-training을 통해 대규모 일반 지식을 습득하고, 소규모의 labeled 데이터를 이용하여 downstream task에 fine-tuning 되는 방식이 큰 효과를 보여왔다. 음성 인식 task에서는 특히나 음성 오디오 신호가 텍스트 형태로 전사되어 있는 데이터를 구하기 어려운데, 이러한 문제를 해결하기 위해서 Wav2Vec은 <code class="language-plaintext highlighter-rouge">unsupervised pre-training</code>을 적용하고자 했다. 즉, 상대적으로 훨씬 수집하기에 수월한 unlabeled 오디오 데이터를 활용하여 pre-training을 수행하겠다는 것이다.</p><p>이렇게 pre-training된 Wav2Vec 모델은 원시 음성 오디오 신호를 입력받아서 general representation을 출력하게 되는데, 이때의 출력은 <code class="language-plaintext highlighter-rouge">speech recognition system</code>의 입력으로 활용된다. 목적식으로는 negative 오디오 샘플로부터 true 오디오 샘플을 구별해내는 <code class="language-plaintext highlighter-rouge">contrastive loss</code>를 활용한다.</p><hr /><h1 id="02-모델">02. 모델</h1><p><img data-src="/assets/img/2022-07-21-Wav2Vec/fig01.png" alt="fig01" data-proofer-ignore> <em>오디오 데이터 $\mathcal{X}$로부터 pre-training 되는 과정</em></p><p>Wav2Vec은 두 개의 convolution neural network가 쌓여 있는 구조로, 네트워크 $f:\mathcal{X} \mapsto \mathcal{Z}$를 <code class="language-plaintext highlighter-rouge">encoder network</code>, $g: \mathcal{Z} \mapsto \mathcal{C}$를 <code class="language-plaintext highlighter-rouge">context network</code>라고 부른다.</p><p>encoder network $f$는 5층짜리 CNN으로 구성되었고, 원시 음성 신호 $\mathbf{x}_i \in \mathcal{X}$를 입력으로 받아서 low frequency feature representation $\mathbf{z_i} \in \mathcal{Z}$로 인코딩한다. 간단히 말해, 오디오 시그널을 latent space $\mathcal{Z}$로 임베딩했다고 보아도 무방할 것이다.</p><p>context network $g$는 9층짜리 CNN으로 구성되었고, encoder network로부터 나오는 multiple latent representation $\mathbf{z}_i, …, \mathbf{z}_{i-v}$를 single contextualized tensor $\mathbf{c}_{i}=g(\mathbf{z}_i, …, \mathbf{z}_{i-v})$로 변환한다. 이는 인코더로부터 나온 여러 타임 스텝들을 하나로 묶어주는 역할을 수행하는데, 이 과정에서 각 타임 스텝 representation의 맥락이 파악된다.</p><p>encoder network와 context network 모두 512 채널의 causal convolution network가 적용되었고, group normalization 및 ReLU 등이 적용되었다. 참고로, <code class="language-plaintext highlighter-rouge">causal convolution</code>이란, 다음 레이어에 미래의 값이 들어가지 않는 형태를 의미한다.</p><hr /><h1 id="03-목적식">03. 목적식</h1><p><strong>[수식01]</strong> $\mathcal{L}_k=- \sum_{i=1}^{T-k}(\log \sigma ({\mathbf{z}\top_{i+k}} {h_k(c_i)}) + \lambda \mathbb{E}_{\mathbf{\tilde{z}} \sim p_n}[\log \sigma (-{\mathbf{\tilde{z}}\top} {h_k}(c_i))])$</p><p>Wav2Vec은 위의 contrastive loss를 최소화하며 학습한다. Wav2Vec은 학습과정에서 매 스텝 $k=1, …, K$ 마다 true(positive) sample인 $\mathbf{z}_{i+k}$를 negative sample인 $\mathbf{\tilde{z}}$로부터 구별하는 task를 수행한다. $\sigma$는 시그모이드 함수이며, $\sigma ({\mathbf{z}\top_{i+k}} {h_k(c_i)})$는 true sample의 확률이 된다. 이때 $h_k(c_i)=W_k \mathbf{c}_i+\mathbf{b}_k$는 affine transformation이다. 쉽게 말해, FC layer 한개라고 보면 되겠다. 또한 $\lambda$는 negative sample의 수를 의미한다. 한편, $\mathbf{\tilde{z}}$는 현재 배치의 다른 음성의 hidden representation들 가운데 랜덤으로 추출하여 만든다.</p><p><strong>[수식02]</strong> $\mathcal{L}= \sum_{k=1}^K \mathcal{L}_k$</p><p>최종적으로는 매 $k$ 스텝에서 구해진 loss들의 합을 최소화하면서 학습되는데, 그 과정에서 true sample로 이루어진 쌍($\mathbf{z}_{i+k}$와 $h_k(c_i)$) 관계의 representation은 벡터 공간에서 가까워지고, 네거티브 쌍은 멀어지게 된다. 즉, encoder network와 context network가 입력 음성의 다음 시퀀스가 무엇일지에 관한 정보를 음성 피처에 녹여내는 것이다.</p><hr /><h1 id="04-기타">04. 기타</h1><h2 id="데이터"><span class="mr-2">데이터</span><a href="#데이터" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>TIMIT, WSJ, Librispeech 데이터셋을 사용하였으며, 모두 16kH의 sampling rate로 구성된 영어 오디오 데이터이다.</p><h2 id="디코딩"><span class="mr-2">디코딩</span><a href="#디코딩" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>디코딩 과정에서는 4-gram KenLM 언어모델, word-based convolution 언어모델, character-based convolution 언어모델이 활용되었다.</p><h2 id="코드-예시"><span class="mr-2">코드 예시</span><a href="#코드-예시" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">fairseq.models.wav2vec</span> <span class="kn">import</span> <span class="n">Wav2VecModel</span>

<span class="n">cp</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="s">'/path/to/wav2vec.pt'</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Wav2VecModel</span><span class="p">.</span><span class="nf">build_model</span><span class="p">(</span><span class="n">cp</span><span class="p">[</span><span class="s">'args'</span><span class="p">],</span> <span class="n">task</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">(</span><span class="n">cp</span><span class="p">[</span><span class="s">'model'</span><span class="p">])</span>
<span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>

<span class="n">wav_input_16khz</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10000</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">feature_extractor</span><span class="p">(</span><span class="n">wav_input_16khz</span><span class="p">)</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">feature_aggregator</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
</pre></table></code></div></div><hr /><h1 id="05-참고-문헌">05. 참고 문헌</h1><p>[1] <a href="https://arxiv.org/pdf/1904.05862.pdf">Schneider, Steffen, et al. “wav2vec: Unsupervised pre-training for speech recognition.” arXiv preprint arXiv:1904.05862</a><br /> [2] <a href="https://ratsgo.github.io/speechbook/docs/neuralfe/wav2vec">ratsgo 님의 블로그</a><br /> [3] <a href="https://youtu.be/mPtyfqWHs3s">김정희 님의 발표자료</a></p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/speech-recognition/'>Speech Recognition</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/speech-ai/" class="post-tag no-text-decoration" >Speech AI</a> <a href="/tags/wav2vec/" class="post-tag no-text-decoration" >Wav2Vec</a> <a href="/tags/feature-extraction/" class="post-tag no-text-decoration" >Feature Extraction</a> <a href="/tags/paper-review/" class="post-tag no-text-decoration" >Paper Review</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=%28Speech+Recognition%29+Wav2Vec%281.0%29+%EB%A6%AC%EB%B7%B0+%EB%B0%8F+%EC%84%A4%EB%AA%85+-+Simon%27s+Research+Center&url=https%3A%2F%2Fzerojsh00.github.io%2Fposts%2FWav2Vec%2F" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=%28Speech+Recognition%29+Wav2Vec%281.0%29+%EB%A6%AC%EB%B7%B0+%EB%B0%8F+%EC%84%A4%EB%AA%85+-+Simon%27s+Research+Center&u=https%3A%2F%2Fzerojsh00.github.io%2Fposts%2FWav2Vec%2F" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fzerojsh00.github.io%2Fposts%2FWav2Vec%2F&text=%28Speech+Recognition%29+Wav2Vec%281.0%29+%EB%A6%AC%EB%B7%B0+%EB%B0%8F+%EC%84%A4%EB%AA%85+-+Simon%27s+Research+Center" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div></div><script src="https://utteranc.es/client.js" repo="zerojsh00/zerojsh00.github.io" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/Network-Policy/">(K8S) 네트워크 정책(Network Policy) 기초 개념</a><li><a href="/posts/CNI-Weave/">(K8S) CNI Weave의 기초 개념</a><li><a href="/posts/Container-Networking-Interface/">(K8S) 네트워크 기초 정리 - CNI</a><li><a href="/posts/Storage-in-Docker/">(K8S) 도커의 스토리지(Storage in Docker)</a><li><a href="/posts/Security-Context/">(K8S) Security Context</a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/k8s/">K8S</a> <a class="post-tag" href="/tags/kubernetes/">Kubernetes</a> <a class="post-tag" href="/tags/paper-review/">Paper Review</a> <a class="post-tag" href="/tags/security/">Security</a> <a class="post-tag" href="/tags/network/">Network</a> <a class="post-tag" href="/tags/speech-ai/">Speech AI</a> <a class="post-tag" href="/tags/scheduling/">Scheduling</a> <a class="post-tag" href="/tags/nlp/">NLP</a> <a class="post-tag" href="/tags/storage/">Storage</a> <a class="post-tag" href="/tags/asr/">ASR</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="tail-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/VQWav2Vec/"><div class="card-body"> <em class="timeago small" data-ts="1658415600" > 2022-07-22 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>(Speech Recognition) VQ-Wav2Vec 리뷰 및 설명</h3><div class="text-muted small"><p> 이 포스트는 개인적으로 공부한 내용을 정리하고 필요한 분들에게 지식을 공유하기 위해 작성되었습니다. 지적하실 내용이 있다면, 언제든 댓글 또는 메일로 알려주시기를 바랍니다. 01. 개요 VQ-Wav2Vec의 핵심은 Wav2Vec에 Vector Quantization을 적용하였다는 점이다. VQ-Wav2Vec은 Wav2Vec 방식과 유사한 sel...</p></div></div></a></div><div class="card"> <a href="/posts/Wav2Vec2/"><div class="card-body"> <em class="timeago small" data-ts="1659193200" > 2022-07-31 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>(Speech Recognition) Wav2Vec2.0 리뷰 및 설명</h3><div class="text-muted small"><p> 이 포스트는 개인적으로 공부한 내용을 정리하고 필요한 분들에게 지식을 공유하기 위해 작성되었습니다. 지적하실 내용이 있다면, 언제든 댓글 또는 메일로 알려주시기를 바랍니다. 00. 들어가며 2020년, Facebook에서 Wav2Vec 2.0을 발표했다. 앞서 살펴보았던 Wav2Vec 및 VQ-Wav2Vec과 마찬가지로, Wav2Vec 2.0 ...</p></div></div></a></div><div class="card"> <a href="/posts/MFCC/"><div class="card-body"> <em class="timeago small" data-ts="1658242800" > 2022-07-20 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>(Speech Recognition) 음성 신호 특징 추출과 MFCC</h3><div class="text-muted small"><p> 이 포스트는 개인적으로 공부한 내용을 정리하고 필요한 분들에게 지식을 공유하기 위해 작성되었습니다. 지적하실 내용이 있다면, 언제든 댓글 또는 메일로 알려주시기를 바랍니다. 00. 들어가며 Wav2Vec과 같이 뉴럴 네트워크 기반의 음성 신호 특징 추출 기법이 개발되기 전에는 음성 도메인 지식과 공식들에 기반하여 음성 신호의 특징을 추출하였다. 대표...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/MFCC/" class="btn btn-outline-primary" prompt="Older"><p>(Speech Recognition) 음성 신호 특징 추출과 MFCC</p></a> <a href="/posts/VQWav2Vec/" class="btn btn-outline-primary" prompt="Newer"><p>(Speech Recognition) VQ-Wav2Vec 리뷰 및 설명</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center text-muted"><div class="footer-left"><p class="mb-0"> © 2023 <a href="https://github.com/zerojsh00">Sanghyeon</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/k8s/">K8S</a> <a class="post-tag" href="/tags/kubernetes/">Kubernetes</a> <a class="post-tag" href="/tags/paper-review/">Paper Review</a> <a class="post-tag" href="/tags/security/">Security</a> <a class="post-tag" href="/tags/network/">Network</a> <a class="post-tag" href="/tags/speech-ai/">Speech AI</a> <a class="post-tag" href="/tags/scheduling/">Scheduling</a> <a class="post-tag" href="/tags/nlp/">NLP</a> <a class="post-tag" href="/tags/storage/">Storage</a> <a class="post-tag" href="/tags/asr/">ASR</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/ko.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script>
