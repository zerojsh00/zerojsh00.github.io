<!DOCTYPE html><html lang="ko" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.3.2" /><meta property="og:title" content="(Speech Recognition) Conformer 리뷰 및 설명" /><meta name="author" content="simon sanghyeon" /><meta property="og:locale" content="ko" /><meta name="description" content="이 포스트는 개인적으로 공부한 내용을 정리하고 필요한 분들에게 지식을 공유하기 위해 작성되었습니다. 지적하실 내용이 있다면, 언제든 댓글 또는 메일로 알려주시기를 바랍니다." /><meta property="og:description" content="이 포스트는 개인적으로 공부한 내용을 정리하고 필요한 분들에게 지식을 공유하기 위해 작성되었습니다. 지적하실 내용이 있다면, 언제든 댓글 또는 메일로 알려주시기를 바랍니다." /><link rel="canonical" href="https://zerojsh00.github.io/posts/Conformer/" /><meta property="og:url" content="https://zerojsh00.github.io/posts/Conformer/" /><meta property="og:site_name" content="Simon’s Research Center" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-07-28T00:00:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="(Speech Recognition) Conformer 리뷰 및 설명" /><meta name="twitter:site" content="@" /><meta name="twitter:creator" content="@simon sanghyeon" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"simon sanghyeon"},"dateModified":"2022-07-31T21:50:46+09:00","datePublished":"2022-07-28T00:00:00+09:00","description":"이 포스트는 개인적으로 공부한 내용을 정리하고 필요한 분들에게 지식을 공유하기 위해 작성되었습니다. 지적하실 내용이 있다면, 언제든 댓글 또는 메일로 알려주시기를 바랍니다.","headline":"(Speech Recognition) Conformer 리뷰 및 설명","mainEntityOfPage":{"@type":"WebPage","@id":"https://zerojsh00.github.io/posts/Conformer/"},"url":"https://zerojsh00.github.io/posts/Conformer/"}</script><title>(Speech Recognition) Conformer 리뷰 및 설명 | Simon's Research Center</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Simon's Research Center"><meta name="application-name" content="Simon's Research Center"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get MODE_ATTR() { return "data-mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.notify(); } /* flipMode() */ </script> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "AMS" } }, tex2jax: { inlineMath: [ ['$', '$'] ], displayMath: [ ['$$', '$$'] ], processEscapes: true, } }); MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) { alert("Math Processing Error: "+message[1]); }); MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) { alert("Math Processing Error: "+message[1]); }); </script> <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script><meta name="google-site-verification" content="J_a4XJ2oefe52p6EYvjTFdh9LW5Yc5RyGg1HpQOjWaA" /><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" class="mx-auto"> <img src="/assets/img/avatar.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Simon's Research Center</a></div><div class="site-subtitle font-italic">성장하는 연구원의 공책</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <button class="mode-toggle btn" aria-label="Switch Mode"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/zerojsh00" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['zerojsh00','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>(Speech Recognition) Conformer 리뷰 및 설명</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>(Speech Recognition) Conformer 리뷰 및 설명</h1><div class="post-meta text-muted"><div> By <em> Sanghyeon </em></div><div class="d-flex"><div> <span> Posted <em class="timeago" data-ts="1658934000" data-toggle="tooltip" data-placement="bottom" data-tooltip-df="llll" > 2022-07-28 </em> </span> <span> Updated <em class="timeago" data-ts="1659271846" data-toggle="tooltip" data-placement="bottom" data-tooltip-df="llll" > 2022-07-31 </em> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="2256 words"> <em>12 min</em> read</span></div></div></div><div class="post-content"><p>이 포스트는 개인적으로 공부한 내용을 정리하고 필요한 분들에게 지식을 공유하기 위해 작성되었습니다. 지적하실 내용이 있다면, 언제든 댓글 또는 메일로 알려주시기를 바랍니다.</p><hr /><p>💡 2020년, 구글에서 발표한 “Conformer : Convolution-augmented Transformer for Speech Recognition” 논문을 설명한 글이다.</p><h1 id="01-들어가며">01. 들어가며</h1><h2 id="기존-end-to-end-자동-음성-인식-접근-방식"><span class="mr-2">기존 end-to-end 자동 음성 인식 접근 방식</span><a href="#기존-end-to-end-자동-음성-인식-접근-방식" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>acoustic model, pronunciation model, language model 등 수많은 컴포넌트들로 이루어져있었던 과거의 음성 인식 아키텍처는 딥러닝 기술이 도입되면서 end-to-end 방식으로 바뀌어왔다. conformer가 발표되었던 2020년까지도 딥러닝을 활용한 end-to-end ASR에 대한 다양한 접근 방식들이 있었다.</p><p>대표적으로 <code class="language-plaintext highlighter-rouge">Recurrent Neural Network(RNN)</code>를 활용한 접근 방식이 있었다. RNN은 음향 신호 sequence에 대한 time step 별 의존성(dependency)을 포착하는 데 탁월하기 때문에 많은 연구들이 RNN을 적용하였다.</p><ul><li>RNN 기반의 ASR 선행 연구<ul><li><a href="https://ieeexplore.ieee.org/abstract/document/8462105">State-of-the-Art Speech Recognition with Sequence-to-Sequence Models</a><li><a href="https://arxiv.org/abs/1801.00841">Exploring Architectures, Data and Units For Streaming End-to-End Speech Recognition with RNN-Transducer</a><li><a href="https://arxiv.org/abs/1811.06621">Streaming End-to-end Speech Recognition For Mobile Devices</a></ul></ul><p>한편, 최근 <code class="language-plaintext highlighter-rouge">transformer</code>는 self-attention 기법으로 sequence의 거리가 멀어도 맥락 정보들을 잘 파악할 수 있다. 이러한 장점으로 RNN 기반의 NLP 판도를 바꾼 transformer는 컴퓨터비전 뿐만 아니라 end-to-end 음성 인식에도 큰 기여를 했다.</p><ul><li>transformer 기반의 ASR 선행 연구<ul><li><a href="https://arxiv.org/abs/2002.02562">Transformer Transducer: A Streamable Speech Recognition Model with Transformer Encoders and RNN-T Loss</a></ul></ul><p>그리고, <code class="language-plaintext highlighter-rouge">Convolution Neural Network(CNN)</code> 역시 레이어마다 receptive field를 통해 지역적인 맥락 정보를 포착하는 데 효과적이므로 end-to-end 음성 인식에서도 성공적으로 사용되어왔다.</p><ul><li>CNN 기반의 ASR 선행 연구<ul><li><a href="https://arxiv.org/abs/1904.03288">Jasper: An End-to-End Convolutional Neural Acoustic Model</a><li><a href="https://arxiv.org/abs/1910.10261">QuartzNet: Deep Automatic Speech Recognition with 1D Time-Channel Separable Convolutions</a><li><a href="https://arxiv.org/abs/2005.03191">ContextNet: Improving Convolutional Neural Networks for Automatic Speech Recognition with Global Context</a></ul></ul><h2 id="기존-end-to-end-방식의-한계"><span class="mr-2">기존 end-to-end 방식의 한계</span><a href="#기존-end-to-end-방식의-한계" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>앞서, 최근의 end-to-end 자동 음성 인식 모델로써 transformer와 CNN이 많이 활용된다고 하였으나, 이 두 모델 모두 근본적으로 한계점이 존재한다. transformer의 경우 긴 길이의 context를 <code class="language-plaintext highlighter-rouge">global</code>하게 파악할 수 있는 반면, 지역적인 패턴 정보를 섬세하게 파악하는 데는 강점이 있지 않다. 반면, CNN은 지역적인(<code class="language-plaintext highlighter-rouge">local</code>) 패턴 정보를 파악하는 데는 탁월하지만, 지역적인 정보를 활용해서 global 패턴 정보를 파악하기 위해서는 아주 많은 레이어와 그에 따른 파라미터가 필요하다.</p><p>따라서 conformer의 저자들은 transformer의 장점과 CNN의 장점을 한 데 모아 그 효과를 극대화 할 수 있도록 자동 음성 인식 모델에 transformer의 self-attention과 convolution 연산을 합쳐서 사용할 것을 제안한다. 즉, conformer란 <strong>CON</strong>volution + trans<strong>FORMER</strong>라는 이름에서도 알 수 있듯이 transformer와 CNN의 장점을 합친 모델이다.</p><hr /><h1 id="02-conformer-모델">02. Conformer 모델</h1><p>conformer는 <code class="language-plaintext highlighter-rouge">conformer encoder</code>와 <code class="language-plaintext highlighter-rouge">LSTM decoder</code>로 구성되어 있다. LSTM decoder는 단순히 글자들의 sequence를 출력하기 위한 것이므로, 여기서는 conformer encoder 위주로 살펴보겠다. conformer encoder의 전체적인 아키텍처는 [그림01]과 같다.</p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 500 500'%3E%3C/svg%3E" data-src="/assets/img/2022-07-28-Conformer/fig01.png" alt="fig01" width="500" height="500" data-proofer-ignore> <em>[그림01] conformer의 encoder</em></p><p>conformer의 encoder는 ASR 데이터 증강 기법인 <a href="https://www.isca-speech.org/archive_v0/Interspeech_2019/pdfs/2680.pdf"><code class="language-plaintext highlighter-rouge">SpecAug</code></a>를 거침으로써 시작된다. 음성 신호는 SpecAug 레이어를 거친 후, <code class="language-plaintext highlighter-rouge">Convolution Subsampling</code> 레이어를 통해 특징을 추출하고, <code class="language-plaintext highlighter-rouge">Linear</code> 레이어 및 <code class="language-plaintext highlighter-rouge">Dropout</code>을 거쳐 <code class="language-plaintext highlighter-rouge">Conformer Block</code>을 통과함으로써 encoding된다. 여기서의 핵심은 단연 conformer block이다.</p><h2 id="conformer-block"><span class="mr-2">Conformer Block</span><a href="#conformer-block" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p><img data-src="/assets/img/2022-07-28-Conformer/fig02.png" alt="fig02" data-proofer-ignore> <em>[그림02] transformer block을 변형한 (좌)macaron-net과 (우)conformer block</em></p><p>conformer block은 <code class="language-plaintext highlighter-rouge">macaron-like</code>한 방식으로 transformer 블록을 변형한다. macaron-like 하다라는 말은 transformer를 numerical ODE solver의 관점에서 해석하고자 했던 연구인 <code class="language-plaintext highlighter-rouge">[macaron-net]</code>(https://arxiv.org/abs/1906.02762) 방식을 차용했음을 의미한다. 이는 기존의 <code class="language-plaintext highlighter-rouge">Multi-Head Self Attention(MHSA)</code> → <code class="language-plaintext highlighter-rouge">Position-wise Feed Forward(FFN)</code>로 이어지는 transformer block 구조를 마치 마카롱 모양처럼 <code class="language-plaintext highlighter-rouge">FFN</code> → <code class="language-plaintext highlighter-rouge">MHSA</code> → <code class="language-plaintext highlighter-rouge">FFN</code> 방식으로 변경한 것이다. 이때 FFN에서는 <code class="language-plaintext highlighter-rouge">half-step residual weights</code>라는 방식으로 독특하게 residual connection을 구성한다. $\tilde{x}_i = x_i + \cfrac{1}{2} \text{FFN}(x_i)$와 같이 residual connection에 1/2만큼의 weight를 주는 방식이다.</p><p>macaron-like한 conformer는 <code class="language-plaintext highlighter-rouge">FFN</code> → <code class="language-plaintext highlighter-rouge">MHSA</code> → <code class="language-plaintext highlighter-rouge">Convolution Module</code> → <code class="language-plaintext highlighter-rouge">FFN</code>로 이루어져있으며, 아래와 같이 표현할 수 있다.</p><p>$\tilde{x}_i = x_i + \cfrac{1}{2} \text{FFN}(x_i)$</p><p>$x’_i = \tilde{x}_i + \text{MHSA}(\tilde{x}_i)$</p><p>$x’'_i=x’_i + \text{Conv}(x’_i)$</p><p>$y_i = \text{Layernorm}(x’'_i + \cfrac{1}{2} \text{FFN}(x’'_i))$</p><p>$x_i$는 $i$번째 conformer block의 입력값을 의미하고 $y_i$는 encoding된 출력값을 의미한다. 그럼, macaron-like FFN 사이에 있는 각각의 모듈에 대해서 살펴보도록 하겠다.</p><h2 id="multi-head-self-attentionmhsa-module"><span class="mr-2">Multi-Head Self-Attention(MHSA) Module</span><a href="#multi-head-self-attentionmhsa-module" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p><img data-src="/assets/img/2022-07-28-Conformer/fig03.png" alt="fig03" data-proofer-ignore> <em>[그림03] conformer의 multi-head self-attention 모듈</em></p><p>먼저, MHSA 모듈을 살펴보겠다. 기본적인 transformer의 MHSA와는 달리, Transformer-XL의 relative positional embedding 기법을 적용한 MHSA를 사용하였다.</p><p>다음은 Transformer-XL의 relative positional embedding에 대한 설명이다.</p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 500 500'%3E%3C/svg%3E" data-src="/assets/img/2022-07-28-Conformer/fig04.png" alt="fig04" width="500" height="500" data-proofer-ignore> <img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 500 500'%3E%3C/svg%3E" data-src="/assets/img/2022-07-28-Conformer/fig05.png" alt="fig05" width="500" height="500" data-proofer-ignore> <img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 500 500'%3E%3C/svg%3E" data-src="/assets/img/2022-07-28-Conformer/fig06.png" alt="fig06" width="500" height="500" data-proofer-ignore> <img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 500 500'%3E%3C/svg%3E" data-src="/assets/img/2022-07-28-Conformer/fig07.png" alt="fig07" width="500" height="500" data-proofer-ignore></p><p>저자들에 따르면, relative positional embedding을 적용하면 self-attention 모듈이 길이가 다른 입력값에 대해서도 잘 generalize 할 수 있기 때문에 발화의 길이가 상이하더라도 강건한 encoder가 될 수 있다고 한다. 이처럼 relative positional embedding을 적용하여 MHSA을 수행되고나면 dropout을 거쳐 residual connection 된 후 다음 단계인 Convolution Module로 넘어가게 된다.</p><h2 id="convolution-module"><span class="mr-2">Convolution Module</span><a href="#convolution-module" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p><img data-src="/assets/img/2022-07-28-Conformer/fig08.png" alt="fig08" data-proofer-ignore> <em>[그림04] conformer의 convolution 모듈</em></p><p>MHSA 모듈을 거친 representation은 Convolution Module로 넘어온다. Convolution Module에서는 Conv 연산을 통해서 음성 신호 representation의 지역적인 정보를 학습한다. Convolution Module로 넘어온 음성 신호 representation은 Layernorm을 거쳐 <a href="https://paperswithcode.com/method/pointwise-convolution">Pointwise Conv</a> → <a href="https://paperswithcode.com/method/glu">Glu Activation</a> → <a href="https://paperswithcode.com/method/depthwise-convolution">1D DepthwiseConv</a> → BatchNorm → <a href="https://paperswithcode.com/method/swish">Swish Activation</a> → Pointwise Conv → Dropout을 거친다. 그리고 다시금 FFN module로 흘러간다.</p><h2 id="feed-forward-module"><span class="mr-2">Feed Forward Module</span><a href="#feed-forward-module" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p><img data-src="/assets/img/2022-07-28-Conformer/fig09.png" alt="fig09" data-proofer-ignore> <em>[그림05] conformer의 feed forward 모듈</em></p><p>FFN 모듈에 대한 세부 사항들은 [그림05]와 같다. 다른 모듈에서와 동일하게 Layernorm은 기본적으로 적용한 후, 2회의 Linear Layer 등을 통과하는 구조다.</p><hr /><h1 id="03-experiments">03. Experiments</h1><p>970 시간의 labeled speech dataset인 LibriSpeech를 이용해서 evaluate 하였다. 추가적으로 활용한 언어모델을 학습하기 위해 800M word token text-only corpus를 사용했다고 한다.</p><p>decoder는 단순히 한 층을 사용한 LSTM decoder를 사용하였으며, 학습한 언어모델은 <code class="language-plaintext highlighter-rouge">shallow fusion</code> 방식으로 활용했다. 참고로 shallow fusion의 개념은 [그림06]과 같다.</p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 400 400'%3E%3C/svg%3E" data-src="/assets/img/2022-07-28-Conformer/fig10.png" alt="fig10" width="400" height="400" data-proofer-ignore> <em>[그림06] shallow fusion 도식</em></p><p>$\mathbf{y}^* = \arg \max_{\mathbf{y}} (\log P_{e2e}(\mathbf{y} | \mathbf{x}) + \lambda \log P_{LM}(\mathbf{y}))$</p><p>즉, 음성 신호 sequence $\mathbf{x}$가 주어졌을 때 <code class="language-plaintext highlighter-rouge">end-to-end(e2e)</code> 방식으로 글자의 sequence $\mathbf{y}$를 decoding 하는 모델 $P(\mathbf{y} | \mathbf{x})$(여기서는 conformer encoder + LSTM decoder)과 별도로 학습한 언어모델(여기서는 3층짜리 LSTM)을 함께 고려하여 최종 출력될 글자의 확률을 결정하는 기법인 것이다.</p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 500 500'%3E%3C/svg%3E" data-src="/assets/img/2022-07-28-Conformer/fig11.png" alt="fig11" width="500" height="500" data-proofer-ignore> <em>[표01] conformer 모델 크기별 파라미터</em></p><p>conformer도 모델의 크기에 따라서 Conformer(S), Conformer(M), Conformer(L)로 구분할 수 있으며, 각각의 파라미터는 [표01]과 같다.</p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 500 500'%3E%3C/svg%3E" data-src="/assets/img/2022-07-28-Conformer/fig12.png" alt="fig12" width="500" height="500" data-proofer-ignore> <em>[표02] conformer 모델의 성능 비교표</em></p><p>[표02]는 confermer가 공개될 당시의 SOTA model이었던 ContextNet, Transformer Transducer 등과 LibriSpeech 데이터셋을 기준으로 <code class="language-plaintext highlighter-rouge">Word Error Rate(WER)</code> 성능을 비교한 결과이다. 언어모델을 사용하지 않은 Conformer(M) 모델로도 기존의 Transformer나 LSTM, 또는 CNN을 기반으로 한 모델보다 우월한 성능을 보임을 알 수 있다. 언어모델을 덧붙여 사용한 conformer 모델들은 가장 낮은 WER을 달성하면서 기존의 SOTA 모델을 뛰어넘었다.</p><hr /><h1 id="04-정리하며">04. 정리하며</h1><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 500 500'%3E%3C/svg%3E" data-src="/assets/img/2022-07-28-Conformer/fig13.png" alt="fig13" width="500" height="500" data-proofer-ignore> <em>[그림07] 2022년 7월 말 기준 LibriSpeech SOTA 랭킹</em></p><p>[그림07]은 현재(2022년 7월 말)를 기준으로 <a href="https://paperswithcode.com/sota/speech-recognition-on-librispeech-test-clean">paperswithcode.com</a>에서 발췌한 LibriSpeech test-clean 데이터 기준 음성 인식 SOTA 모델 랭킹이다. 2020년에 제안된 conformer는 그 자체로 아직도 상위 ranking에 있으며, 현재 기준 최고 성능 역시 conformer를 활용한 모델이다. global context를 파악하는 transformer의 장점과 local context를 파악하는 CNN의 장점을 합쳤다는 점이 확실하게 작용한 것 같다.</p><hr /><h1 id="05-참고-문헌">05. 참고 문헌</h1><p>[1] <code class="language-plaintext highlighter-rouge">원 논문</code> : <a href="https://arxiv.org/abs/2005.08100">Gulati, Anmol, et al. “Conformer: Convolution-augmented transformer for speech recognition.” arXiv preprint arXiv:2005.08100 (2020).</a><br /> [2] <code class="language-plaintext highlighter-rouge">Macaron-Net</code> : <a href="https://arxiv.org/abs/1906.02762">Lu, Yiping, et al. “Understanding and improving transformer from a multi-particle dynamic system point of view.” arXiv preprint arXiv:1906.02762 (2019).</a><br /> [3] <code class="language-plaintext highlighter-rouge">Shallow Fusion</code> : <a href="https://www.arxiv-vanity.com/papers/2104.04487/">Cabrera, Rodrigo, et al. “Language model fusion for streaming end to end speech recognition.” arXiv preprint arXiv:2104.04487 (2021).</a></p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/speech-recognition/'>Speech Recognition</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/speech-ai/" class="post-tag no-text-decoration" >Speech AI</a> <a href="/tags/asr/" class="post-tag no-text-decoration" >ASR</a> <a href="/tags/end-to-end/" class="post-tag no-text-decoration" >End to End</a> <a href="/tags/paper-review/" class="post-tag no-text-decoration" >Paper Review</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=%28Speech+Recognition%29+Conformer+%EB%A6%AC%EB%B7%B0+%EB%B0%8F+%EC%84%A4%EB%AA%85+-+Simon%27s+Research+Center&url=https%3A%2F%2Fzerojsh00.github.io%2Fposts%2FConformer%2F" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=%28Speech+Recognition%29+Conformer+%EB%A6%AC%EB%B7%B0+%EB%B0%8F+%EC%84%A4%EB%AA%85+-+Simon%27s+Research+Center&u=https%3A%2F%2Fzerojsh00.github.io%2Fposts%2FConformer%2F" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fzerojsh00.github.io%2Fposts%2FConformer%2F&text=%28Speech+Recognition%29+Conformer+%EB%A6%AC%EB%B7%B0+%EB%B0%8F+%EC%84%A4%EB%AA%85+-+Simon%27s+Research+Center" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div></div><script src="https://utteranc.es/client.js" repo="zerojsh00/zerojsh00.github.io" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/Network-Policy/">(K8S) 네트워크 정책(Network Policy) 기초 개념</a><li><a href="/posts/CNI-Weave/">(K8S) CNI Weave의 기초 개념</a><li><a href="/posts/Container-Networking-Interface/">(K8S) 네트워크 기초 정리 - CNI</a><li><a href="/posts/Storage-in-Docker/">(K8S) 도커의 스토리지(Storage in Docker)</a><li><a href="/posts/Security-Context/">(K8S) Security Context</a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/k8s/">K8S</a> <a class="post-tag" href="/tags/kubernetes/">Kubernetes</a> <a class="post-tag" href="/tags/paper-review/">Paper Review</a> <a class="post-tag" href="/tags/security/">Security</a> <a class="post-tag" href="/tags/network/">Network</a> <a class="post-tag" href="/tags/speech-ai/">Speech AI</a> <a class="post-tag" href="/tags/scheduling/">Scheduling</a> <a class="post-tag" href="/tags/nlp/">NLP</a> <a class="post-tag" href="/tags/storage/">Storage</a> <a class="post-tag" href="/tags/asr/">ASR</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="tail-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/Connectionist-Temporal-Classification/"><div class="card-body"> <em class="timeago small" data-ts="1658674800" > 2022-07-25 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>(Speech Recognition) Connectionist Temporal Classification 리뷰 및 설명</h3><div class="text-muted small"><p> 이 포스트는 개인적으로 공부한 내용을 정리하고 필요한 분들에게 지식을 공유하기 위해 작성되었습니다. 지적하실 내용이 있다면, 언제든 댓글 또는 메일로 알려주시기를 바랍니다. 상당 부분 ratsgo’s speech book 내용을 참고하였습니다. ratsgo 님께 감사드립니다. 01. 개요 Recurrent Neural Network(RN...</p></div></div></a></div><div class="card"> <a href="/posts/Listen,-Attend-and-Spell/"><div class="card-body"> <em class="timeago small" data-ts="1658761200" > 2022-07-26 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>(Speech Recognition) Listen, Attend and Spell (LAS) 리뷰 및 설명</h3><div class="text-muted small"><p> 이 포스트는 개인적으로 공부한 내용을 정리하고 필요한 분들에게 지식을 공유하기 위해 작성되었습니다. 지적하실 내용이 있다면, 언제든 댓글 또는 메일로 알려주시기를 바랍니다. 01. 개요 2015년, 구글에서 발표한 Listen, Attend and Spell(LAS)는 전통적인 DNN-HMM 모델과 달리 음성 인식에 사용되는 모든 컴포넌트를 결...</p></div></div></a></div><div class="card"> <a href="/posts/Traditional-ASR/"><div class="card-body"> <em class="timeago small" data-ts="1658070000" > 2022-07-18 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>(Speech Recognition) 고전적 음성 인식 기술의 개요</h3><div class="text-muted small"><p> 이 포스트는 개인적으로 공부한 내용을 정리하고 필요한 분들에게 지식을 공유하기 위해 작성되었습니다. 지적하실 내용이 있다면, 언제든 댓글 또는 메일로 알려주시기를 바랍니다. 개요 딥러닝 이전의 시대에서 음성 인식은 Hidden Markov Model(HMM)과 Gaussian Mixture Model(GMM)의 혼합형 모델이 주를 이루었다. 그 후...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/Listen,-Attend-and-Spell/" class="btn btn-outline-primary" prompt="Older"><p>(Speech Recognition) Listen, Attend and Spell (LAS) 리뷰 및 설명</p></a> <a href="/posts/Vector-Quantization/" class="btn btn-outline-primary" prompt="Newer"><p>Vector Quantization과 Codebook 개념 정리</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center text-muted"><div class="footer-left"><p class="mb-0"> © 2023 <a href="https://github.com/zerojsh00">Sanghyeon</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/k8s/">K8S</a> <a class="post-tag" href="/tags/kubernetes/">Kubernetes</a> <a class="post-tag" href="/tags/paper-review/">Paper Review</a> <a class="post-tag" href="/tags/security/">Security</a> <a class="post-tag" href="/tags/network/">Network</a> <a class="post-tag" href="/tags/speech-ai/">Speech AI</a> <a class="post-tag" href="/tags/scheduling/">Scheduling</a> <a class="post-tag" href="/tags/nlp/">NLP</a> <a class="post-tag" href="/tags/storage/">Storage</a> <a class="post-tag" href="/tags/asr/">ASR</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/ko.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script>
