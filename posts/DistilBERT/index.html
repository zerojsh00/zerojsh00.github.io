<!DOCTYPE html><html lang="ko" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.3.2" /><meta property="og:title" content="(NLP) DistilBERT 리뷰 및 설명" /><meta name="author" content="simon sanghyeon" /><meta property="og:locale" content="ko" /><meta name="description" content="이 포스트는 개인적으로 공부한 내용을 정리하고 필요한 분들에게 지식을 공유하기 위해 작성되었습니다. 지적하실 내용이 있다면, 언제든 댓글 또는 메일로 알려주시기를 바랍니다." /><meta property="og:description" content="이 포스트는 개인적으로 공부한 내용을 정리하고 필요한 분들에게 지식을 공유하기 위해 작성되었습니다. 지적하실 내용이 있다면, 언제든 댓글 또는 메일로 알려주시기를 바랍니다." /><link rel="canonical" href="https://zerojsh00.github.io/posts/DistilBERT/" /><meta property="og:url" content="https://zerojsh00.github.io/posts/DistilBERT/" /><meta property="og:site_name" content="Simon’s Research Center" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2023-02-20T00:00:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="(NLP) DistilBERT 리뷰 및 설명" /><meta name="twitter:site" content="@" /><meta name="twitter:creator" content="@simon sanghyeon" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"simon sanghyeon"},"dateModified":"2023-02-20T00:00:00+09:00","datePublished":"2023-02-20T00:00:00+09:00","description":"이 포스트는 개인적으로 공부한 내용을 정리하고 필요한 분들에게 지식을 공유하기 위해 작성되었습니다. 지적하실 내용이 있다면, 언제든 댓글 또는 메일로 알려주시기를 바랍니다.","headline":"(NLP) DistilBERT 리뷰 및 설명","mainEntityOfPage":{"@type":"WebPage","@id":"https://zerojsh00.github.io/posts/DistilBERT/"},"url":"https://zerojsh00.github.io/posts/DistilBERT/"}</script><title>(NLP) DistilBERT 리뷰 및 설명 | Simon's Research Center</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Simon's Research Center"><meta name="application-name" content="Simon's Research Center"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get MODE_ATTR() { return "data-mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.notify(); } /* flipMode() */ </script> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "AMS" } }, tex2jax: { inlineMath: [ ['$', '$'] ], displayMath: [ ['$$', '$$'] ], processEscapes: true, } }); MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) { alert("Math Processing Error: "+message[1]); }); MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) { alert("Math Processing Error: "+message[1]); }); </script> <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script><meta name="google-site-verification" content="J_a4XJ2oefe52p6EYvjTFdh9LW5Yc5RyGg1HpQOjWaA" /><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" class="mx-auto"> <img src="/assets/img/avatar.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Simon's Research Center</a></div><div class="site-subtitle font-italic">성장하는 연구원의 공책</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <button class="mode-toggle btn" aria-label="Switch Mode"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/zerojsh00" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['zerojsh00','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>(NLP) DistilBERT 리뷰 및 설명</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>(NLP) DistilBERT 리뷰 및 설명</h1><div class="post-meta text-muted"><div> By <em> Sanghyeon </em></div><div class="d-flex"><div> <span> Posted <em class="timeago" data-ts="1676818800" data-toggle="tooltip" data-placement="bottom" data-tooltip-df="llll" > 2023-02-20 </em> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="2850 words"> <em>15 min</em> read</span></div></div></div><div class="post-content"><p>이 포스트는 개인적으로 공부한 내용을 정리하고 필요한 분들에게 지식을 공유하기 위해 작성되었습니다. 지적하실 내용이 있다면, 언제든 댓글 또는 메일로 알려주시기를 바랍니다.</p><hr /><h1 id="01-introduction">01. Introduction</h1><hr /><p>2018년, 자연어처리 영역의 위대한 한 획을 그은 BERT를 대표로하여, 자연어처리 영역은 대규모의 사전학습 언어모델(large-scale pre-trained langauge models)을 활용한 전이학습(transfer learning) 방식이 주를 이루고 있다. 당연하게도 언어모델이 커질수록 많은 파라미터들을 사용하게 되며, 그에 따라 훌륭한 성능을 달성할 수 있게 된다.</p><h2 id="01-1-거대-언어모델의-맹점"><span class="mr-2">01-1. 거대 언어모델의 맹점</span><a href="#01-1-거대-언어모델의-맹점" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 500 500'%3E%3C/svg%3E" data-src="/assets/img/2023-02-20-DistilBERT/fig01.png" alt="fig01" width="500" height="500" data-proofer-ignore> <em>[출처] 원 논문 : DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</em></p><p>우선, 주요 언어모델들의 파라미터와 이에 대한 사전학습 비용을 가볍게 살펴보자.</p><p>이제는 국민 언어모델이 된 BERT를 먼저 보자. <code class="language-plaintext highlighter-rouge">BERT-base는 약 110,000,000(1억 1천만)개의 파라미터</code>를, <code class="language-plaintext highlighter-rouge">BERT-large는 약 340,000,000(3억 4천만)개의 파라미터</code>를 가지고 있다. Google research에 따르면, BERT-large를 사전학습 하는 데 16개의 Cloud TPU로 4일이 꼬박 걸렸다고 한다. Cloud TPU v2로 가정했을 때, 16(TPU 수) * 4(학습 일) * 24(시간) * 4.5(시간 당 US$) = $6,912로 계산된다. <strong>즉, BERT-large의 사전학습 비용을 원화로 환산하면 대략 890만 원 정도라고 볼 수 있다.</strong></p><p>생성 모델인 GPT-2는 어떨까? <code class="language-plaintext highlighter-rouge">GPT-2는 약 1,500,000,000(15억)개의 파라미터</code>를 가지고 있다. <a href="https://www.theregister.com/2019/02/14/open_ai_language_bot/">The Register</a>에 따르면, GPT-2의 학습에는 시간 당 $256 만큼 소요되는 256개의 Google Cloud TPU v3 cores를 사용했다고 한다. <strong>$256를 어림 잡아 원화 33만 원으로 계산한다고 했을 때, 하루에 792만 원 가량으로 계산되는데, 놀라운 점은 GPT-2를 개발한 OpenAI는 사전학습에 소요된 시간을 공개하지조차도 않았다.</strong></p><p>이후 나온 <code class="language-plaintext highlighter-rouge">GPT-3와 현재 화두가 되고 있는 ChatGPT의 경우, 175,000,000,000(1750억)개의 파라미터</code>를 가지고 있다. 앞서 살펴본 모델과 단순 비교만 하더라도 실로 엄청난 규모이다. 참고로 GPT-3를 학습하는 데 약 1200만 달러, 즉, 150억 원 정도가 소요되었다.</p><h2 id="01-2-distilbert의-등장-배경"><span class="mr-2">01-2. DistilBERT의 등장 배경</span><a href="#01-2-distilbert의-등장-배경" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>위와 같이 DistilBERT의 등장 이후에도 여전히 언어모델은 일반 기업들은 손대지도 못 할 정도의 초거대 모델로 개발되고 있다. 지금도 여전한 문제이기도 한 언어모델의 거대화는 DistilBERT의 등장 배경이기도 하다. 이와 함께 논문에서 언급한 내용들은 아래와 같다.</p><ul><li>거대 언어모델을 학습하기 위한 연산에는 상당한 carbon footprint, 즉, 환경 비용(environmental cost)이 소요됨<li>거대 언어모델의 연산과 메모리 소요 등을 고려했을 때, 디바이스 상에서 실시간으로 사용되기 어려움</ul><p>따라서 본 논문은 <code class="language-plaintext highlighter-rouge">지식 증류(knowledge distillation)</code> 기법을 활용하여 거대 언어모델인 BERT(오늘날에는 BERT를 거대 모델로 치지 않겠지만…)를 경량화 하는 방식을 제시한다.</p><h1 id="02-지식-증류-knowledge-distillation">02. 지식 증류 (Knowledge Distillation)</h1><hr /><p><code class="language-plaintext highlighter-rouge">지식 증류(knowledge distillation)</code>란, 이미 사전학습 되어있는 대규모 모델인 <code class="language-plaintext highlighter-rouge">teacher</code>로부터 경량화된 압축 모델인 <code class="language-plaintext highlighter-rouge">student</code>로 AI의 지식을 나누어 주는 개념이다. <code class="language-plaintext highlighter-rouge">증류(distillation)</code>라는 단어가 액체 혼합물을 가열하여 액체 혼합물을 분리하는 과정을 의미한다는 점을 생각하면 그 뜻이 쉽게 와닿는다.</p><p>사실 지식 증류에도 다양한 방식이 있겠으나, 본 논문에서는 <a href="https://arxiv.org/abs/1503.02531">Geoffrey Hinton의 Softmax Temperature</a>를 이용하였다.</p><h2 id="02-1-softmax-temperature"><span class="mr-2">02-1. Softmax Temperature</span><a href="#02-1-softmax-temperature" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><h3 id="기본-softmax의-한계"><span class="mr-2">기본 Softmax의 한계</span><a href="#기본-softmax의-한계" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 500 500'%3E%3C/svg%3E" data-src="/assets/img/2023-02-20-DistilBERT/fig02.png" alt="fig02" width="500" height="500" data-proofer-ignore> <em>[출처] : <a href="https://hackernoon.com/softmax-temperature-and-prediction-diversity">Softmax Temperature and Prediction Diversity</a></em></p><p>일반적인 지도학습의 분류 모델은 모델의 예측 결과(logit)에 softmax를 취한 분포와 원 핫 인코딩 된 정답의 분포 간의 크로스 엔트로피 로스를 최소화 한다. 이에 따라 잘 학습된 모델은 정답 클래스에 대해서는 높은 확률로 분류해 낼 것이고, 오답 클래스에 대해서는 <code class="language-plaintext highlighter-rouge">0에 가까운 값(near-zero)</code>을 보일 것이다.</p><p>하지만, 같은 non-zero의 오답 클래스라 하더라도 어떤 클래스는 조금이나마 더 정답 클래스와 유사할 것이다. 극단적인 예를 들면, <code class="language-plaintext highlighter-rouge">바둑이</code>를 분류하는 태스크에서 <code class="language-plaintext highlighter-rouge">멍멍이</code>라는 오답이 <code class="language-plaintext highlighter-rouge">스파게티</code>라는 오답보다 더 강아지와 유사하지 않은가? 여기서 ‘멍멍이’와 같은 지식을 <code class="language-plaintext highlighter-rouge">암흑 지식(dark knowledge)</code>라고 한다. softmax로 잘 학습된 분류기는 바둑이에 대해서는 0.99에 가까운 확률로 정답으로 분류를 하고, 멍멍이에 대해서는 0.000…1에 가까운 확률로 오답으로 분류할 것이다. 무언가 완벽해보이지 않는다. <strong>즉, 단순한 softmax는 엔트로피가 낮다!</strong></p><h3 id="softmax에-랜덤성을-더한-temperature-scaling"><span class="mr-2">Softmax에 랜덤성을 더한 Temperature Scaling</span><a href="#softmax에-랜덤성을-더한-temperature-scaling" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 500 500'%3E%3C/svg%3E" data-src="/assets/img/2023-02-20-DistilBERT/fig03.png" alt="fig03" width="500" height="500" data-proofer-ignore> <em>[출처] : <a href="https://hackernoon.com/softmax-temperature-and-prediction-diversity">Softmax Temperature and Prediction Diversity</a></em></p><p>위와 같은 softmax 함수의 한계에 대한 해결 방법이 바로 <code class="language-plaintext highlighter-rouge">temperature scaling</code>으로, 모델의 예측 결과인 logit 벡터를 temperature 값 $T$만큼 나누어주는 softmax 방식이다.</p><p>이를 수식으로 표현하면 $\cfrac{\exp{(z_i / T)}}{\sum_j \exp{(z_j / T)}}$와 같다. 즉, temperature $T=1$ 일 경우 softmax와 같아지는 구조이며, temperature $T \to \infty$ 일 경우 uniform distribution을 가진다. 일반적으로 $T$는 1~20 정도의 값을 사용한다고 한다.</p><p>이처럼 학습 시 temperature scaling을 적용한 확률을 <code class="language-plaintext highlighter-rouge">soft target probability</code>라고 한다. 학습을 마친 후 추론 시에는 scaling을 하지 않고, $T=1$인 기본 softmax를 사용한다. 이러한 방식은 generalization에 효과적이어서 테스트 데이터셋에 대해 더욱 좋은 성능을 보인다고 알려져 있다.</p><h2 id="02-2-training-loss"><span class="mr-2">02-2. Training Loss</span><a href="#02-2-training-loss" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p><img data-src="/assets/img/2023-02-20-DistilBERT/fig04.png" alt="fig04" data-proofer-ignore> <em>[출처] : <a href="https://alexnim.com/coding-projects-knowledge-distillation.html">Knowledge Distillation of Language Models</a></em></p><p>DistilBERT를 학습하기 위해서는 <code class="language-plaintext highlighter-rouge">Teacher - Student Cross Entropy (Distillation Loss)</code>, <code class="language-plaintext highlighter-rouge">Student Masked Language Modeling Loss (MLM Loss)</code>, 그리고 <code class="language-plaintext highlighter-rouge">Teacher - Student Cosine Embedding Loss</code>을 활용하여 학습한다. 우선, downstream 태스크가 아니므로, <code class="language-plaintext highlighter-rouge">[MASK]</code>를 예측하는 학습을 수행하는 점을 염두에 두자.</p><h3 id="loss-1--teacher---student-cross-entropy-distillation-loss"><span class="mr-2">Loss 1 : Teacher - Student Cross Entropy (Distillation Loss)</span><a href="#loss-1--teacher---student-cross-entropy-distillation-loss" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>먼저, 사전학습 된 teacher의 logit에 대해 temperature scaling이 적용된 softmax를 취하여 <code class="language-plaintext highlighter-rouge">[MASK]</code>에 대한 <code class="language-plaintext highlighter-rouge">soft target label</code>로 삼는다. 이후 사전학습을 진행할 student의 logit에 대해서도 temperature scaling이 적용된 softmax를 취하여 나온 <code class="language-plaintext highlighter-rouge">[MASK]</code>에 대한 예측값을 구하고, 예측값과 soft target label 간의 크로스 엔트로피 로스를 구한다. 이를 <code class="language-plaintext highlighter-rouge">distillation loss</code>라 한다. 즉, student 모델이 teacher 모델이 가지고 있는 지식을 전수받는 과정이다.</p><p>수식으로 $L_{ce} = \sum_{i} t_{i} * \log(s_{i})$로 표현할 수 있으며, $t_i$는 teacher의 확률이며 $s_i$는 student의 확률이다.</p><h3 id="loss-2--student-masked-language-modeling-loss-mlm-loss"><span class="mr-2">Loss 2 : Student Masked Language Modeling Loss (MLM Loss)</span><a href="#loss-2--student-masked-language-modeling-loss-mlm-loss" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>사전학습을 진행할 student 모델에 대해 일반적인 BERT의 <code class="language-plaintext highlighter-rouge">Masked Language Modeling(MLM) loss</code> $L_{mlm}$을 적용한다. MLM loss에 대해서는 temperature scaling을 적용하지 않은 기본적인 softmax를 이용한, <code class="language-plaintext highlighter-rouge">hard target</code>, <code class="language-plaintext highlighter-rouge">hard prediction</code>을 활용한다. 이를 <code class="language-plaintext highlighter-rouge">student loss</code>라고도 부른다.</p><h3 id="loss-3--teacher---student-cosine-embedding-loss"><span class="mr-2">Loss 3 : Teacher - Student Cosine Embedding Loss</span><a href="#loss-3--teacher---student-cosine-embedding-loss" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>저자들은 student 모델이 더욱 teacher 모델과 닮아질 수 있도록 코사인 유사도를 활용하기까지 했다. 단순히 입력 벡터 $x$가 정답 벡터 $y$와 같게 학습하는 지도학습 방식을 넘어, teacher 모델의 hidden 벡터와 student 모델의 hidden 벡터가 일치(align)되도록 학습하는 것이다.</p><p>수식으로 $L_{cos} = 1- \cos (T(x), S(x))$로 표현할 수 있으며, $x$는 입력 벡터, $T$와 $S$는 각각 teacher 모델 및 student 모델을 의미한다.</p><p><em>(위 그림에 따르면, teacher 모델과 student 모델의 출력값인 $T(x)$와 $S(x)$로 cosine loss를 구하지 않고, 각 모델의 단어 임베딩을 직접 활용하여 cosine loss를 구한다. 두 방식 중 어떤 방식을 활용했는지에 대한 구체적인 설명이 논문에 명시되어 있지는 않은 듯 하다.)</em></p><h3 id="triple-loss--loss-1--loss-2--loss-3"><span class="mr-2">Triple Loss = Loss 1 + Loss 2 + Loss 3</span><a href="#triple-loss--loss-1--loss-2--loss-3" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>결과적으로 최종적인 triple loss는 $L = \alpha * L_{ce} + \beta * L_{MLM} + \gamma * L_{cos}$이며, 이때 $\alpha + \beta + \gamma = 1$이다.</p><h1 id="03-distilbert의-디테일">03. DistilBERT의 디테일</h1><hr /><h2 id="03-1-student-model의-구조"><span class="mr-2">03-1. Student Model의 구조</span><a href="#03-1-student-model의-구조" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><h3 id="next-sentence-predictionnsp-태스크를-수행하지-않음"><span class="mr-2">Next Sentence Prediction(NSP) 태스크를 수행하지 않음</span><a href="#next-sentence-predictionnsp-태스크를-수행하지-않음" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>DistilBERT의 student 모델은 기본적으로 BERT와 동일하나, 다음과 같은 차이가 있다.</p><ul><li><code class="language-plaintext highlighter-rouge">token-type embedding</code>의 제거<ul><li>token-type embedding이란, transformers 라이브러리의 <code class="language-plaintext highlighter-rouge">token_type_ids</code>에 대응되며, BERT에서 <code class="language-plaintext highlighter-rouge">[SEP]</code> 토큰으로 두 문장 간 관계를 학습할 때 각 문장을 구분함</ul><li><code class="language-plaintext highlighter-rouge">pooler</code>의 제거<ul><li>pooler란, BERT에서 얻은 contextual embedding 중 <code class="language-plaintext highlighter-rouge">[CLS]</code> 토큰의 임베딩에 해당됨</ul></ul><p>이러한 차이는 기존 BERT가 사전학습 할 때 수행하는 <code class="language-plaintext highlighter-rouge">Next Sentence Prediction(NSP)</code> 태스크를 수행하지 않음을 의미한다.</p><h3 id="teacher-bert를-활용한-초기화"><span class="mr-2">teacher BERT를 활용한 초기화</span><a href="#teacher-bert를-활용한-초기화" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 500 500'%3E%3C/svg%3E" data-src="/assets/img/2023-02-20-DistilBERT/fig05.png" alt="fig05" width="500" height="500" data-proofer-ignore> <em>[출처] : <a href="https://towardsdatascience.com/distillation-of-bert-like-models-the-theory-32e19a02641f">Distillation of BERT-Like Models: The Theory</a></em></p><p>또한 위 그림과 같이 teacher 모델의 레이어의 절반만을 복사하여 초기화함으로써 레이어의 수를 절반으로 감소시켰다.</p><h2 id="03-2-기타-학습-세부-사항"><span class="mr-2">03-2. 기타 학습 세부 사항</span><a href="#03-2-기타-학습-세부-사항" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>DistilBERT는 아주 큰 배치 사이즈(배치당 4K의 데이터)를 활용하여 학습하였다. NSP 태스크를 수행하지 않았으며, RoBERTa의 트릭인 <code class="language-plaintext highlighter-rouge">dynamic masking</code> 기법을 사용하였다. dynamic masking은 입력을 만들 때 마다 masking을 다시 하는 기법이다.</p><h3 id="데이터와-컴퓨트-파워"><span class="mr-2">데이터와 컴퓨트 파워</span><a href="#데이터와-컴퓨트-파워" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>BERT와 동일한 데이터인 English Wikipedia와 Toronto Book Corpus를 활용했다. <code class="language-plaintext highlighter-rouge">DistilBERT 학습은 16GB V100 GPU 8대로 90시간</code>이 소요되었다. 참고로 <code class="language-plaintext highlighter-rouge">RoBERTa의 경우 32GB V100 GPU 1024대로 하루</code>가 걸렸다는 점을 고려하면 대단한 수치다.</p><h1 id="04-결과-및-결론">04. 결과 및 결론</h1><hr /><p><img data-src="/assets/img/2023-02-20-DistilBERT/fig06.png" alt="fig06" data-proofer-ignore></p><p>distilBERT는 distillation 기법으로 빠르게 학습하였음에도 불구하고, 기존 BERT에 비해 40%나 가벼워졌고 60%나 빨라졌으며 97% 성능을 유지했다!</p><h1 id="05-참고-문헌">05. 참고 문헌</h1><hr /><p>[1] <code class="language-plaintext highlighter-rouge">원 논문</code> : <a href="https://arxiv.org/pdf/1910.01108.pdf">DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</a><br /> [2] <code class="language-plaintext highlighter-rouge">블로그</code> : <a href="https://syncedreview.com/2019/06/27/the-staggering-cost-of-training-sota-ai-models/">The Staggering Cost of Training SOTA AI Models</a><br /> [3] <code class="language-plaintext highlighter-rouge">Harshit Sharma의 포스트</code> : <a href="https://hackernoon.com/softmax-temperature-and-prediction-diversity">Softmax Temperature and Prediction Diversity</a><br /> [4] <code class="language-plaintext highlighter-rouge">kaggle</code> : <a href="https://www.kaggle.com/code/venkatkumar001/nlpstarter4-distilbert-bert-base-cased">📢NLPStarter4📄🤗DistilBert,Bert(Base-Cased)🤗:)</a><br /> [5] <code class="language-plaintext highlighter-rouge">Alex Nim의 포스트</code> : <a href="https://alexnim.com/coding-projects-knowledge-distillation.html">Knowledge Distillation of Language Models</a><br /> [6] <code class="language-plaintext highlighter-rouge">Remi Ouazan Reboul의 포스트</code> : <a href="https://towardsdatascience.com/distillation-of-bert-like-models-the-theory-32e19a02641f">Distillation of BERT-Like Models: The Theory</a></p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/natural-language-processing/'>Natural Language Processing</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/language-model/" class="post-tag no-text-decoration" >Language Model</a> <a href="/tags/nlp/" class="post-tag no-text-decoration" >NLP</a> <a href="/tags/distillation/" class="post-tag no-text-decoration" >Distillation</a> <a href="/tags/paper-review/" class="post-tag no-text-decoration" >Paper Review</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=%28NLP%29+DistilBERT+%EB%A6%AC%EB%B7%B0+%EB%B0%8F+%EC%84%A4%EB%AA%85+-+Simon%27s+Research+Center&url=https%3A%2F%2Fzerojsh00.github.io%2Fposts%2FDistilBERT%2F" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=%28NLP%29+DistilBERT+%EB%A6%AC%EB%B7%B0+%EB%B0%8F+%EC%84%A4%EB%AA%85+-+Simon%27s+Research+Center&u=https%3A%2F%2Fzerojsh00.github.io%2Fposts%2FDistilBERT%2F" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fzerojsh00.github.io%2Fposts%2FDistilBERT%2F&text=%28NLP%29+DistilBERT+%EB%A6%AC%EB%B7%B0+%EB%B0%8F+%EC%84%A4%EB%AA%85+-+Simon%27s+Research+Center" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div></div><script src="https://utteranc.es/client.js" repo="zerojsh00/zerojsh00.github.io" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/Network-Policy/">(K8S) 네트워크 정책(Network Policy) 기초 개념</a><li><a href="/posts/CNI-Weave/">(K8S) CNI Weave의 기초 개념</a><li><a href="/posts/Container-Networking-Interface/">(K8S) 네트워크 기초 정리 - CNI</a><li><a href="/posts/Storage-in-Docker/">(K8S) 도커의 스토리지(Storage in Docker)</a><li><a href="/posts/Security-Context/">(K8S) Security Context</a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/k8s/">K8S</a> <a class="post-tag" href="/tags/kubernetes/">Kubernetes</a> <a class="post-tag" href="/tags/paper-review/">Paper Review</a> <a class="post-tag" href="/tags/security/">Security</a> <a class="post-tag" href="/tags/network/">Network</a> <a class="post-tag" href="/tags/speech-ai/">Speech AI</a> <a class="post-tag" href="/tags/scheduling/">Scheduling</a> <a class="post-tag" href="/tags/nlp/">NLP</a> <a class="post-tag" href="/tags/storage/">Storage</a> <a class="post-tag" href="/tags/asr/">ASR</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="tail-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/ELECTRA/"><div class="card-body"> <em class="timeago small" data-ts="1662994800" > 2022-09-13 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>(NLP) ELECTRA 리뷰 및 설명</h3><div class="text-muted small"><p> 이 포스트는 개인적으로 공부한 내용을 정리하고 필요한 분들에게 지식을 공유하기 위해 작성되었습니다. 지적하실 내용이 있다면, 언제든 댓글 또는 메일로 알려주시기를 바랍니다. 01. 개요 ICLR 2020에서 Google Brain 팀은 새로운 사전 학습 방법론으로 ELECTRA를 제안했다. ELECTRA는 “Efficiently Learning...</p></div></div></a></div><div class="card"> <a href="/posts/BERTopic/"><div class="card-body"> <em class="timeago small" data-ts="1663686000" > 2022-09-21 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>(NLP) BERTopic 개념 정리</h3><div class="text-muted small"><p> 이 포스트는 개인적으로 공부한 내용을 정리하고 필요한 분들에게 지식을 공유하기 위해 작성되었습니다. 지적하실 내용이 있다면, 언제든 댓글 또는 메일로 알려주시기를 바랍니다. 01. Introduction 전통적인 토픽 모델링의 방법으로는 Latent Dirichlet Allocation(LDA)와 Non-Negative Matrix Factor...</p></div></div></a></div><div class="card"> <a href="/posts/RoBERTa/"><div class="card-body"> <em class="timeago small" data-ts="1662476400" > 2022-09-07 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>(NLP) RoBERTa 리뷰 및 설명</h3><div class="text-muted small"><p> 이 포스트는 개인적으로 공부한 내용을 정리하고 필요한 분들에게 지식을 공유하기 위해 작성되었습니다. 지적하실 내용이 있다면, 언제든 댓글 또는 메일로 알려주시기를 바랍니다. 01. 요약 RoBERTa는 Robustly Optimized BERT approach의 약자이다. ELMo, GPT, BERT, XLM, XLNet 등 self-traini...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/StarSpace/" class="btn btn-outline-primary" prompt="Older"><p>(NLP) StarSpace</p></a><div class="btn btn-outline-primary disabled" prompt="Newer"><p>-</p></div></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center text-muted"><div class="footer-left"><p class="mb-0"> © 2023 <a href="https://github.com/zerojsh00">Sanghyeon</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/k8s/">K8S</a> <a class="post-tag" href="/tags/kubernetes/">Kubernetes</a> <a class="post-tag" href="/tags/paper-review/">Paper Review</a> <a class="post-tag" href="/tags/security/">Security</a> <a class="post-tag" href="/tags/network/">Network</a> <a class="post-tag" href="/tags/speech-ai/">Speech AI</a> <a class="post-tag" href="/tags/scheduling/">Scheduling</a> <a class="post-tag" href="/tags/nlp/">NLP</a> <a class="post-tag" href="/tags/storage/">Storage</a> <a class="post-tag" href="/tags/asr/">ASR</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/ko.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script>
