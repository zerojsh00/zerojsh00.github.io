<!DOCTYPE html><html lang="ko" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.3.2" /><meta property="og:title" content="(Speech Recognition) Listen, Attend and Spell (LAS) 리뷰 및 설명" /><meta name="author" content="simon sanghyeon" /><meta property="og:locale" content="ko" /><meta name="description" content="이 포스트는 개인적으로 공부한 내용을 정리하고 필요한 분들에게 지식을 공유하기 위해 작성되었습니다. 지적하실 내용이 있다면, 언제든 댓글 또는 메일로 알려주시기를 바랍니다." /><meta property="og:description" content="이 포스트는 개인적으로 공부한 내용을 정리하고 필요한 분들에게 지식을 공유하기 위해 작성되었습니다. 지적하실 내용이 있다면, 언제든 댓글 또는 메일로 알려주시기를 바랍니다." /><link rel="canonical" href="https://zerojsh00.github.io/posts/Listen,-Attend-and-Spell/" /><meta property="og:url" content="https://zerojsh00.github.io/posts/Listen,-Attend-and-Spell/" /><meta property="og:site_name" content="Simon’s Research Center" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-07-26T00:00:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="(Speech Recognition) Listen, Attend and Spell (LAS) 리뷰 및 설명" /><meta name="twitter:site" content="@" /><meta name="twitter:creator" content="@simon sanghyeon" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"simon sanghyeon"},"dateModified":"2022-07-31T21:50:46+09:00","datePublished":"2022-07-26T00:00:00+09:00","description":"이 포스트는 개인적으로 공부한 내용을 정리하고 필요한 분들에게 지식을 공유하기 위해 작성되었습니다. 지적하실 내용이 있다면, 언제든 댓글 또는 메일로 알려주시기를 바랍니다.","headline":"(Speech Recognition) Listen, Attend and Spell (LAS) 리뷰 및 설명","mainEntityOfPage":{"@type":"WebPage","@id":"https://zerojsh00.github.io/posts/Listen,-Attend-and-Spell/"},"url":"https://zerojsh00.github.io/posts/Listen,-Attend-and-Spell/"}</script><title>(Speech Recognition) Listen, Attend and Spell (LAS) 리뷰 및 설명 | Simon's Research Center</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Simon's Research Center"><meta name="application-name" content="Simon's Research Center"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get MODE_ATTR() { return "data-mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.notify(); } /* flipMode() */ </script> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "AMS" } }, tex2jax: { inlineMath: [ ['$', '$'] ], displayMath: [ ['$$', '$$'] ], processEscapes: true, } }); MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) { alert("Math Processing Error: "+message[1]); }); MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) { alert("Math Processing Error: "+message[1]); }); </script> <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script><meta name="google-site-verification" content="J_a4XJ2oefe52p6EYvjTFdh9LW5Yc5RyGg1HpQOjWaA" /><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" class="mx-auto"> <img src="/assets/img/avatar.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Simon's Research Center</a></div><div class="site-subtitle font-italic">성장하는 연구원의 공책</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <button class="mode-toggle btn" aria-label="Switch Mode"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/zerojsh00" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['zerojsh00','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>(Speech Recognition) Listen, Attend and Spell (LAS) 리뷰 및 설명</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>(Speech Recognition) Listen, Attend and Spell (LAS) 리뷰 및 설명</h1><div class="post-meta text-muted"><div> By <em> Sanghyeon </em></div><div class="d-flex"><div> <span> Posted <em class="timeago" data-ts="1658761200" data-toggle="tooltip" data-placement="bottom" data-tooltip-df="llll" > 2022-07-26 </em> </span> <span> Updated <em class="timeago" data-ts="1659271846" data-toggle="tooltip" data-placement="bottom" data-tooltip-df="llll" > 2022-07-31 </em> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="3200 words"> <em>17 min</em> read</span></div></div></div><div class="post-content"><p>이 포스트는 개인적으로 공부한 내용을 정리하고 필요한 분들에게 지식을 공유하기 위해 작성되었습니다. 지적하실 내용이 있다면, 언제든 댓글 또는 메일로 알려주시기를 바랍니다.</p><hr /><h1 id="01-개요">01. 개요</h1><p>2015년, 구글에서 발표한 <code class="language-plaintext highlighter-rouge">Listen, Attend and Spell(LAS)</code>는 전통적인 DNN-HMM 모델과 달리 음성 인식에 사용되는 모든 컴포넌트를 결합하여 한번에 학습할 수 있는 <code class="language-plaintext highlighter-rouge">end-to-end</code> 방식을 제시한다. 또한 LAS는 당시 유명했던 <code class="language-plaintext highlighter-rouge">Connectionist Temporal Classification(CTC)</code> 방법론과 마찬가지로 음성 신호의 sequence $\mathbf{x}$가 주어졌을 때, 이에 맞는 토큰(참고로 LAS는 글자의 sequence)의 sequence $\mathbf{y}$ 간의 명시적인 정렬 관계(alignment) 없이도 학습할 수 있다. CTC와의 차이점으로는, CTC는 RNN(등의 sequence를 모델링하는 신경망)의 출력으로 나오는 음소의 sequence가 <code class="language-plaintext highlighter-rouge">조건부 독립(conditional independence)</code>임을 가정하는 반면, LAS는 이러한 가정 없앰으로써 더욱 효과적인 decoding이 가능하게 되었다. 예를 들어, CTC는 “triple a”라는 발화에 대해 “aaa”로 교정할 수 없으나, LAS는 조건부 독립 가정을 제거하여 더욱 풍부한 표현(i.e., <code class="language-plaintext highlighter-rouge">multiple spelling variants</code>)으로 출력할 수 있게 되었다. 그럼, 마치 NLP의 Seq2Seq 모델을 빼닮은 LAS를 살펴보자.</p><hr /><h1 id="02-훑어보는-las">02. 훑어보는 LAS</h1><h2 id="주요-특징-요약"><span class="mr-2">주요 특징 요약</span><a href="#주요-특징-요약" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><ul><li><code class="language-plaintext highlighter-rouge">listener</code> : pyramidal RNN으로 음성 신호 정보를 벡터로 encoding하는 encoder를 구현함<ul><li>(참고) pyramidal RNN 형태가 아니면, 모델 학습이 지나치게 느리게 수렴됨</ul><li><code class="language-plaintext highlighter-rouge">attention mechanism</code> : decoding 시 음성 신호의 sequence에서 더욱 집중해야 할 곳을 포착할 수 있음<ul><li>(참고) attention mechanism이 없이는 심각하게 학습 데이터에 과적합됨</ul><li><code class="language-plaintext highlighter-rouge">speller</code> : character sequence를 출력하여 <code class="language-plaintext highlighter-rouge">out-of-vocabulary(OOV)</code> 문제를 해결한 decoder를 구현함<li><code class="language-plaintext highlighter-rouge">조건부 독립을 제거</code> : 동일한 음성 sequnece가 주어져도 beam search에 따라 출력 sequence를 다양하게 표현할 수 있음</ul><hr /><h1 id="03-조금-더-들여다보는-las">03. 조금 더 들여다보는 LAS</h1><p>LAS가 <code class="language-plaintext highlighter-rouge">로그-멜 스펙트럼(log-mel filter banks)</code>을 입력으로 받는다고 할때, 입력되는 음성 신호의 sequence를 $\mathbf{x}=(x_1, x_2, …, x_T)$라고 표기한다.</p><p>또한 이에 대응되는 글자들의 sequence를 $\mathbf{y} = (&lt;sos&gt;, y_1, …, y_S, &lt;eos&gt;)$라고 한다. 이때 $y_{i} \in \{ a, b, c, … , z, 0, … , 9, &lt;space&gt;, &lt;comma&gt;, &lt;period&gt;, &lt;apostrophe&gt;, &lt;unk&gt; \}$이다.</p><p>LAS는 음성 신호 $\mathbf{x}$와 처음부터 이전 시점까지의 글자 출력 결과 $y_{&lt;i}$가 주어졌을 때 글자의 출력 $y_i$가 나올 조건부 확률들을 chain rule을 이용하여 곱함으로써 $p(\mathbf{y} | \mathbf{x}) = \prod_i P(y_i | \mathbf{x} , y_{&lt;i})$와 같이 입력 sequence와 출력 sequence의 관계를 모델링한다.</p><h2 id="전반적인-아키텍처"><span class="mr-2">전반적인 아키텍처</span><a href="#전반적인-아키텍처" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>LAS는 음향 모델의 encoder 역할을 수행하는 <code class="language-plaintext highlighter-rouge">listener</code>와 attention 기반으로 글자들을 출력하는 decoder인 <code class="language-plaintext highlighter-rouge">speller</code>로 구성된다. 일종의 Sequence2Sequence(Seq2Seq) 모델의 변형이라고 생각해도 좋을 것 같다. 이들은 $Listen$ 함수와 $AttendAndSpell$ 함수로 구현된다.</p><p>encoder인 listener는 원시 음성 신호 $\mathbf{x}$를 high level representation $\mathbf{h}=(h_1, …, h_U)$로 변환하는 역할을 한다. 이때 $U \le T$이며, $T$는 $\mathbf{x}$의 최대 길이를 의미한다. 추후 다룰 pyramidal 구조로 encoding을 하기때문에 이 조건은 당연하게 만족된다. 이는 다음과 같이 $Listen$ 함수로 표현한다.</p><p>$\mathbf{h} = Listen(\mathbf{x})$</p><p>한편 attention mechanism 기반의 decoder인 speller는 아래와 같이 표현한다.</p><p>$P(\mathbf{y} | \mathbf{x}) = AttendAndSpell( \mathbf{h}, \mathbf{y})$</p><p>이는 encoder를 통해서 encoding된 high level representation $\mathbf{h}=(h_1, …, h_U)$와 글자의 sequence $\mathbf{y}$(엄밀하게 말하면 이전 step까지의 글자 sequence $y_{&lt;i}$)를 사용하여 현 시점에 나올 글자의 확률 $y_i$를 예측하는 역할을 한다. 즉, 최종적으로 $P(\mathbf{y} | \mathbf{x})$를 예측하는 모듈이라고 보면 되겠다.</p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 500 500'%3E%3C/svg%3E" data-src="/assets/img/2022-07-26-Listen, Attend and Spell/fig01.png" alt="fig01" width="500" height="500" data-proofer-ignore></p><p>LAS의 전체적인 아키텍처는 위 그림과 같다. listener는 위와 같이 피라미드 모양으로 쌓은 양방향 LSTM을 사용하며, 음성 신호 sequence $\mathbf{x}$를 high level feature인 $\mathbf{h}$로 encoding 한다. speller는 attention을 활용하는 decoder이며, 매 time step마다 $\mathbf{h}$로부터 글자의 sequence $\mathbf{z}$를 생성해낸다.</p><h2 id="listen-함수"><span class="mr-2">Listen 함수</span><a href="#listen-함수" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>$\mathbf{h} = Listen(\mathbf{x})$ 함수를 살펴보도록 하자. 그림에서도 알 수 있듯이, listener에서는 음성 신호 sequence $\mathbf{x}$의 길이보다 더 적은 수의 $h_U$ representation들을 만들기 위해서 <code class="language-plaintext highlighter-rouge">pyramidal BLSTM(pBLSTM)</code>을 사용하였다. 왜냐하면, 음성 신호의 입력 값은 수백에서 수천 프레임으로 이루어질수 있기 때문이다. 이렇게 너무 많은 프레임으로 학습을 한다면, 아주 오랜 시간동안(심지어 약 한 달) 학습을 진행해도 모델의 수렴 속도가 너무 느려서 제대로 학습되지 않는 단점이 있다. 이는 $AttendAndSpell$ 함수 파트에서 attention 연산을 할 때, 매우 많은 입력 sequence 내에서 모델이 집중할 중요한 정보들을 파악하는데 많은 소요 시간이 걸리기 때문인 것으로 추정된다. 따라서 pBLSTM으로 $h_U$의 개수를 조절하여 computational complexity를 낮춤으로써 이러한 문제를 해결하고자 한 것이다.</p><p>pBLSTM을 수식으로 표현하면 $h_i^j =$ pBLSTM$(h_{i-1}^j , \left[ h_{2i}^{j-1} , h_{2i+1}^{j-1} \right])$으로 쓸 수 있는데, 이전 레이어의 hidden state를 concatenate($\left[ h_{2i}^{j-1} , h_{2i+1}^{j-1} \right]$ 부분)하여 BLSTM의 입력으로 사용함으로써 구현하는 것을 알 수 있다. 참고로 일반적인 BLSTM은 단순히 $h_i^j =$ BLSTM$(h_{i-1}^j , h_{i}^{j-1})$이다.</p><h2 id="attend-and-spell-함수"><span class="mr-2">Attend and Spell 함수</span><a href="#attend-and-spell-함수" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>decoder의 $AttendAndSpell( \mathbf{h}, \mathbf{y})$ 함수를 살펴보도록 하자. decoder는 매 time step마다 지금까지 보았던 글자들의 sequence $y_{&lt;i}$ 정보를 토대로 다음 글자가 어떤 글자가 될 것인지 $y_i$에 대한 확률 분포를 생성한다.</p><p>이때 $y_i$에 대한 확률 분포는 (1) <code class="language-plaintext highlighter-rouge">decoder의 state</code> $s_i$와 (2) <code class="language-plaintext highlighter-rouge">context vector</code> $c_i$를 이용하여 만들어지며, 아래와 같이 표현할 수 있다.</p><p>$P(y_i | \mathbf{x}, y_{&lt;i} = CharacterDistribution(s_i, c_i))$</p><p>이를 구성하고 있는 요소들을 하나씩 살펴보겠다.</p><h3 id="decoder-state"><span class="mr-2">decoder state</span><a href="#decoder-state" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>decoder state $s_i$는 (1) 이전 시점의 state $s_{i-1}$, (2) decoder에 의해 이전 시점에 생성되었던 글자 $y_{i-1}$, 그리고 (3) 이전 시점에 $\mathbf{h}$와 $s_{i-1}$ 을 활용하여 만들어졌으며 $h_U$ 벡터 중 어디에 더욱 집중할지가 반영된 이전 시점의 context vector $c_{i-1}$를 입력으로 받아 만들어지며, 아래와 같이 RNN 함수로 표현할 수 있다.</p><p>$s_i =$ RNN$(s_{i-1}, y_{i-1}, c_{i-1})$</p><p>이러한 decoder의 state $s_i$에는 $s_{i-1}$과 $y_{i-1}$을 사용하므로 <code class="language-plaintext highlighter-rouge">지금까지 나왔던 글자들의 sequence 정보</code>가 함축되어 있다고 해석할 수 있다. 또한 이전 시점 context vector $c_{i-1}$을 사용하므로 <code class="language-plaintext highlighter-rouge">음성 신호 sequence에 어떤 부분이 집중적으로 고려되고 있는지에 대한 정보</code>가 포함되어 있다고 볼 수 있다.</p><h3 id="context-vector"><span class="mr-2">context vector</span><a href="#context-vector" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>context vector $c_i$는 매 time step $i$마다 attention mechanism인 $AttentionContext$ 함수에 의해 생성되는데, 이는 다음 글자를 생성할 때 ‘음성 신호 부분 중 어느 부분이 특히 중요한지’ 맥락 정보가 반영된 벡터이다.</p><p>$c_i = AttentionContext(s_i, \mathbf{h})$</p><p>이러한 $c_i$를 출력해내는 $AttentionContext$ 함수에서는 어떤 일이 벌어지는지 살펴보도록 하겠다. decoder인 speller는 매 time step $i$마다 decoder state인 $s_i$와 음성 신호 sequence가 encoding된 $h_u \in \mathbf{h}$ 간의 scalar energy $e_{i, u}$를 계산한다.</p><p>$e_{i,u} = &lt;\phi(s_i) , \psi(h_u)&gt;$</p><p>$\phi$와 $\psi$는 MLP 레이어이며, $s_i$ 벡터와 $h_u$ 벡터 각각에 대해 MLP 레이어를 통과시킨 후 내적하여 scalar energe $e_{i, u}$를 구할 수 있다. 두 벡터 간 내적은 두 벡터가 유사할수록 큰 값이 되므로, 현 시점 decoder state와 가장 유사한 음성 신호 구간 $u$가 어디인지를 확인하고자 하는 것이다. 이 연산은 현 시점 $i$에서 모든 $u$개의 벡터 $h_u$에 대해 각각 계산된다. 예를 들어, 현재가 $i$=3 시점이라고 하면, $e_{i=3, 1}, e_{i=3, 2} … , e_{i=3, u}$ 와 같이 여러 scalar energy가 나온다.</p><p>이후, 지금까지 구한 scalar energy 들에 대해서 softmax 함수를 적용한다.</p><p>$\alpha_{i,u} = \cfrac{ \exp(e_{i,u}) }{ \sum_u \exp(e_{i,u}) }$</p><p>앞선 예시를 계속 이어서 하자면, $e_{i=3, 1}, e_{i=3, 2} … , e_{i=3, u}$ 와 같은 각각의 scalar energy들에 대해 softmax가 적용되는 것으로, 각각의 $e_{i, u}$ 값들은 energy의 크기대로 확률값처럼 변환된 $\alpha_{i,u}$로 변환된다. 따라서 당연하게도 $\sum_u \alpha_{i,u} = 1$이다.</p><p>$s_i$와 $h_i$의 관계에 대한 energy가 확률값처럼 변환된 $\alpha_{i,u}$는 time step $i$ 당시에 $u$번째 음성 신호 구간의 representation인 $h_u$가 얼마나 더 집중되어야 하는지에 대한 가중치다. 이제 이 가중치를 이용하여 $h_u$를 가중합 하면, <code class="language-plaintext highlighter-rouge">time step i 당시에 u번째 음성 신호 구간 중 어디에 더 집중해야 하는지</code>에 대한 정보가 담긴 맥락정보 $c_i$가 나온다.</p><p>$c_i = \sum_u \alpha_{i, u}h_u$</p><p>복잡한 과정 끝에 구한 $c_i$는 다음 시점에 decoder state를 구하는 과정에서 RNN의 입력값으로 사용된다. 이 복잡한 과정은 아래의 도식을 통해 정리할 수 있다.</p><p><img data-src="/assets/img/2022-07-26-Listen, Attend and Spell/fig02.png" alt="fig02" data-proofer-ignore></p><h2 id="학습-테크닉"><span class="mr-2">학습 테크닉</span><a href="#학습-테크닉" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>$Listen$으로 구현되는 encoder인 listener 파트와 $AttendAndSpell$로 구현되는 decoder인 speller 파트는 동시에 결합되어(jointly) end-to-end로 학습된다. sequence to sequence 방식은 input 신호 $\mathbf{x}$와 이전 time step까지의 글자 sequence가 주어졌을 때 현재 글자를 예측하는 방식이므로 아래와 같은 로그 확률(log probability)를 극대화 하는 것을 목표로 한다.</p><p>$\max_\theta \sum_{i} \log P(y_{i} | \mathbf{x}, y_{&lt;i}^{*} ; \theta )$</p><p>$y_{&lt;i}^{*}$는 진짜 정답인 ground truth를 의미하는데, 학습을 마치고 실제 추론할 때는 현재 time step $i$ 이전까지의 글자 sequence 값들로 ground truth 값을 사용할 수는 없다. 따라서 추론 환경과 학습 환경 간의 불일치가 일어나게되어 강건하지 못한 성능의 원인이 된다.</p><p>LAS의 저자들은 이러한 문제를 해결하기 위해서 학습할 때 다음 글자를 예측하기 위해 이전 글자들을 항상 ground truth의 글자 sequence $y_{&lt;i}^{*}$로 사용하는 것이 아니라, sampling 기법을 통해 추출한 $\tilde{y}_{&lt;i}$를 활용하는 방식을 제시했다.</p><p>$\tilde{y}_i \sim \text{CharacterDistribution}(s_i, c_i)$</p><p>$\max_{\theta} \sum_{i} \log P(y_i | \mathbf{x}, \tilde{y}_{&lt;i} ; \theta)$</p><p>구체적으로 10%를 <code class="language-plaintext highlighter-rouge">sampling rate</code>로 설정함에 따라, 90% 확률로는 ground truth에서 실제 정답을 활용하겠으나, 10%의 확률로는 $\tilde{y}_{i-1}$ 값을 글자의 분포에서 sampling을 하여 사용한다.</p><h2 id="decoding-and-rescoring"><span class="mr-2">decoding and rescoring</span><a href="#decoding-and-rescoring" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>학습이 완료된 이후의 추론은 음성 신호의 sequence $\mathbf{x}$가 주어졌을 때 이에 대응되는 가장 적합한 글자의 sequence $\hat{\mathbf{y}}$을 찾아야 한다.</p><p>$\hat{\mathbf{y}} = \arg \max_{y} \log P(\mathbf{y} | \mathbf{x})$</p><p>LAS의 저자들은 추론 시 decoding 전략으로 <code class="language-plaintext highlighter-rouge">left-to-right beam search</code> 알고리즘을 적용하였다. 여기에 더해 방대한 텍스트 데이터로 학습한 언어모델을 함께 활용하였다. 이때 LAS 모델이 예측하는 단어가 짧을 때 생기는 bias 문제를 해결하기 위해서 글자의 수 $|\mathbf{y}|_c$ 만큼 normalize를 적용했으며, 언어모델의 출력값을 beam score에 더해서 rescoring 하였다.</p><p>$s(\mathbf{y} | \mathbf{x}) = \cfrac{ \log P(\mathbf{y} | \mathbf{x})}{|\mathbf{y}|_c} + \lambda \log P_{LM}(\mathbf{y})$</p><hr /><h1 id="04-정리하며">04. 정리하며</h1><p>LAS는 end-to-end 모델이면서도 CTC에서의 조건부 독립 가정 없이 음성 신호 sequence $\mathbf{x}$와 글자 sequence $\mathbf{y}$의 관계를 학습하는 데 성공했다. attention mechanism으로 CTC에서 복잡하게 구했던 $\mathbf{x}$와 $\mathbf{y}$ 간의 정렬 관계를 포착할 수 있다는 점이 대단히 흥미롭다.</p><hr /><h1 id="05-참고-문헌">05. 참고 문헌</h1><p><a href="https://arxiv.org/pdf/1508.01211.pdf">Chan, William, et al. “Listen, attend and spell.” <em>arXiv preprint arXiv:1508.01211</em> (2015).APA</a></p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/speech-recognition/'>Speech Recognition</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/speech-ai/" class="post-tag no-text-decoration" >Speech AI</a> <a href="/tags/asr/" class="post-tag no-text-decoration" >ASR</a> <a href="/tags/end-to-end/" class="post-tag no-text-decoration" >End to End</a> <a href="/tags/paper-review/" class="post-tag no-text-decoration" >Paper Review</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=%28Speech+Recognition%29+Listen%2C+Attend+and+Spell+%28LAS%29+%EB%A6%AC%EB%B7%B0+%EB%B0%8F+%EC%84%A4%EB%AA%85+-+Simon%27s+Research+Center&url=https%3A%2F%2Fzerojsh00.github.io%2Fposts%2FListen%2C-Attend-and-Spell%2F" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=%28Speech+Recognition%29+Listen%2C+Attend+and+Spell+%28LAS%29+%EB%A6%AC%EB%B7%B0+%EB%B0%8F+%EC%84%A4%EB%AA%85+-+Simon%27s+Research+Center&u=https%3A%2F%2Fzerojsh00.github.io%2Fposts%2FListen%2C-Attend-and-Spell%2F" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fzerojsh00.github.io%2Fposts%2FListen%2C-Attend-and-Spell%2F&text=%28Speech+Recognition%29+Listen%2C+Attend+and+Spell+%28LAS%29+%EB%A6%AC%EB%B7%B0+%EB%B0%8F+%EC%84%A4%EB%AA%85+-+Simon%27s+Research+Center" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div></div><script src="https://utteranc.es/client.js" repo="zerojsh00/zerojsh00.github.io" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/Network-Policy/">(K8S) 네트워크 정책(Network Policy) 기초 개념</a><li><a href="/posts/CNI-Weave/">(K8S) CNI Weave의 기초 개념</a><li><a href="/posts/Container-Networking-Interface/">(K8S) 네트워크 기초 정리 - CNI</a><li><a href="/posts/Storage-in-Docker/">(K8S) 도커의 스토리지(Storage in Docker)</a><li><a href="/posts/Security-Context/">(K8S) Security Context</a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/k8s/">K8S</a> <a class="post-tag" href="/tags/kubernetes/">Kubernetes</a> <a class="post-tag" href="/tags/paper-review/">Paper Review</a> <a class="post-tag" href="/tags/security/">Security</a> <a class="post-tag" href="/tags/network/">Network</a> <a class="post-tag" href="/tags/speech-ai/">Speech AI</a> <a class="post-tag" href="/tags/scheduling/">Scheduling</a> <a class="post-tag" href="/tags/nlp/">NLP</a> <a class="post-tag" href="/tags/storage/">Storage</a> <a class="post-tag" href="/tags/asr/">ASR</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="tail-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/Connectionist-Temporal-Classification/"><div class="card-body"> <em class="timeago small" data-ts="1658674800" > 2022-07-25 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>(Speech Recognition) Connectionist Temporal Classification 리뷰 및 설명</h3><div class="text-muted small"><p> 이 포스트는 개인적으로 공부한 내용을 정리하고 필요한 분들에게 지식을 공유하기 위해 작성되었습니다. 지적하실 내용이 있다면, 언제든 댓글 또는 메일로 알려주시기를 바랍니다. 상당 부분 ratsgo’s speech book 내용을 참고하였습니다. ratsgo 님께 감사드립니다. 01. 개요 Recurrent Neural Network(RN...</p></div></div></a></div><div class="card"> <a href="/posts/Conformer/"><div class="card-body"> <em class="timeago small" data-ts="1658934000" > 2022-07-28 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>(Speech Recognition) Conformer 리뷰 및 설명</h3><div class="text-muted small"><p> 이 포스트는 개인적으로 공부한 내용을 정리하고 필요한 분들에게 지식을 공유하기 위해 작성되었습니다. 지적하실 내용이 있다면, 언제든 댓글 또는 메일로 알려주시기를 바랍니다. 💡 2020년, 구글에서 발표한 “Conformer : Convolution-augmented Transformer for Speech Recognition” 논문을 설명한 ...</p></div></div></a></div><div class="card"> <a href="/posts/Traditional-ASR/"><div class="card-body"> <em class="timeago small" data-ts="1658070000" > 2022-07-18 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>(Speech Recognition) 고전적 음성 인식 기술의 개요</h3><div class="text-muted small"><p> 이 포스트는 개인적으로 공부한 내용을 정리하고 필요한 분들에게 지식을 공유하기 위해 작성되었습니다. 지적하실 내용이 있다면, 언제든 댓글 또는 메일로 알려주시기를 바랍니다. 개요 딥러닝 이전의 시대에서 음성 인식은 Hidden Markov Model(HMM)과 Gaussian Mixture Model(GMM)의 혼합형 모델이 주를 이루었다. 그 후...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/Connectionist-Temporal-Classification/" class="btn btn-outline-primary" prompt="Older"><p>(Speech Recognition) Connectionist Temporal Classification 리뷰 및 설명</p></a> <a href="/posts/Conformer/" class="btn btn-outline-primary" prompt="Newer"><p>(Speech Recognition) Conformer 리뷰 및 설명</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center text-muted"><div class="footer-left"><p class="mb-0"> © 2023 <a href="https://github.com/zerojsh00">Sanghyeon</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/k8s/">K8S</a> <a class="post-tag" href="/tags/kubernetes/">Kubernetes</a> <a class="post-tag" href="/tags/paper-review/">Paper Review</a> <a class="post-tag" href="/tags/security/">Security</a> <a class="post-tag" href="/tags/network/">Network</a> <a class="post-tag" href="/tags/speech-ai/">Speech AI</a> <a class="post-tag" href="/tags/scheduling/">Scheduling</a> <a class="post-tag" href="/tags/nlp/">NLP</a> <a class="post-tag" href="/tags/storage/">Storage</a> <a class="post-tag" href="/tags/asr/">ASR</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/ko.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script>
